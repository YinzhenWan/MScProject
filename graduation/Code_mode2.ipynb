{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab15096",
   "metadata": {},
   "source": [
    "## 1. Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f2e2b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from tensorflow.python.ops import math_ops\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.ops import summary_ops_v2\n",
    "import time\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b15b3a",
   "metadata": {},
   "source": [
    "## 2.Unzip the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d99996ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ml-1m.zip\n",
      "   creating: ml-1m/\n",
      "  inflating: ml-1m/movies.dat        \n",
      "  inflating: ml-1m/ratings.dat       \n",
      "  inflating: ml-1m/README            \n",
      "  inflating: ml-1m/users.dat         \n"
     ]
    }
   ],
   "source": [
    "!unzip ml-1m.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba00b9",
   "metadata": {},
   "source": [
    "## 3.View movies.dat and users.dat and ratings.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ccf7a1",
   "metadata": {},
   "source": [
    "load users.dat \" UserID::Gender::Age::Occupation::Zip-code \" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e9292eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>OccupationID</th>\n",
       "      <th>Zip-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID Gender  Age  OccupationID Zip-code\n",
       "0       1      F    1            10    48067\n",
       "1       2      M   56            16    70072\n",
       "2       3      M   25            15    55117\n",
       "3       4      M   45             7    02460\n",
       "4       5      M   25            20    55455"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_name = ['UserID', 'Gender', 'Age', 'OccupationID', 'Zip-code']\n",
    "users = pd.read_csv('./ml-1m/users.dat', sep='::', header = None, names = users_name, engine = 'python',encoding = \"ISO-8859-1\")\n",
    "users.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c854ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Age</th>\n",
       "      <th>OccupationID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6040.000000</td>\n",
       "      <td>6040.000000</td>\n",
       "      <td>6040.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3020.500000</td>\n",
       "      <td>30.639238</td>\n",
       "      <td>8.146854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1743.742145</td>\n",
       "      <td>12.895962</td>\n",
       "      <td>6.329511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1510.750000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3020.500000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4530.250000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6040.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            UserID          Age  OccupationID\n",
       "count  6040.000000  6040.000000   6040.000000\n",
       "mean   3020.500000    30.639238      8.146854\n",
       "std    1743.742145    12.895962      6.329511\n",
       "min       1.000000     1.000000      0.000000\n",
       "25%    1510.750000    25.000000      3.000000\n",
       "50%    3020.500000    25.000000      7.000000\n",
       "75%    4530.250000    35.000000     14.000000\n",
       "max    6040.000000    56.000000     20.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf8c806",
   "metadata": {},
   "source": [
    "load movies.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "221ed926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                               Title                        Genres\n",
       "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4        5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_name = ['MovieID', 'Title', 'Genres']\n",
    "movies = pd.read_csv('./ml-1m/movies.dat', sep='::', header = None, names = movies_name, engine = 'python',encoding = \"ISO-8859-1\")\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5922f127",
   "metadata": {},
   "source": [
    "load ratings.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb93b8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  timestamps\n",
       "0       1     1193       5   978300760\n",
       "1       1      661       3   978302109\n",
       "2       1      914       3   978301968\n",
       "3       1     3408       4   978300275\n",
       "4       1     2355       5   978824291"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_name = ['UserID','MovieID', 'Rating', 'timestamps']\n",
    "ratings = pd.read_csv('./ml-1m/ratings.dat', sep='::', header = None, names = ratings_name, engine = 'python',encoding = \"ISO-8859-1\")\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156b3eb7",
   "metadata": {},
   "source": [
    "## 4.Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded244e0",
   "metadata": {},
   "source": [
    "### 4.1 Data pre-processing of users.dat„ÄÅmovies.dat and ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c2b2a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dat():\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    \n",
    "    users_name = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']\n",
    "    users = pd.read_table('./ml-1m/users.dat', sep='::', header = None, names = users_name, engine = 'python',encoding = \"ISO-8859-1\")\n",
    "    \n",
    "    users = users.filter(regex='UserID|Gender|Age|JobID')\n",
    "    users_orga = users.values\n",
    "    \n",
    "    gender_map = {'F':0, 'M':1}\n",
    "    users['Gender'] = users['Gender'].map(gender_map)\n",
    "\n",
    "    age_map = {val:ii for ii,val in enumerate(set(users['Age']))}\n",
    "    users['Age'] = users['Age'].map(age_map)\n",
    "\n",
    "    \n",
    "    movies_name = ['MovieID', 'Title', 'Genres']\n",
    "    movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names = movies_name, engine = 'python',encoding = \"ISO-8859-1\")\n",
    "    movies_orga = movies.values\n",
    "    \n",
    "    pattern = re.compile(r'^(.*)\\((\\d+)\\)$')\n",
    "\n",
    "    title_change = {val:pattern.match(val).group(1) for ii,val in enumerate(set(movies['Title']))}\n",
    "    movies['Title'] = movies['Title'].map(title_change)\n",
    "\n",
    "    \n",
    "    genres_set = set()\n",
    "    for val in movies['Genres'].str.split('|'):\n",
    "        genres_set.update(val)\n",
    "\n",
    "    genres_set.add('<PAD>')\n",
    "    \n",
    "    genres2int = {val:ii for ii, val in enumerate(genres_set)}\n",
    "\n",
    "    \n",
    "    genres_change = {val:[genres2int[row] for row in val.split('|')] for ii,val in enumerate(set(movies['Genres']))}\n",
    "\n",
    "    \n",
    "    for key in genres_change:\n",
    "        for cnt in range(max(genres2int.values()) - len(genres_change[key])):\n",
    "            genres_change[key].insert(len(genres_change[key]) + cnt,genres2int['<PAD>'])\n",
    "    \n",
    "    movies['Genres'] = movies['Genres'].map(genres_change)\n",
    "\n",
    "    \n",
    "    title_set = set()\n",
    "    for val in movies['Title'].str.split():\n",
    "        title_set.update(val)\n",
    "    \n",
    "    title_set.add('<PAD>')\n",
    "    title2int = {val:ii for ii, val in enumerate(title_set)}\n",
    "\n",
    "    \n",
    "    title_count = 15\n",
    "    title_change = {val:[title2int[row] for row in val.split()] for ii,val in enumerate(set(movies['Title']))}\n",
    "    \n",
    "    for key in title_change:\n",
    "        for cnt in range(title_count - len(title_change[key])):\n",
    "            title_change[key].insert(len(title_change[key]) + cnt,title2int['<PAD>'])\n",
    "    \n",
    "    movies['Title'] = movies['Title'].map(title_change)\n",
    "\n",
    "    \n",
    "    ratings_name = ['UserID','MovieID', 'ratings', 'timestamps']\n",
    "    ratings = pd.read_csv('./ml-1m/ratings.dat', sep='::', header=None, names = ratings_name, engine = 'python',encoding = \"ISO-8859-1\")\n",
    "    ratings = ratings.filter(regex = 'UserID|MovieID|ratings')\n",
    "\n",
    "    \n",
    "    data = pd.merge(pd.merge(ratings, users), movies)\n",
    "    \n",
    "    \n",
    "    target_fields = ['ratings']\n",
    "    features_pd, targets_pd = data.drop(target_fields, axis=1), data[target_fields]\n",
    "    \n",
    "    features = features_pd.values\n",
    "    targets_values = targets_pd.values\n",
    "    \n",
    "    return title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orga, users_orga"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1debb90",
   "metadata": {},
   "source": [
    "### 4.2 View the user, movie and rating files after  the pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ba4cfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>JobID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  Gender  Age  JobID\n",
       "0       1       0    0     10\n",
       "1       2       1    5     16\n",
       "2       3       1    6     15\n",
       "3       4       1    2      7\n",
       "4       5       1    6     20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9ed0f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[98, 1780, 4565, 4565, 4565, 4565, 4565, 4565,...</td>\n",
       "      <td>[10, 17, 16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[4502, 4565, 4565, 4565, 4565, 4565, 4565, 456...</td>\n",
       "      <td>[14, 17, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[2915, 2014, 4808, 4565, 4565, 4565, 4565, 456...</td>\n",
       "      <td>[16, 9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[3731, 821, 4529, 4565, 4565, 4565, 4565, 4565...</td>\n",
       "      <td>[16, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[3227, 1715, 792, 3443, 3747, 113, 4565, 4565,...</td>\n",
       "      <td>[16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                                              Title  \\\n",
       "0        1  [98, 1780, 4565, 4565, 4565, 4565, 4565, 4565,...   \n",
       "1        2  [4502, 4565, 4565, 4565, 4565, 4565, 4565, 456...   \n",
       "2        3  [2915, 2014, 4808, 4565, 4565, 4565, 4565, 456...   \n",
       "3        4  [3731, 821, 4529, 4565, 4565, 4565, 4565, 4565...   \n",
       "4        5  [3227, 1715, 792, 3443, 3747, 113, 4565, 4565,...   \n",
       "\n",
       "                                              Genres  \n",
       "0  [10, 17, 16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [14, 17, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...  \n",
       "2  [16, 9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...  \n",
       "3  [16, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...  \n",
       "4  [16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f04d2cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  ratings\n",
       "0       1     1193        5\n",
       "1       1      661        3\n",
       "2       1      914        3\n",
       "3       1     3408        4\n",
       "4       1     2355        5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b80600f",
   "metadata": {},
   "source": [
    "### 4.3 Read the pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60297982",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orga, users_orga= read_dat()\n",
    "\n",
    "with open('./pre_process.pkl', 'wb') as kfc:\n",
    "    pickle.dump((title_count, \n",
    "                 title_set, \n",
    "                 genres2int, \n",
    "                 features, \n",
    "                 targets_values, \n",
    "                 ratings, \n",
    "                 users, \n",
    "                 movies, \n",
    "                 data, \n",
    "                 movies_orga, \n",
    "                 users_orga), \n",
    "                kfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9a4045",
   "metadata": {},
   "source": [
    "### 5.Building neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a91057",
   "metadata": {},
   "source": [
    "### 5.1 Define the required parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d37ea0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32\n",
    "uid_max = max(features.take(0,1)) + 1 \n",
    "gender_max = max(features.take(2,1)) + 1 \n",
    "age_max = max(features.take(3,1)) + 1 \n",
    "job_max = max(features.take(4,1)) + 1\n",
    "movie_id_max = max(features.take(1,1)) + 1 \n",
    "movie_categories_max = max(genres2int.values()) + 1 \n",
    "movie_title_max = len(title_set) \n",
    "combiner = \"sum\"\n",
    "sentences_size = title_count \n",
    "window_sizes = {2, 3, 4, 5}\n",
    "filter_num = 8\n",
    "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}\n",
    "batch_size = 256\n",
    "dropout_keep = 0.5\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2100235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_inputs():\n",
    "    uid = tf.keras.layers.Input(shape=(1,), dtype='int32', name='uid')  \n",
    "    user_gender = tf.keras.layers.Input(shape=(1,), dtype='int32', name ='user_gender')  \n",
    "    user_age = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_age') \n",
    "    user_job = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_job')\n",
    "\n",
    "    movie_id = tf.keras.layers.Input(shape=(1,), dtype='int32', name='movie_id') \n",
    "    movie_categories = tf.keras.layers.Input(shape=(18,), dtype='int32', name='movie_categories') \n",
    "    movie_titles = tf.keras.layers.Input(shape=(15,), dtype='int32', name='movie_titles') \n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8bc70",
   "metadata": {},
   "source": [
    "### 5.2 Building neural network models for obtaining user features and movie features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eac1d4",
   "metadata": {},
   "source": [
    "#### 5.2.1 Define the embedding matrix for User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e80563c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_user_embedding(uid, user_gender, user_age, user_job):\n",
    "    uid_embed_layer = tf.keras.layers.Embedding(uid_max, embed_dim, input_length=1, name='uid_embed_layer')(uid)\n",
    "    gender_embed_layer = tf.keras.layers.Embedding(gender_max, embed_dim // 2, input_length=1, name='gender_embed_layer')(user_gender)\n",
    "    age_embed_layer = tf.keras.layers.Embedding(age_max, embed_dim // 2, input_length=1, name='age_embed_layer')(user_age)\n",
    "    job_embed_layer = tf.keras.layers.Embedding(job_max, embed_dim // 2, input_length=1, name='job_embed_layer')(user_job)\n",
    "    return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c052fa4",
   "metadata": {},
   "source": [
    "#### 5.2.2 Generating User's features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "610129c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
    "    \n",
    "    uid_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"uid_fc_layer\", activation='relu')(uid_embed_layer)\n",
    "    gender_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"gender_fc_layer\", activation='relu')(gender_embed_layer)\n",
    "    age_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"age_fc_layer\", activation='relu')(age_embed_layer)\n",
    "    job_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"job_fc_layer\", activation='relu')(job_embed_layer)\n",
    "\n",
    "    user_combine_layer = tf.keras.layers.concatenate([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)  \n",
    "    user_combine_layer = tf.keras.layers.Dense(200, activation='tanh')(user_combine_layer)  \n",
    "\n",
    "    user_combine_layer_flat = tf.keras.layers.Reshape([200], name=\"user_combine_layer_flat\")(user_combine_layer)\n",
    "    return user_combine_layer, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7f060",
   "metadata": {},
   "source": [
    "#### 5.2.3 Define the embedding matrix for Movie ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9307d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_movie_id_embed_layer(movie_id):\n",
    "    movie_id_embed_layer = tf.keras.layers.Embedding(movie_id_max, embed_dim, input_length=1, name='movie_id_embed_layer')(movie_id)\n",
    "    return movie_id_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f977da0e",
   "metadata": {},
   "source": [
    "#### 5.2.4 Merging multiple embedding vectors of movie type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a831c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_movie_categories_layers(movie_categories):\n",
    "    movie_categories_embed_layer = tf.keras.layers.Embedding(movie_categories_max, embed_dim,\\\n",
    "        input_length=18, name='movie_categories_embed_layer')(movie_categories)\n",
    "    movie_categories_embed_layer = tf.keras.layers.Lambda(lambda layer: tf.reduce_sum(layer, \\\n",
    "         axis=1, keepdims=True))(movie_categories_embed_layer)\n",
    "    return movie_categories_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da740967",
   "metadata": {},
   "source": [
    "#### 5.2.5 Text Convolutional Network Implementation of Movie Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98eb91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_movie_cnn_layer(movie_titles):\n",
    "    movie_title_embed_layer = tf.keras.layers.Embedding(movie_title_max, embed_dim, \\\n",
    "                    input_length=15, name='movie_title_embed_layer')(movie_titles)\n",
    "    sp=movie_title_embed_layer.shape\n",
    "    movie_title_embed_layer_expand = tf.keras.layers.Reshape([sp[1], sp[2], 1])(movie_title_embed_layer)\n",
    "    pool_layer_lst = []\n",
    "    for window_size in window_sizes:\n",
    "        conv_layer = tf.keras.layers.Conv2D(filter_num, (window_size, embed_dim), 1, activation='relu')\\\n",
    "        (movie_title_embed_layer_expand)\n",
    "        maxpool_layer = tf.keras.layers.MaxPooling2D(pool_size=(sentences_size - window_size + 1 ,1),\\\n",
    "                                                     strides=1)(conv_layer)\n",
    "        pool_layer_lst.append(maxpool_layer)\n",
    "    pool_layer = tf.keras.layers.concatenate(pool_layer_lst, 3, name =\"pool_layer\")  \n",
    "    max_num = len(window_sizes) * filter_num\n",
    "    pool_layer_flat = tf.keras.layers.Reshape([1, max_num], name = \"pool_layer_flat\")(pool_layer)\n",
    "\n",
    "    dropout_layer = tf.keras.layers.Dropout(dropout_keep, name = \"dropout_layer\")(pool_layer_flat)\n",
    "    return pool_layer_flat, dropout_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c25a6b",
   "metadata": {},
   "source": [
    "#### 5.2.6Movie's layers are joined together for full connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b354971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_movie_feature_layer(movie_id_embed_layer, movie_categories_embed_layer, dropout_layer):\n",
    "    movie_id_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"movie_id_fc_layer\", activation='relu')(movie_id_embed_layer)\n",
    "    movie_categories_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"movie_categories_fc_layer\", activation='relu')(movie_categories_embed_layer)\n",
    "\n",
    "    movie_combine_layer = tf.keras.layers.concatenate([movie_id_fc_layer, movie_categories_fc_layer, dropout_layer], 2)  \n",
    "    movie_combine_layer = tf.keras.layers.Dense(200, activation='tanh')(movie_combine_layer)\n",
    "\n",
    "    movie_combine_layer_flat = tf.keras.layers.Reshape([200], name=\"movie_combine_layer_flat\")(movie_combine_layer)\n",
    "    return movie_combine_layer, movie_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a52339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = tf.keras.layers.Lambda(lambda layer: \n",
    "            tf.reduce_sum(layer[0] * layer[1], axis=1), name=\"inference\")((user_combine_layer_flat, \n",
    "                                                                           movie_combine_layer_flat))\n",
    "inference = tf.keras.layers.Lambda(lambda layer: tf.expand_dims(layer, axis=1))(inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db925394",
   "metadata": {},
   "source": [
    "### 6.1Constructing computational diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66b3bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"./models\"\n",
    "\n",
    "class first_net(object):\n",
    "    def __init__(self, batch_size=256):\n",
    "        self.batch_size = batch_size\n",
    "        self.best_loss = 9999\n",
    "        self.losses = {'train': [], 'test': []}\n",
    "\n",
    "        \n",
    "        uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles = obtain_inputs()\n",
    "        \n",
    "        uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = obtain_user_embedding(uid, user_gender,\n",
    "                                                                                                   user_age, user_job)\n",
    "        \n",
    "        user_combine_layer, user_combine_layer_flat = obtain_user_feature_layer(uid_embed_layer, gender_embed_layer,\n",
    "                                                                             age_embed_layer, job_embed_layer)\n",
    "        \n",
    "        movie_id_embed_layer = obtain_movie_id_embed_layer(movie_id)\n",
    "        movie_categories_embed_layer = obtain_movie_categories_layers(movie_categories)\n",
    "        pool_layer_flat, dropout_layer = obtain_movie_cnn_layer(movie_titles)\n",
    "        movie_combine_layer, movie_combine_layer_flat = obtain_movie_feature_layer(movie_id_embed_layer,\n",
    "                                                                                movie_categories_embed_layer,\n",
    "                                                                                dropout_layer)\n",
    "\n",
    "        inference_layer = tf.keras.layers.concatenate([user_combine_layer_flat, movie_combine_layer_flat],\n",
    "                                                      1)  \n",
    "        \n",
    "        inference_dense = tf.keras.layers.Dense(64, kernel_regularizer=tf.nn.l2_loss, activation='relu')(\n",
    "            inference_layer)\n",
    "        inference = tf.keras.layers.Dense(1, name=\"inference\")(inference_layer)  # inference_dense\n",
    "        \n",
    "\n",
    "        self.model = tf.keras.Model(\n",
    "            inputs=[uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles],\n",
    "            outputs=[inference])\n",
    "\n",
    "        self.model.summary()\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "        self.ComputeLoss = tf.keras.losses.MeanSquaredError()\n",
    "        self.ComputeMetrics = tf.keras.metrics.MeanAbsoluteError()\n",
    "\n",
    "        if tf.io.gfile.exists(dir_path):\n",
    "            pass\n",
    "        else:\n",
    "            tf.io.gfile.makedirs(dir_path)\n",
    "\n",
    "        train_dir = os.path.join(dir_path, 'summaries', 'train')\n",
    "        test_dir = os.path.join(dir_path, 'summaries', 'eval')\n",
    "\n",
    "        \n",
    "        checkpoint_dir = os.path.join(dir_path, 'checkpoints')\n",
    "        self.checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "        self.checkpoint = tf.train.Checkpoint(model=self.model, optimizer=self.optimizer)\n",
    "        self.checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "    def compute_loss(self, labels, logits):\n",
    "        return tf.reduce_mean(tf.keras.losses.mse(labels, logits))\n",
    "\n",
    "    def compute_metrics(self, labels, logits):\n",
    "        return tf.keras.metrics.mae(labels, logits)  \n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.model([x[0],\n",
    "                                 x[1],\n",
    "                                 x[2],\n",
    "                                 x[3],\n",
    "                                 x[4],\n",
    "                                 x[5],\n",
    "                                 x[6]], training=True)\n",
    "            loss = self.ComputeLoss(y, logits)\n",
    "            self.ComputeMetrics(y, logits)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss, logits\n",
    "\n",
    "    def training(self, features, targets_values, epochs=5, log_freq=50):\n",
    "\n",
    "        for epoch_i in range(epochs):\n",
    "            train_X, test_X, train_y, test_y = train_test_split(features,\n",
    "                                                                targets_values,\n",
    "                                                                test_size = 0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "            train_batches = obtain_batches(train_X, train_y, self.batch_size)\n",
    "            batch_num = (len(train_X) // self.batch_size)\n",
    "\n",
    "            train_start = time.time()\n",
    "            if True:\n",
    "                start = time.time()\n",
    "                avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
    "                for batch_i in range(batch_num):\n",
    "                    x, y = next(train_batches)\n",
    "                    categories = np.zeros([self.batch_size, 18])\n",
    "                    for i in range(self.batch_size):\n",
    "                        categories[i] = x.take(6, 1)[i]\n",
    "\n",
    "                    titles = np.zeros([self.batch_size, sentences_size])\n",
    "                    for i in range(self.batch_size):\n",
    "                        titles[i] = x.take(5, 1)[i]\n",
    "\n",
    "                    loss, logits = self.train_step([np.reshape(x.take(0, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                                    np.reshape(x.take(2, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                                    np.reshape(x.take(3, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                                    np.reshape(x.take(4, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                                    np.reshape(x.take(1, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                                    categories.astype(np.float32),\n",
    "                                                    titles.astype(np.float32)],\n",
    "                                                   np.reshape(y, [self.batch_size, 1]).astype(np.float32))\n",
    "                    avg_loss(loss)\n",
    "                    self.losses['train'].append(loss)\n",
    "\n",
    "                    if tf.equal(self.optimizer.iterations % log_freq, 0):\n",
    "                        rate = log_freq / (time.time() - start)\n",
    "                        print('Step #{}\\tEpoch {:>3} Batch {:>4}/{}   Loss: {:0.6f} mae: {:0.6f} ({} steps/sec)'.format(\n",
    "                            self.optimizer.iterations.numpy(),\n",
    "                            epoch_i,\n",
    "                            batch_i,\n",
    "                            batch_num,\n",
    "                            loss, (self.ComputeMetrics.result()), rate))\n",
    "                        avg_loss.reset_states()\n",
    "                        self.ComputeMetrics.reset_states()\n",
    "                        start = time.time()\n",
    "\n",
    "            train_end = time.time()\n",
    "            print(\n",
    "                '\\nTrain time for epoch #{} ({} total steps): {}'.format(epoch_i + 1, self.optimizer.iterations.numpy(),\n",
    "                                                                         train_end - train_start))\n",
    "            self.testing((test_X, test_y), self.optimizer.iterations)\n",
    "        self.export_path = os.path.join(dir_path, 'export')\n",
    "        tf.saved_model.save(self.model, self.export_path)\n",
    "\n",
    "    def testing(self, test_dataset, step_num):\n",
    "        test_X, test_y = test_dataset\n",
    "        test_batches = obtain_batches(test_X, test_y, self.batch_size)\n",
    "\n",
    "        \"\"\"Perform an evaluation of `model` on the examples from `dataset`.\"\"\"\n",
    "        avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
    "\n",
    "        batch_num = (len(test_X) // self.batch_size)\n",
    "        for batch_i in range(batch_num):\n",
    "            x, y = next(test_batches)\n",
    "            categories = np.zeros([self.batch_size, 18])\n",
    "            for i in range(self.batch_size):\n",
    "                categories[i] = x.take(6, 1)[i]\n",
    "\n",
    "            titles = np.zeros([self.batch_size, sentences_size])\n",
    "            for i in range(self.batch_size):\n",
    "                titles[i] = x.take(5, 1)[i]\n",
    "\n",
    "            logits = self.model([np.reshape(x.take(0, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                 np.reshape(x.take(2, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                 np.reshape(x.take(3, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                 np.reshape(x.take(4, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                 np.reshape(x.take(1, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                 categories.astype(np.float32),\n",
    "                                 titles.astype(np.float32)], training=False)\n",
    "            test_loss = self.ComputeLoss(np.reshape(y, [self.batch_size, 1]).astype(np.float32), logits)\n",
    "            avg_loss(test_loss)\n",
    "            \n",
    "            self.losses['test'].append(test_loss)\n",
    "            self.ComputeMetrics(np.reshape(y, [self.batch_size, 1]).astype(np.float32), logits)\n",
    "\n",
    "        print('Model test set loss: {:0.6f} mae: {:0.6f}'.format(avg_loss.result(), self.ComputeMetrics.result()))\n",
    "\n",
    "        if avg_loss.result() < self.best_loss:\n",
    "            self.best_loss = avg_loss.result()\n",
    "            print(\"best loss = {}\".format(self.best_loss))\n",
    "            self.checkpoint.save(self.checkpoint_prefix)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        predictions = self.model(xs)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c9e8d",
   "metadata": {},
   "source": [
    "### 6.2 obtain Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "339fee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d3166",
   "metadata": {},
   "source": [
    "### 6.3 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b84b55de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 18:46:01.776667: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-15 18:46:01.778148: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "movie_titles (InputLayer)       [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movie_title_embed_layer (Embedd (None, 15, 32)       166944      movie_titles[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 15, 32, 1)    0           movie_title_embed_layer[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 14, 1, 8)     520         reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 13, 1, 8)     776         reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 12, 1, 8)     1032        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 11, 1, 8)     1288        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "movie_categories (InputLayer)   [(None, 18)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 1, 1, 8)      0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "uid (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_gender (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_age (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_job (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movie_id (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movie_categories_embed_layer (E (None, 18, 32)       608         movie_categories[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "pool_layer (Concatenate)        (None, 1, 1, 32)     0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "uid_embed_layer (Embedding)     (None, 1, 32)        193312      uid[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "gender_embed_layer (Embedding)  (None, 1, 16)        32          user_gender[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "age_embed_layer (Embedding)     (None, 1, 16)        112         user_age[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "job_embed_layer (Embedding)     (None, 1, 16)        336         user_job[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "movie_id_embed_layer (Embedding (None, 1, 32)        126496      movie_id[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1, 32)        0           movie_categories_embed_layer[0][0\n",
      "__________________________________________________________________________________________________\n",
      "pool_layer_flat (Reshape)       (None, 1, 32)        0           pool_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "uid_fc_layer (Dense)            (None, 1, 32)        1056        uid_embed_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "gender_fc_layer (Dense)         (None, 1, 32)        544         gender_embed_layer[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "age_fc_layer (Dense)            (None, 1, 32)        544         age_embed_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "job_fc_layer (Dense)            (None, 1, 32)        544         job_embed_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "movie_id_fc_layer (Dense)       (None, 1, 32)        1056        movie_id_embed_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "movie_categories_fc_layer (Dens (None, 1, 32)        1056        lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_layer (Dropout)         (None, 1, 32)        0           pool_layer_flat[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1, 128)       0           uid_fc_layer[0][0]               \n",
      "                                                                 gender_fc_layer[0][0]            \n",
      "                                                                 age_fc_layer[0][0]               \n",
      "                                                                 job_fc_layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 96)        0           movie_id_fc_layer[0][0]          \n",
      "                                                                 movie_categories_fc_layer[0][0]  \n",
      "                                                                 dropout_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1, 200)       25800       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 200)       19400       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "user_combine_layer_flat (Reshap (None, 200)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "movie_combine_layer_flat (Resha (None, 200)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 400)          0           user_combine_layer_flat[0][0]    \n",
      "                                                                 movie_combine_layer_flat[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "inference (Dense)               (None, 1)            401         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 541,857\n",
      "Trainable params: 541,857\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #50\tEpoch   0 Batch   49/3125   Loss: 8.800690 mae: 3.197470 (16.32878574183764 steps/sec)\n",
      "Step #100\tEpoch   0 Batch   99/3125   Loss: 2.985691 mae: 2.070314 (44.81526848319538 steps/sec)\n",
      "Step #150\tEpoch   0 Batch  149/3125   Loss: 1.239390 mae: 1.078838 (44.9302453330671 steps/sec)\n",
      "Step #200\tEpoch   0 Batch  199/3125   Loss: 1.264295 mae: 0.927268 (45.31431638465933 steps/sec)\n",
      "Step #250\tEpoch   0 Batch  249/3125   Loss: 1.136747 mae: 0.919832 (44.9968019612635 steps/sec)\n",
      "Step #300\tEpoch   0 Batch  299/3125   Loss: 1.350852 mae: 0.921343 (45.10533925058727 steps/sec)\n",
      "Step #350\tEpoch   0 Batch  349/3125   Loss: 1.211747 mae: 0.919327 (45.30318639559884 steps/sec)\n",
      "Step #400\tEpoch   0 Batch  399/3125   Loss: 1.319141 mae: 0.916161 (45.24235697968643 steps/sec)\n",
      "Step #450\tEpoch   0 Batch  449/3125   Loss: 1.136827 mae: 0.913696 (44.99349068399738 steps/sec)\n",
      "Step #500\tEpoch   0 Batch  499/3125   Loss: 1.069892 mae: 0.901127 (45.49515621257823 steps/sec)\n",
      "Step #550\tEpoch   0 Batch  549/3125   Loss: 1.108964 mae: 0.890955 (41.032006362728346 steps/sec)\n",
      "Step #600\tEpoch   0 Batch  599/3125   Loss: 0.984146 mae: 0.890740 (45.2544433858746 steps/sec)\n",
      "Step #650\tEpoch   0 Batch  649/3125   Loss: 1.157964 mae: 0.880489 (44.71640510425183 steps/sec)\n",
      "Step #700\tEpoch   0 Batch  699/3125   Loss: 0.959616 mae: 0.866226 (44.90462580477732 steps/sec)\n",
      "Step #750\tEpoch   0 Batch  749/3125   Loss: 1.016689 mae: 0.853248 (45.255625036955 steps/sec)\n",
      "Step #800\tEpoch   0 Batch  799/3125   Loss: 1.145639 mae: 0.841983 (45.40028043519629 steps/sec)\n",
      "Step #850\tEpoch   0 Batch  849/3125   Loss: 1.125500 mae: 0.832002 (45.12419658608662 steps/sec)\n",
      "Step #900\tEpoch   0 Batch  899/3125   Loss: 1.017695 mae: 0.814201 (45.111442138604914 steps/sec)\n",
      "Step #950\tEpoch   0 Batch  949/3125   Loss: 0.825716 mae: 0.800270 (43.65135316672089 steps/sec)\n",
      "Step #1000\tEpoch   0 Batch  999/3125   Loss: 1.015054 mae: 0.792819 (43.442186410852095 steps/sec)\n",
      "Step #1050\tEpoch   0 Batch 1049/3125   Loss: 0.983588 mae: 0.791574 (44.143486876421086 steps/sec)\n",
      "Step #1100\tEpoch   0 Batch 1099/3125   Loss: 0.912778 mae: 0.776913 (45.00613021902898 steps/sec)\n",
      "Step #1150\tEpoch   0 Batch 1149/3125   Loss: 0.877854 mae: 0.779572 (43.91566123774136 steps/sec)\n",
      "Step #1200\tEpoch   0 Batch 1199/3125   Loss: 0.858696 mae: 0.761498 (41.13784888379743 steps/sec)\n",
      "Step #1250\tEpoch   0 Batch 1249/3125   Loss: 1.006858 mae: 0.766451 (42.26763855706282 steps/sec)\n",
      "Step #1300\tEpoch   0 Batch 1299/3125   Loss: 0.908350 mae: 0.770344 (45.17127762986232 steps/sec)\n",
      "Step #1350\tEpoch   0 Batch 1349/3125   Loss: 0.964165 mae: 0.748881 (45.05786303703221 steps/sec)\n",
      "Step #1400\tEpoch   0 Batch 1399/3125   Loss: 0.894623 mae: 0.759367 (44.427816703733065 steps/sec)\n",
      "Step #1450\tEpoch   0 Batch 1449/3125   Loss: 0.916435 mae: 0.755121 (45.00948199708286 steps/sec)\n",
      "Step #1500\tEpoch   0 Batch 1499/3125   Loss: 0.889365 mae: 0.750988 (45.10647432096232 steps/sec)\n",
      "Step #1550\tEpoch   0 Batch 1549/3125   Loss: 0.885626 mae: 0.749356 (44.995508284731194 steps/sec)\n",
      "Step #1600\tEpoch   0 Batch 1599/3125   Loss: 0.904566 mae: 0.747382 (44.672179599393935 steps/sec)\n",
      "Step #1650\tEpoch   0 Batch 1649/3125   Loss: 0.940198 mae: 0.747871 (44.69742018454839 steps/sec)\n",
      "Step #1700\tEpoch   0 Batch 1699/3125   Loss: 0.908800 mae: 0.737414 (45.16923450696626 steps/sec)\n",
      "Step #1750\tEpoch   0 Batch 1749/3125   Loss: 0.864643 mae: 0.741212 (45.47372949493776 steps/sec)\n",
      "Step #1800\tEpoch   0 Batch 1799/3125   Loss: 0.984086 mae: 0.742775 (45.380612306061714 steps/sec)\n",
      "Step #1850\tEpoch   0 Batch 1849/3125   Loss: 0.782039 mae: 0.742769 (44.906241193918376 steps/sec)\n",
      "Step #1900\tEpoch   0 Batch 1899/3125   Loss: 0.886522 mae: 0.739439 (45.31382682427331 steps/sec)\n",
      "Step #1950\tEpoch   0 Batch 1949/3125   Loss: 0.833899 mae: 0.731086 (43.589738260203355 steps/sec)\n",
      "Step #2000\tEpoch   0 Batch 1999/3125   Loss: 0.944199 mae: 0.739845 (43.8239737167162 steps/sec)\n",
      "Step #2050\tEpoch   0 Batch 2049/3125   Loss: 0.825566 mae: 0.742834 (45.3227874541267 steps/sec)\n",
      "Step #2100\tEpoch   0 Batch 2099/3125   Loss: 0.876422 mae: 0.727595 (44.99146361190263 steps/sec)\n",
      "Step #2150\tEpoch   0 Batch 2149/3125   Loss: 0.804199 mae: 0.728411 (45.62548175776561 steps/sec)\n",
      "Step #2200\tEpoch   0 Batch 2199/3125   Loss: 0.780996 mae: 0.728552 (44.776831324002124 steps/sec)\n",
      "Step #2250\tEpoch   0 Batch 2249/3125   Loss: 0.833232 mae: 0.731168 (44.90805864563086 steps/sec)\n",
      "Step #2300\tEpoch   0 Batch 2299/3125   Loss: 0.896782 mae: 0.737452 (45.015964805519154 steps/sec)\n",
      "Step #2350\tEpoch   0 Batch 2349/3125   Loss: 0.847076 mae: 0.730990 (45.05332319903962 steps/sec)\n",
      "Step #2400\tEpoch   0 Batch 2399/3125   Loss: 0.783060 mae: 0.734593 (44.71729184244705 steps/sec)\n",
      "Step #2450\tEpoch   0 Batch 2449/3125   Loss: 0.851425 mae: 0.724346 (44.51381993437818 steps/sec)\n",
      "Step #2500\tEpoch   0 Batch 2499/3125   Loss: 0.946874 mae: 0.730613 (45.474971929859564 steps/sec)\n",
      "Step #2550\tEpoch   0 Batch 2549/3125   Loss: 0.697172 mae: 0.735216 (45.326470663882006 steps/sec)\n",
      "Step #2600\tEpoch   0 Batch 2599/3125   Loss: 1.010154 mae: 0.731963 (46.03207876292652 steps/sec)\n",
      "Step #2650\tEpoch   0 Batch 2649/3125   Loss: 0.913125 mae: 0.728496 (45.418175352560915 steps/sec)\n",
      "Step #2700\tEpoch   0 Batch 2699/3125   Loss: 0.812717 mae: 0.727570 (42.918342158208425 steps/sec)\n",
      "Step #2750\tEpoch   0 Batch 2749/3125   Loss: 1.000704 mae: 0.726033 (44.80771363258671 steps/sec)\n",
      "Step #2800\tEpoch   0 Batch 2799/3125   Loss: 0.851792 mae: 0.728866 (44.96334477310594 steps/sec)\n",
      "Step #2850\tEpoch   0 Batch 2849/3125   Loss: 0.857498 mae: 0.724894 (45.06960890925268 steps/sec)\n",
      "Step #2900\tEpoch   0 Batch 2899/3125   Loss: 0.876333 mae: 0.725395 (44.83962603426209 steps/sec)\n",
      "Step #2950\tEpoch   0 Batch 2949/3125   Loss: 0.882368 mae: 0.727116 (44.623616331953514 steps/sec)\n",
      "Step #3000\tEpoch   0 Batch 2999/3125   Loss: 0.761902 mae: 0.723602 (44.09925545580963 steps/sec)\n",
      "Step #3050\tEpoch   0 Batch 3049/3125   Loss: 0.814493 mae: 0.716386 (43.58206560693239 steps/sec)\n",
      "Step #3100\tEpoch   0 Batch 3099/3125   Loss: 0.886316 mae: 0.721548 (42.82189079351156 steps/sec)\n",
      "\n",
      "Train time for epoch #1 (3125 total steps): 72.0043568611145\n",
      "Model test set loss: 0.845350 mae: 0.725909\n",
      "best loss = 0.8453497886657715\n",
      "Step #3150\tEpoch   1 Batch   24/3125   Loss: 0.737325 mae: 0.725858 (82.54882210148479 steps/sec)\n",
      "Step #3200\tEpoch   1 Batch   74/3125   Loss: 0.901004 mae: 0.721032 (44.11808822010406 steps/sec)\n",
      "Step #3250\tEpoch   1 Batch  124/3125   Loss: 0.726739 mae: 0.727002 (41.07798688968745 steps/sec)\n",
      "Step #3300\tEpoch   1 Batch  174/3125   Loss: 0.761996 mae: 0.726084 (43.859186453282895 steps/sec)\n",
      "Step #3350\tEpoch   1 Batch  224/3125   Loss: 0.620289 mae: 0.720293 (43.43286546459339 steps/sec)\n",
      "Step #3400\tEpoch   1 Batch  274/3125   Loss: 0.825277 mae: 0.720105 (43.55462199010466 steps/sec)\n",
      "Step #3450\tEpoch   1 Batch  324/3125   Loss: 0.807806 mae: 0.724265 (43.26966187117304 steps/sec)\n",
      "Step #3500\tEpoch   1 Batch  374/3125   Loss: 0.819475 mae: 0.724948 (44.30333564938976 steps/sec)\n",
      "Step #3550\tEpoch   1 Batch  424/3125   Loss: 0.882379 mae: 0.727919 (42.28486237043008 steps/sec)\n",
      "Step #3600\tEpoch   1 Batch  474/3125   Loss: 0.798758 mae: 0.722211 (44.74068391323112 steps/sec)\n",
      "Step #3650\tEpoch   1 Batch  524/3125   Loss: 0.834539 mae: 0.715852 (44.1666078733856 steps/sec)\n",
      "Step #3700\tEpoch   1 Batch  574/3125   Loss: 0.916904 mae: 0.726848 (43.73412745582589 steps/sec)\n",
      "Step #3750\tEpoch   1 Batch  624/3125   Loss: 0.649677 mae: 0.721681 (42.88061609478935 steps/sec)\n",
      "Step #3800\tEpoch   1 Batch  674/3125   Loss: 0.846255 mae: 0.724942 (44.35184174751779 steps/sec)\n",
      "Step #3850\tEpoch   1 Batch  724/3125   Loss: 0.669061 mae: 0.722456 (44.90966466454462 steps/sec)\n",
      "Step #3900\tEpoch   1 Batch  774/3125   Loss: 0.778825 mae: 0.715872 (44.0329217242941 steps/sec)\n",
      "Step #3950\tEpoch   1 Batch  824/3125   Loss: 0.855212 mae: 0.719978 (43.41446025266751 steps/sec)\n",
      "Step #4000\tEpoch   1 Batch  874/3125   Loss: 0.789231 mae: 0.720345 (44.697020073624316 steps/sec)\n",
      "Step #4050\tEpoch   1 Batch  924/3125   Loss: 0.918058 mae: 0.724260 (43.34509787386726 steps/sec)\n",
      "Step #4100\tEpoch   1 Batch  974/3125   Loss: 0.756894 mae: 0.721532 (43.101850491196004 steps/sec)\n",
      "Step #4150\tEpoch   1 Batch 1024/3125   Loss: 0.816930 mae: 0.718956 (43.204383330456 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #4200\tEpoch   1 Batch 1074/3125   Loss: 0.851825 mae: 0.720058 (43.86734238813488 steps/sec)\n",
      "Step #4250\tEpoch   1 Batch 1124/3125   Loss: 0.843254 mae: 0.723146 (41.033323021939005 steps/sec)\n",
      "Step #4300\tEpoch   1 Batch 1174/3125   Loss: 0.759921 mae: 0.723091 (44.104625329235986 steps/sec)\n",
      "Step #4350\tEpoch   1 Batch 1224/3125   Loss: 0.762533 mae: 0.714078 (43.42373733462594 steps/sec)\n",
      "Step #4400\tEpoch   1 Batch 1274/3125   Loss: 0.943165 mae: 0.733355 (44.56688884034182 steps/sec)\n",
      "Step #4450\tEpoch   1 Batch 1324/3125   Loss: 0.791572 mae: 0.717060 (43.34103991529993 steps/sec)\n",
      "Step #4500\tEpoch   1 Batch 1374/3125   Loss: 0.797445 mae: 0.715452 (42.745152792144594 steps/sec)\n",
      "Step #4550\tEpoch   1 Batch 1424/3125   Loss: 0.827710 mae: 0.719556 (42.502086950494636 steps/sec)\n",
      "Step #4600\tEpoch   1 Batch 1474/3125   Loss: 0.721142 mae: 0.720524 (44.26897628287873 steps/sec)\n",
      "Step #4650\tEpoch   1 Batch 1524/3125   Loss: 0.746169 mae: 0.715573 (43.5733273272849 steps/sec)\n",
      "Step #4700\tEpoch   1 Batch 1574/3125   Loss: 0.781837 mae: 0.724827 (42.66375887873853 steps/sec)\n",
      "Step #4750\tEpoch   1 Batch 1624/3125   Loss: 0.904180 mae: 0.712975 (43.12839814600403 steps/sec)\n",
      "Step #4800\tEpoch   1 Batch 1674/3125   Loss: 0.800976 mae: 0.720484 (42.7328194219934 steps/sec)\n",
      "Step #4850\tEpoch   1 Batch 1724/3125   Loss: 0.922262 mae: 0.717353 (40.44087190139699 steps/sec)\n",
      "Step #4900\tEpoch   1 Batch 1774/3125   Loss: 0.759897 mae: 0.717207 (42.823805778201425 steps/sec)\n",
      "Step #4950\tEpoch   1 Batch 1824/3125   Loss: 0.733911 mae: 0.721315 (41.022069917118735 steps/sec)\n",
      "Step #5000\tEpoch   1 Batch 1874/3125   Loss: 0.911940 mae: 0.720208 (43.367802412663224 steps/sec)\n",
      "Step #5050\tEpoch   1 Batch 1924/3125   Loss: 1.001418 mae: 0.716836 (44.41227358688355 steps/sec)\n",
      "Step #5100\tEpoch   1 Batch 1974/3125   Loss: 0.829309 mae: 0.714850 (44.427101406353735 steps/sec)\n",
      "Step #5150\tEpoch   1 Batch 2024/3125   Loss: 0.877679 mae: 0.723255 (43.28937416812812 steps/sec)\n",
      "Step #5200\tEpoch   1 Batch 2074/3125   Loss: 0.894936 mae: 0.714788 (43.808200417704 steps/sec)\n",
      "Step #5250\tEpoch   1 Batch 2124/3125   Loss: 0.661646 mae: 0.719369 (43.873647196505594 steps/sec)\n",
      "Step #5300\tEpoch   1 Batch 2174/3125   Loss: 0.731114 mae: 0.710345 (43.851886299484796 steps/sec)\n",
      "Step #5350\tEpoch   1 Batch 2224/3125   Loss: 0.722857 mae: 0.716938 (44.67943180062956 steps/sec)\n",
      "Step #5400\tEpoch   1 Batch 2274/3125   Loss: 0.848497 mae: 0.717959 (44.38817129332719 steps/sec)\n",
      "Step #5450\tEpoch   1 Batch 2324/3125   Loss: 0.692425 mae: 0.726184 (44.991917274077885 steps/sec)\n",
      "Step #5500\tEpoch   1 Batch 2374/3125   Loss: 0.779885 mae: 0.716705 (44.10816886336208 steps/sec)\n",
      "Step #5550\tEpoch   1 Batch 2424/3125   Loss: 0.779264 mae: 0.718921 (44.16344555201885 steps/sec)\n",
      "Step #5600\tEpoch   1 Batch 2474/3125   Loss: 0.898044 mae: 0.712782 (44.434801475374265 steps/sec)\n",
      "Step #5650\tEpoch   1 Batch 2524/3125   Loss: 0.784971 mae: 0.723058 (43.101106387071674 steps/sec)\n",
      "Step #5700\tEpoch   1 Batch 2574/3125   Loss: 0.891605 mae: 0.723996 (44.40299242197274 steps/sec)\n",
      "Step #5750\tEpoch   1 Batch 2624/3125   Loss: 0.967747 mae: 0.722796 (45.37279694473033 steps/sec)\n",
      "Step #5800\tEpoch   1 Batch 2674/3125   Loss: 0.847738 mae: 0.718994 (42.97403875866693 steps/sec)\n",
      "Step #5850\tEpoch   1 Batch 2724/3125   Loss: 0.800580 mae: 0.711322 (42.34879224828931 steps/sec)\n",
      "Step #5900\tEpoch   1 Batch 2774/3125   Loss: 0.783608 mae: 0.717957 (43.246168135183865 steps/sec)\n",
      "Step #5950\tEpoch   1 Batch 2824/3125   Loss: 0.861689 mae: 0.725697 (43.61474974289544 steps/sec)\n",
      "Step #6000\tEpoch   1 Batch 2874/3125   Loss: 0.866658 mae: 0.710655 (42.94987087300498 steps/sec)\n",
      "Step #6050\tEpoch   1 Batch 2924/3125   Loss: 0.846297 mae: 0.718031 (43.22400054577424 steps/sec)\n",
      "Step #6100\tEpoch   1 Batch 2974/3125   Loss: 0.748595 mae: 0.719398 (43.755716832357436 steps/sec)\n",
      "Step #6150\tEpoch   1 Batch 3024/3125   Loss: 0.712999 mae: 0.710460 (43.386203980235365 steps/sec)\n",
      "Step #6200\tEpoch   1 Batch 3074/3125   Loss: 0.838850 mae: 0.710884 (44.02215348238906 steps/sec)\n",
      "Step #6250\tEpoch   1 Batch 3124/3125   Loss: 0.869065 mae: 0.712140 (43.68761763501922 steps/sec)\n",
      "\n",
      "Train time for epoch #2 (6250 total steps): 71.92865109443665\n",
      "Model test set loss: 0.834988 mae: 0.722107\n",
      "best loss = 0.8349884748458862\n",
      "Step #6300\tEpoch   2 Batch   49/3125   Loss: 0.923069 mae: 0.721872 (39.039682254539265 steps/sec)\n",
      "Step #6350\tEpoch   2 Batch   99/3125   Loss: 1.009362 mae: 0.710808 (42.93102894144096 steps/sec)\n",
      "Step #6400\tEpoch   2 Batch  149/3125   Loss: 0.780666 mae: 0.722794 (42.5581413282454 steps/sec)\n",
      "Step #6450\tEpoch   2 Batch  199/3125   Loss: 0.878268 mae: 0.718871 (43.35160294215533 steps/sec)\n",
      "Step #6500\tEpoch   2 Batch  249/3125   Loss: 0.823160 mae: 0.711134 (43.89426288936852 steps/sec)\n",
      "Step #6550\tEpoch   2 Batch  299/3125   Loss: 0.793651 mae: 0.718082 (43.88471022705396 steps/sec)\n",
      "Step #6600\tEpoch   2 Batch  349/3125   Loss: 0.895775 mae: 0.719572 (42.36443907548384 steps/sec)\n",
      "Step #6650\tEpoch   2 Batch  399/3125   Loss: 0.928464 mae: 0.719009 (41.63236265690908 steps/sec)\n",
      "Step #6700\tEpoch   2 Batch  449/3125   Loss: 0.809881 mae: 0.724685 (42.29445617748751 steps/sec)\n",
      "Step #6750\tEpoch   2 Batch  499/3125   Loss: 0.799249 mae: 0.712046 (43.892838911561604 steps/sec)\n",
      "Step #6800\tEpoch   2 Batch  549/3125   Loss: 0.791799 mae: 0.712084 (43.59629884479221 steps/sec)\n",
      "Step #6850\tEpoch   2 Batch  599/3125   Loss: 0.727809 mae: 0.725735 (43.742474159499324 steps/sec)\n",
      "Step #6900\tEpoch   2 Batch  649/3125   Loss: 0.784886 mae: 0.720766 (44.4727100069917 steps/sec)\n",
      "Step #6950\tEpoch   2 Batch  699/3125   Loss: 0.771477 mae: 0.720659 (43.669541525953484 steps/sec)\n",
      "Step #7000\tEpoch   2 Batch  749/3125   Loss: 0.790656 mae: 0.708109 (43.4685063753334 steps/sec)\n",
      "Step #7050\tEpoch   2 Batch  799/3125   Loss: 0.899896 mae: 0.718639 (43.14030423538376 steps/sec)\n",
      "Step #7100\tEpoch   2 Batch  849/3125   Loss: 0.949417 mae: 0.717357 (44.51982994954785 steps/sec)\n",
      "Step #7150\tEpoch   2 Batch  899/3125   Loss: 0.890771 mae: 0.716657 (39.1890381797273 steps/sec)\n",
      "Step #7200\tEpoch   2 Batch  949/3125   Loss: 0.710639 mae: 0.716283 (41.144604267428 steps/sec)\n",
      "Step #7250\tEpoch   2 Batch  999/3125   Loss: 0.867082 mae: 0.715639 (43.28592522996432 steps/sec)\n",
      "Step #7300\tEpoch   2 Batch 1049/3125   Loss: 0.890778 mae: 0.721980 (42.347184590861346 steps/sec)\n",
      "Step #7350\tEpoch   2 Batch 1099/3125   Loss: 0.787508 mae: 0.716355 (44.58247404380501 steps/sec)\n",
      "Step #7400\tEpoch   2 Batch 1149/3125   Loss: 0.747142 mae: 0.721394 (43.626172641445066 steps/sec)\n",
      "Step #7450\tEpoch   2 Batch 1199/3125   Loss: 0.822915 mae: 0.713228 (39.66123401987495 steps/sec)\n",
      "Step #7500\tEpoch   2 Batch 1249/3125   Loss: 0.919695 mae: 0.722605 (34.83592587744623 steps/sec)\n",
      "Step #7550\tEpoch   2 Batch 1299/3125   Loss: 0.801139 mae: 0.726354 (38.85127856274397 steps/sec)\n",
      "Step #7600\tEpoch   2 Batch 1349/3125   Loss: 0.848961 mae: 0.705269 (39.50736362375891 steps/sec)\n",
      "Step #7650\tEpoch   2 Batch 1399/3125   Loss: 0.831882 mae: 0.720112 (39.57277333897917 steps/sec)\n",
      "Step #7700\tEpoch   2 Batch 1449/3125   Loss: 0.830785 mae: 0.717888 (38.97808612696308 steps/sec)\n",
      "Step #7750\tEpoch   2 Batch 1499/3125   Loss: 0.848089 mae: 0.716537 (36.966929071122074 steps/sec)\n",
      "Step #7800\tEpoch   2 Batch 1549/3125   Loss: 0.903128 mae: 0.716463 (38.84195286221478 steps/sec)\n",
      "Step #7850\tEpoch   2 Batch 1599/3125   Loss: 0.838099 mae: 0.715277 (43.790499883379965 steps/sec)\n",
      "Step #7900\tEpoch   2 Batch 1649/3125   Loss: 0.857945 mae: 0.718527 (43.56094580015263 steps/sec)\n",
      "Step #7950\tEpoch   2 Batch 1699/3125   Loss: 0.901990 mae: 0.713006 (43.3275636302933 steps/sec)\n",
      "Step #8000\tEpoch   2 Batch 1749/3125   Loss: 0.821461 mae: 0.714430 (43.198669816349955 steps/sec)\n",
      "Step #8050\tEpoch   2 Batch 1799/3125   Loss: 0.936539 mae: 0.719342 (44.48011456670134 steps/sec)\n",
      "Step #8100\tEpoch   2 Batch 1849/3125   Loss: 0.782040 mae: 0.718708 (43.96791183070415 steps/sec)\n",
      "Step #8150\tEpoch   2 Batch 1899/3125   Loss: 0.869985 mae: 0.714692 (43.71443639145452 steps/sec)\n",
      "Step #8200\tEpoch   2 Batch 1949/3125   Loss: 0.802710 mae: 0.711360 (43.309837848733025 steps/sec)\n",
      "Step #8250\tEpoch   2 Batch 1999/3125   Loss: 0.901500 mae: 0.719467 (44.83122917593902 steps/sec)\n",
      "Step #8300\tEpoch   2 Batch 2049/3125   Loss: 0.808797 mae: 0.723916 (44.01636946052058 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #8350\tEpoch   2 Batch 2099/3125   Loss: 0.864681 mae: 0.709579 (43.7806907119431 steps/sec)\n",
      "Step #8400\tEpoch   2 Batch 2149/3125   Loss: 0.752288 mae: 0.708569 (44.34055134758443 steps/sec)\n",
      "Step #8450\tEpoch   2 Batch 2199/3125   Loss: 0.760500 mae: 0.712145 (44.800946113730184 steps/sec)\n",
      "Step #8500\tEpoch   2 Batch 2249/3125   Loss: 0.780273 mae: 0.714550 (41.73421035820959 steps/sec)\n",
      "Step #8550\tEpoch   2 Batch 2299/3125   Loss: 0.876973 mae: 0.724080 (44.322858369882425 steps/sec)\n",
      "Step #8600\tEpoch   2 Batch 2349/3125   Loss: 0.820211 mae: 0.717711 (45.183238470491254 steps/sec)\n",
      "Step #8650\tEpoch   2 Batch 2399/3125   Loss: 0.749523 mae: 0.719608 (44.781965856279314 steps/sec)\n",
      "Step #8700\tEpoch   2 Batch 2449/3125   Loss: 0.814411 mae: 0.708362 (44.304833186541515 steps/sec)\n",
      "Step #8750\tEpoch   2 Batch 2499/3125   Loss: 0.897584 mae: 0.720241 (43.58194786588623 steps/sec)\n",
      "Step #8800\tEpoch   2 Batch 2549/3125   Loss: 0.668645 mae: 0.723283 (44.40221211582273 steps/sec)\n",
      "Step #8850\tEpoch   2 Batch 2599/3125   Loss: 0.970666 mae: 0.719323 (45.08590639752095 steps/sec)\n",
      "Step #8900\tEpoch   2 Batch 2649/3125   Loss: 0.890690 mae: 0.716335 (44.860267293909075 steps/sec)\n",
      "Step #8950\tEpoch   2 Batch 2699/3125   Loss: 0.769611 mae: 0.713840 (44.21628778774347 steps/sec)\n",
      "Step #9000\tEpoch   2 Batch 2749/3125   Loss: 0.967926 mae: 0.714855 (43.84776040326143 steps/sec)\n",
      "Step #9050\tEpoch   2 Batch 2799/3125   Loss: 0.839205 mae: 0.718241 (44.63704644640792 steps/sec)\n",
      "Step #9100\tEpoch   2 Batch 2849/3125   Loss: 0.825163 mae: 0.715651 (45.30196311559869 steps/sec)\n",
      "Step #9150\tEpoch   2 Batch 2899/3125   Loss: 0.842562 mae: 0.714074 (44.81394691866078 steps/sec)\n",
      "Step #9200\tEpoch   2 Batch 2949/3125   Loss: 0.862243 mae: 0.717182 (42.38221296720881 steps/sec)\n",
      "Step #9250\tEpoch   2 Batch 2999/3125   Loss: 0.740986 mae: 0.713924 (43.23497899534304 steps/sec)\n",
      "Step #9300\tEpoch   2 Batch 3049/3125   Loss: 0.780013 mae: 0.706402 (43.80885016683549 steps/sec)\n",
      "Step #9350\tEpoch   2 Batch 3099/3125   Loss: 0.871395 mae: 0.711170 (44.12135543939855 steps/sec)\n",
      "\n",
      "Train time for epoch #3 (9375 total steps): 73.10429382324219\n",
      "Model test set loss: 0.833494 mae: 0.721063\n",
      "best loss = 0.8334940075874329\n",
      "Step #9400\tEpoch   3 Batch   24/3125   Loss: 0.728228 mae: 0.720843 (80.87160702255564 steps/sec)\n",
      "Step #9450\tEpoch   3 Batch   74/3125   Loss: 0.876898 mae: 0.710767 (44.51966928337868 steps/sec)\n",
      "Step #9500\tEpoch   3 Batch  124/3125   Loss: 0.713244 mae: 0.720023 (44.597747689889275 steps/sec)\n",
      "Step #9550\tEpoch   3 Batch  174/3125   Loss: 0.748808 mae: 0.718516 (42.33151632531744 steps/sec)\n",
      "Step #9600\tEpoch   3 Batch  224/3125   Loss: 0.607107 mae: 0.714183 (44.0183373356962 steps/sec)\n",
      "Step #9650\tEpoch   3 Batch  274/3125   Loss: 0.819928 mae: 0.713846 (44.59574663541292 steps/sec)\n",
      "Step #9700\tEpoch   3 Batch  324/3125   Loss: 0.788072 mae: 0.717074 (40.35756093270975 steps/sec)\n",
      "Step #9750\tEpoch   3 Batch  374/3125   Loss: 0.798676 mae: 0.718374 (43.57458578574119 steps/sec)\n",
      "Step #9800\tEpoch   3 Batch  424/3125   Loss: 0.870222 mae: 0.721073 (43.699115162264704 steps/sec)\n",
      "Step #9850\tEpoch   3 Batch  474/3125   Loss: 0.794537 mae: 0.716045 (43.56667409690878 steps/sec)\n",
      "Step #9900\tEpoch   3 Batch  524/3125   Loss: 0.831168 mae: 0.709954 (43.527456839957175 steps/sec)\n",
      "Step #9950\tEpoch   3 Batch  574/3125   Loss: 0.896723 mae: 0.720649 (43.32150427306641 steps/sec)\n",
      "Step #10000\tEpoch   3 Batch  624/3125   Loss: 0.645023 mae: 0.715509 (43.64337724894339 steps/sec)\n",
      "Step #10050\tEpoch   3 Batch  674/3125   Loss: 0.845416 mae: 0.719951 (41.36349605674889 steps/sec)\n",
      "Step #10100\tEpoch   3 Batch  724/3125   Loss: 0.665014 mae: 0.716187 (42.495360702117566 steps/sec)\n",
      "Step #10150\tEpoch   3 Batch  774/3125   Loss: 0.770238 mae: 0.711058 (37.51042551482771 steps/sec)\n",
      "Step #10200\tEpoch   3 Batch  824/3125   Loss: 0.832636 mae: 0.714520 (43.59483975952027 steps/sec)\n",
      "Step #10250\tEpoch   3 Batch  874/3125   Loss: 0.770708 mae: 0.716472 (42.753309267907234 steps/sec)\n",
      "Step #10300\tEpoch   3 Batch  924/3125   Loss: 0.933755 mae: 0.719705 (45.00130144545206 steps/sec)\n",
      "Step #10350\tEpoch   3 Batch  974/3125   Loss: 0.747748 mae: 0.717128 (44.025203179325565 steps/sec)\n",
      "Step #10400\tEpoch   3 Batch 1024/3125   Loss: 0.819607 mae: 0.713739 (43.17487087593549 steps/sec)\n",
      "Step #10450\tEpoch   3 Batch 1074/3125   Loss: 0.859083 mae: 0.715585 (43.63264433982893 steps/sec)\n",
      "Step #10500\tEpoch   3 Batch 1124/3125   Loss: 0.842042 mae: 0.719527 (43.258656879958814 steps/sec)\n",
      "Step #10550\tEpoch   3 Batch 1174/3125   Loss: 0.758652 mae: 0.718933 (43.71684212588954 steps/sec)\n",
      "Step #10600\tEpoch   3 Batch 1224/3125   Loss: 0.758699 mae: 0.711375 (44.95227087899863 steps/sec)\n",
      "Step #10650\tEpoch   3 Batch 1274/3125   Loss: 0.943479 mae: 0.729152 (44.41976152743764 steps/sec)\n",
      "Step #10700\tEpoch   3 Batch 1324/3125   Loss: 0.778779 mae: 0.713557 (43.11456623969139 steps/sec)\n",
      "Step #10750\tEpoch   3 Batch 1374/3125   Loss: 0.797136 mae: 0.712629 (39.34502504428574 steps/sec)\n",
      "Step #10800\tEpoch   3 Batch 1424/3125   Loss: 0.820622 mae: 0.716120 (43.53552600576487 steps/sec)\n",
      "Step #10850\tEpoch   3 Batch 1474/3125   Loss: 0.724393 mae: 0.716876 (44.178582292603224 steps/sec)\n",
      "Step #10900\tEpoch   3 Batch 1524/3125   Loss: 0.737752 mae: 0.712365 (42.46976703872103 steps/sec)\n",
      "Step #10950\tEpoch   3 Batch 1574/3125   Loss: 0.790308 mae: 0.721300 (43.336454351340386 steps/sec)\n",
      "Step #11000\tEpoch   3 Batch 1624/3125   Loss: 0.909394 mae: 0.709891 (44.615565938388606 steps/sec)\n",
      "Step #11050\tEpoch   3 Batch 1674/3125   Loss: 0.778782 mae: 0.717332 (40.46719336455597 steps/sec)\n",
      "Step #11100\tEpoch   3 Batch 1724/3125   Loss: 0.917246 mae: 0.714307 (43.576641122396396 steps/sec)\n",
      "Step #11150\tEpoch   3 Batch 1774/3125   Loss: 0.752520 mae: 0.714567 (41.15270235134247 steps/sec)\n",
      "Step #11200\tEpoch   3 Batch 1824/3125   Loss: 0.738230 mae: 0.717946 (42.593173684485244 steps/sec)\n",
      "Step #11250\tEpoch   3 Batch 1874/3125   Loss: 0.903056 mae: 0.716507 (44.75285708249592 steps/sec)\n",
      "Step #11300\tEpoch   3 Batch 1924/3125   Loss: 0.994895 mae: 0.713746 (44.085831857430406 steps/sec)\n",
      "Step #11350\tEpoch   3 Batch 1974/3125   Loss: 0.822656 mae: 0.711577 (43.39702252198096 steps/sec)\n",
      "Step #11400\tEpoch   3 Batch 2024/3125   Loss: 0.870622 mae: 0.720685 (41.69958945683312 steps/sec)\n",
      "Step #11450\tEpoch   3 Batch 2074/3125   Loss: 0.887528 mae: 0.712090 (44.35718887601489 steps/sec)\n",
      "Step #11500\tEpoch   3 Batch 2124/3125   Loss: 0.655472 mae: 0.715918 (44.02548044545435 steps/sec)\n",
      "Step #11550\tEpoch   3 Batch 2174/3125   Loss: 0.723356 mae: 0.707361 (43.0559060024452 steps/sec)\n",
      "Step #11600\tEpoch   3 Batch 2224/3125   Loss: 0.715262 mae: 0.713957 (43.84638527704537 steps/sec)\n",
      "Step #11650\tEpoch   3 Batch 2274/3125   Loss: 0.843069 mae: 0.715137 (43.329730015636414 steps/sec)\n",
      "Step #11700\tEpoch   3 Batch 2324/3125   Loss: 0.686868 mae: 0.723544 (43.454149352917845 steps/sec)\n",
      "Step #11750\tEpoch   3 Batch 2374/3125   Loss: 0.786079 mae: 0.714530 (44.54164405586311 steps/sec)\n",
      "Step #11800\tEpoch   3 Batch 2424/3125   Loss: 0.775777 mae: 0.715732 (43.32053779675361 steps/sec)\n",
      "Step #11850\tEpoch   3 Batch 2474/3125   Loss: 0.885817 mae: 0.709730 (43.751344312309804 steps/sec)\n",
      "Step #11900\tEpoch   3 Batch 2524/3125   Loss: 0.777747 mae: 0.720957 (41.75399461398572 steps/sec)\n",
      "Step #11950\tEpoch   3 Batch 2574/3125   Loss: 0.890056 mae: 0.721469 (43.598936321269065 steps/sec)\n",
      "Step #12000\tEpoch   3 Batch 2624/3125   Loss: 0.954216 mae: 0.719872 (43.72185492317853 steps/sec)\n",
      "Step #12050\tEpoch   3 Batch 2674/3125   Loss: 0.827686 mae: 0.716178 (43.64666537213838 steps/sec)\n",
      "Step #12100\tEpoch   3 Batch 2724/3125   Loss: 0.796163 mae: 0.708371 (42.94342424516072 steps/sec)\n",
      "Step #12150\tEpoch   3 Batch 2774/3125   Loss: 0.769520 mae: 0.715513 (44.96985286896302 steps/sec)\n",
      "Step #12200\tEpoch   3 Batch 2824/3125   Loss: 0.854162 mae: 0.723245 (45.08500498005934 steps/sec)\n",
      "Step #12250\tEpoch   3 Batch 2874/3125   Loss: 0.858618 mae: 0.708356 (45.2038758274652 steps/sec)\n",
      "Step #12300\tEpoch   3 Batch 2924/3125   Loss: 0.845399 mae: 0.714975 (43.780882647973144 steps/sec)\n",
      "Step #12350\tEpoch   3 Batch 2974/3125   Loss: 0.729756 mae: 0.716611 (43.13617805784436 steps/sec)\n",
      "Step #12400\tEpoch   3 Batch 3024/3125   Loss: 0.706613 mae: 0.708289 (41.291298150000394 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #12450\tEpoch   3 Batch 3074/3125   Loss: 0.831556 mae: 0.707980 (41.907728547925515 steps/sec)\n",
      "Step #12500\tEpoch   3 Batch 3124/3125   Loss: 0.862898 mae: 0.710178 (42.26317509873792 steps/sec)\n",
      "\n",
      "Train time for epoch #4 (12500 total steps): 72.45296883583069\n",
      "Model test set loss: 0.832952 mae: 0.720870\n",
      "best loss = 0.832951545715332\n",
      "Step #12550\tEpoch   4 Batch   49/3125   Loss: 0.915907 mae: 0.720536 (42.10831513691238 steps/sec)\n",
      "Step #12600\tEpoch   4 Batch   99/3125   Loss: 1.003366 mae: 0.708283 (43.71091939265193 steps/sec)\n",
      "Step #12650\tEpoch   4 Batch  149/3125   Loss: 0.779118 mae: 0.720564 (44.07636238526422 steps/sec)\n",
      "Step #12700\tEpoch   4 Batch  199/3125   Loss: 0.872413 mae: 0.716640 (43.68888270395625 steps/sec)\n",
      "Step #12750\tEpoch   4 Batch  249/3125   Loss: 0.821701 mae: 0.709481 (43.8307698963583 steps/sec)\n",
      "Step #12800\tEpoch   4 Batch  299/3125   Loss: 0.787222 mae: 0.716223 (45.13828921710008 steps/sec)\n",
      "Step #12850\tEpoch   4 Batch  349/3125   Loss: 0.900534 mae: 0.717411 (43.33176231781885 steps/sec)\n",
      "Step #12900\tEpoch   4 Batch  399/3125   Loss: 0.928592 mae: 0.716784 (42.0392763113319 steps/sec)\n",
      "Step #12950\tEpoch   4 Batch  449/3125   Loss: 0.803740 mae: 0.723054 (44.480265513303664 steps/sec)\n",
      "Step #13000\tEpoch   4 Batch  499/3125   Loss: 0.797712 mae: 0.710133 (44.012295686624945 steps/sec)\n",
      "Step #13050\tEpoch   4 Batch  549/3125   Loss: 0.787064 mae: 0.709628 (40.7592527195646 steps/sec)\n",
      "Step #13100\tEpoch   4 Batch  599/3125   Loss: 0.722527 mae: 0.723403 (42.99950339890846 steps/sec)\n",
      "Step #13150\tEpoch   4 Batch  649/3125   Loss: 0.779508 mae: 0.718892 (44.434170684768446 steps/sec)\n",
      "Step #13200\tEpoch   4 Batch  699/3125   Loss: 0.775882 mae: 0.718375 (43.3081206236826 steps/sec)\n",
      "Step #13250\tEpoch   4 Batch  749/3125   Loss: 0.786662 mae: 0.706696 (44.1020932570747 steps/sec)\n",
      "Step #13300\tEpoch   4 Batch  799/3125   Loss: 0.896117 mae: 0.716879 (43.63021155478107 steps/sec)\n",
      "Step #13350\tEpoch   4 Batch  849/3125   Loss: 0.948204 mae: 0.715633 (44.0091646587323 steps/sec)\n",
      "Step #13400\tEpoch   4 Batch  899/3125   Loss: 0.886038 mae: 0.714615 (43.07036369946992 steps/sec)\n",
      "Step #13450\tEpoch   4 Batch  949/3125   Loss: 0.707839 mae: 0.714622 (43.674343380828084 steps/sec)\n",
      "Step #13500\tEpoch   4 Batch  999/3125   Loss: 0.864498 mae: 0.713883 (40.54384749658438 steps/sec)\n",
      "Step #13550\tEpoch   4 Batch 1049/3125   Loss: 0.886506 mae: 0.720058 (41.24742689389067 steps/sec)\n",
      "Step #13600\tEpoch   4 Batch 1099/3125   Loss: 0.788494 mae: 0.714290 (42.31002019112978 steps/sec)\n",
      "Step #13650\tEpoch   4 Batch 1149/3125   Loss: 0.743435 mae: 0.719434 (44.087629848841054 steps/sec)\n",
      "Step #13700\tEpoch   4 Batch 1199/3125   Loss: 0.823402 mae: 0.711907 (43.890340283493884 steps/sec)\n",
      "Step #13750\tEpoch   4 Batch 1249/3125   Loss: 0.913216 mae: 0.721002 (44.89918433707958 steps/sec)\n",
      "Step #13800\tEpoch   4 Batch 1299/3125   Loss: 0.793397 mae: 0.724495 (42.95695296073216 steps/sec)\n",
      "Step #13850\tEpoch   4 Batch 1349/3125   Loss: 0.845385 mae: 0.703604 (44.32207151039179 steps/sec)\n",
      "Step #13900\tEpoch   4 Batch 1399/3125   Loss: 0.830742 mae: 0.718710 (44.09194006453735 steps/sec)\n",
      "Step #13950\tEpoch   4 Batch 1449/3125   Loss: 0.833814 mae: 0.716296 (45.025610351232274 steps/sec)\n",
      "Step #14000\tEpoch   4 Batch 1499/3125   Loss: 0.843671 mae: 0.715324 (44.302399740077625 steps/sec)\n",
      "Step #14050\tEpoch   4 Batch 1549/3125   Loss: 0.897066 mae: 0.714905 (44.80966673383 steps/sec)\n",
      "Step #14100\tEpoch   4 Batch 1599/3125   Loss: 0.834050 mae: 0.713928 (44.56295873199629 steps/sec)\n",
      "Step #14150\tEpoch   4 Batch 1649/3125   Loss: 0.851795 mae: 0.717193 (44.18994864265355 steps/sec)\n",
      "Step #14200\tEpoch   4 Batch 1699/3125   Loss: 0.903257 mae: 0.711371 (44.32521911599473 steps/sec)\n",
      "Step #14250\tEpoch   4 Batch 1749/3125   Loss: 0.819438 mae: 0.712981 (41.62875951714933 steps/sec)\n",
      "Step #14300\tEpoch   4 Batch 1799/3125   Loss: 0.932420 mae: 0.717557 (43.76276582336065 steps/sec)\n",
      "Step #14350\tEpoch   4 Batch 1849/3125   Loss: 0.784914 mae: 0.717125 (44.12322131293784 steps/sec)\n",
      "Step #14400\tEpoch   4 Batch 1899/3125   Loss: 0.871434 mae: 0.713062 (44.598971171842884 steps/sec)\n",
      "Step #14450\tEpoch   4 Batch 1949/3125   Loss: 0.799403 mae: 0.710203 (41.55685217275855 steps/sec)\n",
      "Step #14500\tEpoch   4 Batch 1999/3125   Loss: 0.899465 mae: 0.717993 (42.98855607457861 steps/sec)\n",
      "Step #14550\tEpoch   4 Batch 2049/3125   Loss: 0.803531 mae: 0.722633 (41.09229793734359 steps/sec)\n",
      "Step #14600\tEpoch   4 Batch 2099/3125   Loss: 0.862325 mae: 0.708173 (42.721502677488665 steps/sec)\n",
      "Step #14650\tEpoch   4 Batch 2149/3125   Loss: 0.747528 mae: 0.706777 (43.2020336650674 steps/sec)\n",
      "Step #14700\tEpoch   4 Batch 2199/3125   Loss: 0.758940 mae: 0.710574 (43.39554980576881 steps/sec)\n",
      "Step #14750\tEpoch   4 Batch 2249/3125   Loss: 0.775095 mae: 0.712671 (43.964003700507135 steps/sec)\n",
      "Step #14800\tEpoch   4 Batch 2299/3125   Loss: 0.876860 mae: 0.722418 (43.3906025704741 steps/sec)\n",
      "Step #14850\tEpoch   4 Batch 2349/3125   Loss: 0.818650 mae: 0.716553 (44.06622103544122 steps/sec)\n",
      "Step #14900\tEpoch   4 Batch 2399/3125   Loss: 0.747429 mae: 0.718060 (43.11057791533425 steps/sec)\n",
      "Step #14950\tEpoch   4 Batch 2449/3125   Loss: 0.809120 mae: 0.706672 (42.00226319109945 steps/sec)\n",
      "Step #15000\tEpoch   4 Batch 2499/3125   Loss: 0.898108 mae: 0.719159 (44.11378217027864 steps/sec)\n",
      "Step #15050\tEpoch   4 Batch 2549/3125   Loss: 0.661609 mae: 0.721781 (44.56288297768129 steps/sec)\n",
      "Step #15100\tEpoch   4 Batch 2599/3125   Loss: 0.965112 mae: 0.717678 (45.06696482775116 steps/sec)\n",
      "Step #15150\tEpoch   4 Batch 2649/3125   Loss: 0.888874 mae: 0.714864 (44.490551163395565 steps/sec)\n",
      "Step #15200\tEpoch   4 Batch 2699/3125   Loss: 0.765438 mae: 0.711970 (45.576369984626425 steps/sec)\n",
      "Step #15250\tEpoch   4 Batch 2749/3125   Loss: 0.959819 mae: 0.713429 (44.43318216835163 steps/sec)\n",
      "Step #15300\tEpoch   4 Batch 2799/3125   Loss: 0.838581 mae: 0.716804 (45.208485041322774 steps/sec)\n",
      "Step #15350\tEpoch   4 Batch 2849/3125   Loss: 0.817785 mae: 0.714036 (44.914473791294974 steps/sec)\n",
      "Step #15400\tEpoch   4 Batch 2899/3125   Loss: 0.835635 mae: 0.712379 (44.72988159704604 steps/sec)\n",
      "Step #15450\tEpoch   4 Batch 2949/3125   Loss: 0.858092 mae: 0.715665 (43.64850039659604 steps/sec)\n",
      "Step #15500\tEpoch   4 Batch 2999/3125   Loss: 0.744189 mae: 0.712669 (44.20703243490159 steps/sec)\n",
      "Step #15550\tEpoch   4 Batch 3049/3125   Loss: 0.774035 mae: 0.704885 (44.47968060092635 steps/sec)\n",
      "Step #15600\tEpoch   4 Batch 3099/3125   Loss: 0.869492 mae: 0.709654 (44.300322162729884 steps/sec)\n",
      "\n",
      "Train time for epoch #5 (15625 total steps): 71.68464207649231\n",
      "Model test set loss: 0.832750 mae: 0.720337\n",
      "best loss = 0.8327499032020569\n",
      "Step #15650\tEpoch   5 Batch   24/3125   Loss: 0.725072 mae: 0.720089 (83.66590480491539 steps/sec)\n",
      "Step #15700\tEpoch   5 Batch   74/3125   Loss: 0.870405 mae: 0.709433 (41.42032379773732 steps/sec)\n",
      "Step #15750\tEpoch   5 Batch  124/3125   Loss: 0.707983 mae: 0.718451 (43.3375021388204 steps/sec)\n",
      "Step #15800\tEpoch   5 Batch  174/3125   Loss: 0.746899 mae: 0.717103 (42.54117746337048 steps/sec)\n",
      "Step #15850\tEpoch   5 Batch  224/3125   Loss: 0.602068 mae: 0.712911 (42.25146726070973 steps/sec)\n",
      "Step #15900\tEpoch   5 Batch  274/3125   Loss: 0.815379 mae: 0.712735 (43.68594312406638 steps/sec)\n",
      "Step #15950\tEpoch   5 Batch  324/3125   Loss: 0.786550 mae: 0.715615 (43.74364203982865 steps/sec)\n",
      "Step #16000\tEpoch   5 Batch  374/3125   Loss: 0.795269 mae: 0.716797 (43.27977926229525 steps/sec)\n",
      "Step #16050\tEpoch   5 Batch  424/3125   Loss: 0.868001 mae: 0.719937 (42.89224539049158 steps/sec)\n",
      "Step #16100\tEpoch   5 Batch  474/3125   Loss: 0.791958 mae: 0.714816 (41.665936522563044 steps/sec)\n",
      "Step #16150\tEpoch   5 Batch  524/3125   Loss: 0.828981 mae: 0.708560 (43.63771956692546 steps/sec)\n",
      "Step #16200\tEpoch   5 Batch  574/3125   Loss: 0.895862 mae: 0.719218 (44.12015802160784 steps/sec)\n",
      "Step #16250\tEpoch   5 Batch  624/3125   Loss: 0.641096 mae: 0.714109 (44.78853633257381 steps/sec)\n",
      "Step #16300\tEpoch   5 Batch  674/3125   Loss: 0.844106 mae: 0.718549 (41.77144291251539 steps/sec)\n",
      "Step #16350\tEpoch   5 Batch  724/3125   Loss: 0.660033 mae: 0.714756 (43.478266277926856 steps/sec)\n",
      "Step #16400\tEpoch   5 Batch  774/3125   Loss: 0.769190 mae: 0.709980 (43.125684264728505 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #16450\tEpoch   5 Batch  824/3125   Loss: 0.822619 mae: 0.713435 (43.35596762906355 steps/sec)\n",
      "Step #16500\tEpoch   5 Batch  874/3125   Loss: 0.766582 mae: 0.715278 (39.27628253425844 steps/sec)\n",
      "Step #16550\tEpoch   5 Batch  924/3125   Loss: 0.933172 mae: 0.718309 (42.76834939188508 steps/sec)\n",
      "Step #16600\tEpoch   5 Batch  974/3125   Loss: 0.746866 mae: 0.716058 (44.17701882980041 steps/sec)\n",
      "Step #16650\tEpoch   5 Batch 1024/3125   Loss: 0.819253 mae: 0.712211 (44.21363102108797 steps/sec)\n",
      "Step #16700\tEpoch   5 Batch 1074/3125   Loss: 0.858333 mae: 0.714133 (43.567171887354306 steps/sec)\n",
      "Step #16750\tEpoch   5 Batch 1124/3125   Loss: 0.839687 mae: 0.718108 (44.02435291830705 steps/sec)\n",
      "Step #16800\tEpoch   5 Batch 1174/3125   Loss: 0.754720 mae: 0.717554 (45.149659087728665 steps/sec)\n",
      "Step #16850\tEpoch   5 Batch 1224/3125   Loss: 0.755690 mae: 0.710242 (44.87690346558542 steps/sec)\n",
      "Step #16900\tEpoch   5 Batch 1274/3125   Loss: 0.944556 mae: 0.727871 (45.1319750963585 steps/sec)\n",
      "Step #16950\tEpoch   5 Batch 1324/3125   Loss: 0.772133 mae: 0.712057 (45.038877309632184 steps/sec)\n",
      "Step #17000\tEpoch   5 Batch 1374/3125   Loss: 0.794717 mae: 0.711526 (45.23702851581315 steps/sec)\n",
      "Step #17050\tEpoch   5 Batch 1424/3125   Loss: 0.817295 mae: 0.715450 (44.92669360501908 steps/sec)\n",
      "Step #17100\tEpoch   5 Batch 1474/3125   Loss: 0.723583 mae: 0.715800 (44.789779871090815 steps/sec)\n",
      "Step #17150\tEpoch   5 Batch 1524/3125   Loss: 0.738033 mae: 0.711253 (45.567932582599745 steps/sec)\n",
      "Step #17200\tEpoch   5 Batch 1574/3125   Loss: 0.790969 mae: 0.720067 (43.665422594229376 steps/sec)\n",
      "Step #17250\tEpoch   5 Batch 1624/3125   Loss: 0.907848 mae: 0.709038 (43.49183648752453 steps/sec)\n",
      "Step #17300\tEpoch   5 Batch 1674/3125   Loss: 0.771313 mae: 0.716500 (44.14878388073577 steps/sec)\n",
      "Step #17350\tEpoch   5 Batch 1724/3125   Loss: 0.920391 mae: 0.713171 (43.890992471471876 steps/sec)\n",
      "Step #17400\tEpoch   5 Batch 1774/3125   Loss: 0.749847 mae: 0.713747 (44.66805013279102 steps/sec)\n",
      "Step #17450\tEpoch   5 Batch 1824/3125   Loss: 0.736534 mae: 0.716833 (44.83966438329511 steps/sec)\n",
      "Step #17500\tEpoch   5 Batch 1874/3125   Loss: 0.900764 mae: 0.715643 (45.455702109646296 steps/sec)\n",
      "Step #17550\tEpoch   5 Batch 1924/3125   Loss: 0.994306 mae: 0.712588 (44.9957592907792 steps/sec)\n",
      "Step #17600\tEpoch   5 Batch 1974/3125   Loss: 0.819384 mae: 0.710549 (44.975031766541385 steps/sec)\n",
      "Step #17650\tEpoch   5 Batch 2024/3125   Loss: 0.870374 mae: 0.719617 (45.244933830035066 steps/sec)\n",
      "Step #17700\tEpoch   5 Batch 2074/3125   Loss: 0.886744 mae: 0.711005 (44.78918679026191 steps/sec)\n",
      "Step #17750\tEpoch   5 Batch 2124/3125   Loss: 0.655224 mae: 0.714902 (45.164827828304254 steps/sec)\n",
      "Step #17800\tEpoch   5 Batch 2174/3125   Loss: 0.721171 mae: 0.706203 (45.345758511227395 steps/sec)\n",
      "Step #17850\tEpoch   5 Batch 2224/3125   Loss: 0.711917 mae: 0.712862 (46.01738220667307 steps/sec)\n",
      "Step #17900\tEpoch   5 Batch 2274/3125   Loss: 0.839102 mae: 0.713787 (45.20304763284249 steps/sec)\n",
      "Step #17950\tEpoch   5 Batch 2324/3125   Loss: 0.685187 mae: 0.722527 (42.672031924586875 steps/sec)\n",
      "Step #18000\tEpoch   5 Batch 2374/3125   Loss: 0.781459 mae: 0.713520 (45.20138159206393 steps/sec)\n",
      "Step #18050\tEpoch   5 Batch 2424/3125   Loss: 0.773403 mae: 0.714324 (44.9066835231183 steps/sec)\n",
      "Step #18100\tEpoch   5 Batch 2474/3125   Loss: 0.879377 mae: 0.708738 (44.75334414705178 steps/sec)\n",
      "Step #18150\tEpoch   5 Batch 2524/3125   Loss: 0.775706 mae: 0.719859 (44.727725575515805 steps/sec)\n",
      "Step #18200\tEpoch   5 Batch 2574/3125   Loss: 0.889956 mae: 0.720278 (44.58938428585826 steps/sec)\n",
      "Step #18250\tEpoch   5 Batch 2624/3125   Loss: 0.949745 mae: 0.718574 (44.51464196329093 steps/sec)\n",
      "Step #18300\tEpoch   5 Batch 2674/3125   Loss: 0.822166 mae: 0.715166 (44.99948609462167 steps/sec)\n",
      "Step #18350\tEpoch   5 Batch 2724/3125   Loss: 0.795734 mae: 0.706927 (45.16687055255949 steps/sec)\n",
      "Step #18400\tEpoch   5 Batch 2774/3125   Loss: 0.765352 mae: 0.714394 (44.67261732954648 steps/sec)\n",
      "Step #18450\tEpoch   5 Batch 2824/3125   Loss: 0.851993 mae: 0.722128 (44.373482111609945 steps/sec)\n",
      "Step #18500\tEpoch   5 Batch 2874/3125   Loss: 0.855628 mae: 0.707245 (44.142669206563035 steps/sec)\n",
      "Step #18550\tEpoch   5 Batch 2924/3125   Loss: 0.849174 mae: 0.713577 (45.12798354077718 steps/sec)\n",
      "Step #18600\tEpoch   5 Batch 2974/3125   Loss: 0.722621 mae: 0.715290 (45.26853932875313 steps/sec)\n",
      "Step #18650\tEpoch   5 Batch 3024/3125   Loss: 0.705342 mae: 0.707183 (41.747935199638015 steps/sec)\n",
      "Step #18700\tEpoch   5 Batch 3074/3125   Loss: 0.827076 mae: 0.707002 (45.1896058941765 steps/sec)\n",
      "Step #18750\tEpoch   5 Batch 3124/3125   Loss: 0.857953 mae: 0.709054 (44.88721006291031 steps/sec)\n",
      "\n",
      "Train time for epoch #6 (18750 total steps): 70.94819188117981\n",
      "Model test set loss: 0.832808 mae: 0.720400\n",
      "Step #18800\tEpoch   6 Batch   49/3125   Loss: 0.915789 mae: 0.720024 (43.21780089887335 steps/sec)\n",
      "Step #18850\tEpoch   6 Batch   99/3125   Loss: 0.999555 mae: 0.707039 (36.75365862549527 steps/sec)\n",
      "Step #18900\tEpoch   6 Batch  149/3125   Loss: 0.777169 mae: 0.719248 (43.24119248791829 steps/sec)\n",
      "Step #18950\tEpoch   6 Batch  199/3125   Loss: 0.870424 mae: 0.715694 (43.03985032986618 steps/sec)\n",
      "Step #19000\tEpoch   6 Batch  249/3125   Loss: 0.819377 mae: 0.708369 (43.49823229940046 steps/sec)\n",
      "Step #19050\tEpoch   6 Batch  299/3125   Loss: 0.783886 mae: 0.715113 (44.353924160075394 steps/sec)\n",
      "Step #19100\tEpoch   6 Batch  349/3125   Loss: 0.902926 mae: 0.716252 (44.39633719717891 steps/sec)\n",
      "Step #19150\tEpoch   6 Batch  399/3125   Loss: 0.928274 mae: 0.715439 (43.662558894378414 steps/sec)\n",
      "Step #19200\tEpoch   6 Batch  449/3125   Loss: 0.802958 mae: 0.721901 (41.58312145059651 steps/sec)\n",
      "Step #19250\tEpoch   6 Batch  499/3125   Loss: 0.797366 mae: 0.709153 (43.59657979789034 steps/sec)\n",
      "Step #19300\tEpoch   6 Batch  549/3125   Loss: 0.782609 mae: 0.708336 (44.27587380453777 steps/sec)\n",
      "Step #19350\tEpoch   6 Batch  599/3125   Loss: 0.719769 mae: 0.722051 (42.72382647356978 steps/sec)\n",
      "Step #19400\tEpoch   6 Batch  649/3125   Loss: 0.778386 mae: 0.717718 (43.727662090781386 steps/sec)\n",
      "Step #19450\tEpoch   6 Batch  699/3125   Loss: 0.774783 mae: 0.716917 (43.84696281955052 steps/sec)\n",
      "Step #19500\tEpoch   6 Batch  749/3125   Loss: 0.784711 mae: 0.705616 (42.436817837631544 steps/sec)\n",
      "Step #19550\tEpoch   6 Batch  799/3125   Loss: 0.896141 mae: 0.715973 (40.904411543843345 steps/sec)\n",
      "Step #19600\tEpoch   6 Batch  849/3125   Loss: 0.946091 mae: 0.714816 (43.07894563859857 steps/sec)\n",
      "Step #19650\tEpoch   6 Batch  899/3125   Loss: 0.884057 mae: 0.713225 (43.570548111840836 steps/sec)\n",
      "Step #19700\tEpoch   6 Batch  949/3125   Loss: 0.707257 mae: 0.713843 (43.90039164182054 steps/sec)\n",
      "Step #19750\tEpoch   6 Batch  999/3125   Loss: 0.862535 mae: 0.712887 (43.86438791965681 steps/sec)\n",
      "Step #19800\tEpoch   6 Batch 1049/3125   Loss: 0.882237 mae: 0.718951 (44.00699444360158 steps/sec)\n",
      "Step #19850\tEpoch   6 Batch 1099/3125   Loss: 0.786954 mae: 0.713032 (43.63500476371495 steps/sec)\n",
      "Step #19900\tEpoch   6 Batch 1149/3125   Loss: 0.742869 mae: 0.718045 (43.9312819588957 steps/sec)\n",
      "Step #19950\tEpoch   6 Batch 1199/3125   Loss: 0.820415 mae: 0.710923 (43.278323423927816 steps/sec)\n",
      "Step #20000\tEpoch   6 Batch 1249/3125   Loss: 0.913433 mae: 0.719674 (42.51064209322879 steps/sec)\n",
      "Step #20050\tEpoch   6 Batch 1299/3125   Loss: 0.788932 mae: 0.723226 (44.32615598970575 steps/sec)\n",
      "Step #20100\tEpoch   6 Batch 1349/3125   Loss: 0.839576 mae: 0.702469 (44.226414394032375 steps/sec)\n",
      "Step #20150\tEpoch   6 Batch 1399/3125   Loss: 0.830611 mae: 0.717866 (43.940238064657656 steps/sec)\n",
      "Step #20200\tEpoch   6 Batch 1449/3125   Loss: 0.834769 mae: 0.715608 (39.22420629576761 steps/sec)\n",
      "Step #20250\tEpoch   6 Batch 1499/3125   Loss: 0.843202 mae: 0.714341 (41.1981769605532 steps/sec)\n",
      "Step #20300\tEpoch   6 Batch 1549/3125   Loss: 0.890635 mae: 0.713841 (44.43938701400195 steps/sec)\n",
      "Step #20350\tEpoch   6 Batch 1599/3125   Loss: 0.835937 mae: 0.713173 (43.79652652097842 steps/sec)\n",
      "Step #20400\tEpoch   6 Batch 1649/3125   Loss: 0.848195 mae: 0.716439 (43.55962479540715 steps/sec)\n",
      "Step #20450\tEpoch   6 Batch 1699/3125   Loss: 0.900585 mae: 0.710702 (42.29947227316258 steps/sec)\n",
      "Step #20500\tEpoch   6 Batch 1749/3125   Loss: 0.817971 mae: 0.712030 (42.86737199538775 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #20550\tEpoch   6 Batch 1799/3125   Loss: 0.928103 mae: 0.716933 (44.36259359998037 steps/sec)\n",
      "Step #20600\tEpoch   6 Batch 1849/3125   Loss: 0.785035 mae: 0.716325 (43.72768944372903 steps/sec)\n",
      "Step #20650\tEpoch   6 Batch 1899/3125   Loss: 0.872059 mae: 0.712117 (44.08833425659149 steps/sec)\n",
      "Step #20700\tEpoch   6 Batch 1949/3125   Loss: 0.797256 mae: 0.709323 (44.30086493440377 steps/sec)\n",
      "Step #20750\tEpoch   6 Batch 1999/3125   Loss: 0.894687 mae: 0.716915 (42.23276521593435 steps/sec)\n",
      "Step #20800\tEpoch   6 Batch 2049/3125   Loss: 0.802527 mae: 0.721675 (40.90046266635696 steps/sec)\n",
      "Step #20850\tEpoch   6 Batch 2099/3125   Loss: 0.854894 mae: 0.707421 (42.89043831382645 steps/sec)\n",
      "Step #20900\tEpoch   6 Batch 2149/3125   Loss: 0.747558 mae: 0.705896 (40.97615036551516 steps/sec)\n",
      "Step #20950\tEpoch   6 Batch 2199/3125   Loss: 0.760924 mae: 0.709860 (42.97456712972829 steps/sec)\n",
      "Step #21000\tEpoch   6 Batch 2249/3125   Loss: 0.772928 mae: 0.711683 (44.87895864394476 steps/sec)\n",
      "Step #21050\tEpoch   6 Batch 2299/3125   Loss: 0.875924 mae: 0.721330 (44.30860555691479 steps/sec)\n",
      "Step #21100\tEpoch   6 Batch 2349/3125   Loss: 0.818074 mae: 0.715600 (42.450948619857144 steps/sec)\n",
      "Step #21150\tEpoch   6 Batch 2399/3125   Loss: 0.745816 mae: 0.717095 (23.183965862232885 steps/sec)\n",
      "Step #21200\tEpoch   6 Batch 2449/3125   Loss: 0.807442 mae: 0.705465 (42.722964811825804 steps/sec)\n",
      "Step #21250\tEpoch   6 Batch 2499/3125   Loss: 0.893368 mae: 0.718145 (43.55890099116838 steps/sec)\n",
      "Step #21300\tEpoch   6 Batch 2549/3125   Loss: 0.660477 mae: 0.720940 (44.14710171180242 steps/sec)\n",
      "Step #21350\tEpoch   6 Batch 2599/3125   Loss: 0.966343 mae: 0.716714 (43.58456549005182 steps/sec)\n",
      "Step #21400\tEpoch   6 Batch 2649/3125   Loss: 0.885988 mae: 0.713887 (41.98015885646088 steps/sec)\n",
      "Step #21450\tEpoch   6 Batch 2699/3125   Loss: 0.764239 mae: 0.711022 (41.05973827439818 steps/sec)\n",
      "Step #21500\tEpoch   6 Batch 2749/3125   Loss: 0.953597 mae: 0.712331 (40.98266052967959 steps/sec)\n",
      "Step #21550\tEpoch   6 Batch 2799/3125   Loss: 0.836519 mae: 0.715848 (38.21385672136118 steps/sec)\n",
      "Step #21600\tEpoch   6 Batch 2849/3125   Loss: 0.812497 mae: 0.712921 (39.28383841032807 steps/sec)\n",
      "Step #21650\tEpoch   6 Batch 2899/3125   Loss: 0.831818 mae: 0.711298 (38.11542083226632 steps/sec)\n",
      "Step #21700\tEpoch   6 Batch 2949/3125   Loss: 0.853745 mae: 0.714555 (36.77106472483228 steps/sec)\n",
      "Step #21750\tEpoch   6 Batch 2999/3125   Loss: 0.743371 mae: 0.711682 (37.347630663156444 steps/sec)\n",
      "Step #21800\tEpoch   6 Batch 3049/3125   Loss: 0.770066 mae: 0.703789 (35.74468926446784 steps/sec)\n",
      "Step #21850\tEpoch   6 Batch 3099/3125   Loss: 0.865275 mae: 0.708637 (35.57774097549572 steps/sec)\n",
      "\n",
      "Train time for epoch #7 (21875 total steps): 75.05656504631042\n",
      "Model test set loss: 0.833072 mae: 0.720078\n",
      "Step #21900\tEpoch   7 Batch   24/3125   Loss: 0.720984 mae: 0.719803 (53.805162971389635 steps/sec)\n",
      "Step #21950\tEpoch   7 Batch   74/3125   Loss: 0.863396 mae: 0.708516 (26.194992784720622 steps/sec)\n",
      "Step #22000\tEpoch   7 Batch  124/3125   Loss: 0.702407 mae: 0.717303 (28.032230584943598 steps/sec)\n",
      "Step #22050\tEpoch   7 Batch  174/3125   Loss: 0.745222 mae: 0.716136 (27.350784051149173 steps/sec)\n",
      "Step #22100\tEpoch   7 Batch  224/3125   Loss: 0.599917 mae: 0.711862 (25.605635615358214 steps/sec)\n",
      "Step #22150\tEpoch   7 Batch  274/3125   Loss: 0.812629 mae: 0.711722 (24.82726648404499 steps/sec)\n",
      "Step #22200\tEpoch   7 Batch  324/3125   Loss: 0.784999 mae: 0.714548 (25.1241891986276 steps/sec)\n",
      "Step #22250\tEpoch   7 Batch  374/3125   Loss: 0.794024 mae: 0.715535 (23.128466950746443 steps/sec)\n",
      "Step #22300\tEpoch   7 Batch  424/3125   Loss: 0.867028 mae: 0.718727 (24.480955558904533 steps/sec)\n",
      "Step #22350\tEpoch   7 Batch  474/3125   Loss: 0.787872 mae: 0.713697 (25.310856218566634 steps/sec)\n",
      "Step #22400\tEpoch   7 Batch  524/3125   Loss: 0.826467 mae: 0.707437 (41.864159550574804 steps/sec)\n",
      "Step #22450\tEpoch   7 Batch  574/3125   Loss: 0.891850 mae: 0.718003 (42.090229532675764 steps/sec)\n",
      "Step #22500\tEpoch   7 Batch  624/3125   Loss: 0.638530 mae: 0.712824 (41.85830205332325 steps/sec)\n",
      "Step #22550\tEpoch   7 Batch  674/3125   Loss: 0.843496 mae: 0.717141 (41.97965465478991 steps/sec)\n",
      "Step #22600\tEpoch   7 Batch  724/3125   Loss: 0.654858 mae: 0.713392 (34.825293319973255 steps/sec)\n",
      "Step #22650\tEpoch   7 Batch  774/3125   Loss: 0.769735 mae: 0.709077 (37.72812889599701 steps/sec)\n",
      "Step #22700\tEpoch   7 Batch  824/3125   Loss: 0.816113 mae: 0.712356 (38.997329889965584 steps/sec)\n",
      "Step #22750\tEpoch   7 Batch  874/3125   Loss: 0.765324 mae: 0.714199 (37.59689866891168 steps/sec)\n",
      "Step #22800\tEpoch   7 Batch  924/3125   Loss: 0.933384 mae: 0.717166 (38.833170938376206 steps/sec)\n",
      "Step #22850\tEpoch   7 Batch  974/3125   Loss: 0.747247 mae: 0.715296 (34.46053453830749 steps/sec)\n",
      "Step #22900\tEpoch   7 Batch 1024/3125   Loss: 0.817760 mae: 0.711017 (39.30845377898929 steps/sec)\n",
      "Step #22950\tEpoch   7 Batch 1074/3125   Loss: 0.857519 mae: 0.713258 (38.26599893658523 steps/sec)\n",
      "Step #23000\tEpoch   7 Batch 1124/3125   Loss: 0.837283 mae: 0.716703 (40.22791480663611 steps/sec)\n",
      "Step #23050\tEpoch   7 Batch 1174/3125   Loss: 0.751455 mae: 0.716672 (39.12990504082407 steps/sec)\n",
      "Step #23100\tEpoch   7 Batch 1224/3125   Loss: 0.754425 mae: 0.709121 (38.37086587088417 steps/sec)\n",
      "Step #23150\tEpoch   7 Batch 1274/3125   Loss: 0.943151 mae: 0.726656 (37.654825618005205 steps/sec)\n",
      "Step #23200\tEpoch   7 Batch 1324/3125   Loss: 0.770828 mae: 0.710829 (36.078600575185746 steps/sec)\n",
      "Step #23250\tEpoch   7 Batch 1374/3125   Loss: 0.793337 mae: 0.710269 (35.450278882360934 steps/sec)\n",
      "Step #23300\tEpoch   7 Batch 1424/3125   Loss: 0.814771 mae: 0.714452 (34.32635201978835 steps/sec)\n",
      "Step #23350\tEpoch   7 Batch 1474/3125   Loss: 0.722451 mae: 0.714653 (33.977338417629475 steps/sec)\n",
      "Step #23400\tEpoch   7 Batch 1524/3125   Loss: 0.739045 mae: 0.710239 (32.06215663147016 steps/sec)\n",
      "Step #23450\tEpoch   7 Batch 1574/3125   Loss: 0.793072 mae: 0.719119 (33.28250065663503 steps/sec)\n",
      "Step #23500\tEpoch   7 Batch 1624/3125   Loss: 0.903994 mae: 0.708184 (32.635865539397365 steps/sec)\n",
      "Step #23550\tEpoch   7 Batch 1674/3125   Loss: 0.766002 mae: 0.715979 (31.593159017618877 steps/sec)\n",
      "Step #23600\tEpoch   7 Batch 1724/3125   Loss: 0.918353 mae: 0.712197 (29.121764031659744 steps/sec)\n",
      "Step #23650\tEpoch   7 Batch 1774/3125   Loss: 0.745185 mae: 0.713025 (29.433633791011644 steps/sec)\n",
      "Step #23700\tEpoch   7 Batch 1824/3125   Loss: 0.736597 mae: 0.716042 (28.021072670998997 steps/sec)\n",
      "Step #23750\tEpoch   7 Batch 1874/3125   Loss: 0.899242 mae: 0.714829 (27.376859079705532 steps/sec)\n",
      "Step #23800\tEpoch   7 Batch 1924/3125   Loss: 0.993971 mae: 0.711802 (27.794547839206743 steps/sec)\n",
      "Step #23850\tEpoch   7 Batch 1974/3125   Loss: 0.815749 mae: 0.709700 (27.57341431817677 steps/sec)\n",
      "Step #23900\tEpoch   7 Batch 2024/3125   Loss: 0.870312 mae: 0.718607 (25.747991231338816 steps/sec)\n",
      "Step #23950\tEpoch   7 Batch 2074/3125   Loss: 0.889063 mae: 0.710201 (26.357057574119924 steps/sec)\n",
      "Step #24000\tEpoch   7 Batch 2124/3125   Loss: 0.654116 mae: 0.713991 (25.687567261149276 steps/sec)\n",
      "Step #24050\tEpoch   7 Batch 2174/3125   Loss: 0.719683 mae: 0.705357 (23.76825982933881 steps/sec)\n",
      "Step #24100\tEpoch   7 Batch 2224/3125   Loss: 0.709622 mae: 0.712287 (26.425823100947014 steps/sec)\n",
      "Step #24150\tEpoch   7 Batch 2274/3125   Loss: 0.837948 mae: 0.712572 (26.2711292196587 steps/sec)\n",
      "Step #24200\tEpoch   7 Batch 2324/3125   Loss: 0.684009 mae: 0.721769 (21.620950656978735 steps/sec)\n",
      "Step #24250\tEpoch   7 Batch 2374/3125   Loss: 0.778960 mae: 0.712690 (22.56659946317553 steps/sec)\n",
      "Step #24300\tEpoch   7 Batch 2424/3125   Loss: 0.772870 mae: 0.713278 (20.880950963291816 steps/sec)\n",
      "Step #24350\tEpoch   7 Batch 2474/3125   Loss: 0.874274 mae: 0.707826 (16.660174939018408 steps/sec)\n",
      "Step #24400\tEpoch   7 Batch 2524/3125   Loss: 0.774301 mae: 0.719011 (31.307645110195818 steps/sec)\n",
      "Step #24450\tEpoch   7 Batch 2574/3125   Loss: 0.885216 mae: 0.719218 (39.05524081667771 steps/sec)\n",
      "Step #24500\tEpoch   7 Batch 2624/3125   Loss: 0.947135 mae: 0.717495 (41.92826447959739 steps/sec)\n",
      "Step #24550\tEpoch   7 Batch 2674/3125   Loss: 0.820833 mae: 0.714446 (41.90900150897441 steps/sec)\n",
      "Step #24600\tEpoch   7 Batch 2724/3125   Loss: 0.795840 mae: 0.706012 (41.1333787984079 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #24650\tEpoch   7 Batch 2774/3125   Loss: 0.760029 mae: 0.713418 (39.7888026070117 steps/sec)\n",
      "Step #24700\tEpoch   7 Batch 2824/3125   Loss: 0.850159 mae: 0.721141 (39.11845298384825 steps/sec)\n",
      "Step #24750\tEpoch   7 Batch 2874/3125   Loss: 0.851969 mae: 0.706373 (37.95112097893404 steps/sec)\n",
      "Step #24800\tEpoch   7 Batch 2924/3125   Loss: 0.852185 mae: 0.712465 (34.86167219503567 steps/sec)\n",
      "Step #24850\tEpoch   7 Batch 2974/3125   Loss: 0.717036 mae: 0.714235 (34.76297661567748 steps/sec)\n",
      "Step #24900\tEpoch   7 Batch 3024/3125   Loss: 0.703782 mae: 0.706172 (33.530676186535366 steps/sec)\n",
      "Step #24950\tEpoch   7 Batch 3074/3125   Loss: 0.821703 mae: 0.706006 (33.699420386269885 steps/sec)\n",
      "Step #25000\tEpoch   7 Batch 3124/3125   Loss: 0.856172 mae: 0.708140 (31.61301360828783 steps/sec)\n",
      "\n",
      "Train time for epoch #8 (25000 total steps): 101.1793532371521\n",
      "Model test set loss: 0.833498 mae: 0.720422\n",
      "Step #25050\tEpoch   8 Batch   49/3125   Loss: 0.912784 mae: 0.719993 (28.49042141846203 steps/sec)\n",
      "Step #25100\tEpoch   8 Batch   99/3125   Loss: 0.995055 mae: 0.706139 (36.57826020449899 steps/sec)\n",
      "Step #25150\tEpoch   8 Batch  149/3125   Loss: 0.774357 mae: 0.718001 (34.58580277362322 steps/sec)\n",
      "Step #25200\tEpoch   8 Batch  199/3125   Loss: 0.868294 mae: 0.714629 (34.4066827799241 steps/sec)\n",
      "Step #25250\tEpoch   8 Batch  249/3125   Loss: 0.819058 mae: 0.707531 (35.00157637543138 steps/sec)\n",
      "Step #25300\tEpoch   8 Batch  299/3125   Loss: 0.782470 mae: 0.714138 (33.66794414705066 steps/sec)\n",
      "Step #25350\tEpoch   8 Batch  349/3125   Loss: 0.903460 mae: 0.715394 (33.78051825256074 steps/sec)\n",
      "Step #25400\tEpoch   8 Batch  399/3125   Loss: 0.927509 mae: 0.714308 (32.50517030343245 steps/sec)\n",
      "Step #25450\tEpoch   8 Batch  449/3125   Loss: 0.802047 mae: 0.720902 (31.406268368602298 steps/sec)\n",
      "Step #25500\tEpoch   8 Batch  499/3125   Loss: 0.795711 mae: 0.708064 (30.04249766997707 steps/sec)\n",
      "Step #25550\tEpoch   8 Batch  549/3125   Loss: 0.778218 mae: 0.707206 (29.50876186297811 steps/sec)\n",
      "Step #25600\tEpoch   8 Batch  599/3125   Loss: 0.716997 mae: 0.720792 (29.259751252936752 steps/sec)\n",
      "Step #25650\tEpoch   8 Batch  649/3125   Loss: 0.777974 mae: 0.716354 (27.39364147836046 steps/sec)\n",
      "Step #25700\tEpoch   8 Batch  699/3125   Loss: 0.775175 mae: 0.715716 (27.99598766610483 steps/sec)\n",
      "Step #25750\tEpoch   8 Batch  749/3125   Loss: 0.782281 mae: 0.704483 (26.227595759116305 steps/sec)\n",
      "Step #25800\tEpoch   8 Batch  799/3125   Loss: 0.895334 mae: 0.715335 (27.319437080614506 steps/sec)\n",
      "Step #25850\tEpoch   8 Batch  849/3125   Loss: 0.941736 mae: 0.713599 (25.624044437205665 steps/sec)\n",
      "Step #25900\tEpoch   8 Batch  899/3125   Loss: 0.880197 mae: 0.712098 (27.53810507710964 steps/sec)\n",
      "Step #25950\tEpoch   8 Batch  949/3125   Loss: 0.705125 mae: 0.713122 (24.03173563477343 steps/sec)\n",
      "Step #26000\tEpoch   8 Batch  999/3125   Loss: 0.862370 mae: 0.711813 (26.25164123454636 steps/sec)\n",
      "Step #26050\tEpoch   8 Batch 1049/3125   Loss: 0.882609 mae: 0.718137 (22.053741187555737 steps/sec)\n",
      "Step #26100\tEpoch   8 Batch 1099/3125   Loss: 0.784642 mae: 0.711933 (23.276904158605344 steps/sec)\n",
      "Step #26150\tEpoch   8 Batch 1149/3125   Loss: 0.744934 mae: 0.716742 (23.63267234260451 steps/sec)\n",
      "Step #26200\tEpoch   8 Batch 1199/3125   Loss: 0.818889 mae: 0.710192 (22.632432763775537 steps/sec)\n",
      "Step #26250\tEpoch   8 Batch 1249/3125   Loss: 0.913060 mae: 0.718608 (22.792905648992086 steps/sec)\n",
      "Step #26300\tEpoch   8 Batch 1299/3125   Loss: 0.784239 mae: 0.721760 (23.359147200079484 steps/sec)\n",
      "Step #26350\tEpoch   8 Batch 1349/3125   Loss: 0.834499 mae: 0.701330 (22.810718585624763 steps/sec)\n",
      "Step #26400\tEpoch   8 Batch 1399/3125   Loss: 0.832509 mae: 0.716432 (22.323959014606498 steps/sec)\n",
      "Step #26450\tEpoch   8 Batch 1449/3125   Loss: 0.830913 mae: 0.714235 (22.37335619226036 steps/sec)\n",
      "Step #26500\tEpoch   8 Batch 1499/3125   Loss: 0.842215 mae: 0.713070 (22.201846420304378 steps/sec)\n",
      "Step #26550\tEpoch   8 Batch 1549/3125   Loss: 0.884295 mae: 0.712464 (22.843984281602232 steps/sec)\n",
      "Step #26600\tEpoch   8 Batch 1599/3125   Loss: 0.834209 mae: 0.712189 (21.209769338984817 steps/sec)\n",
      "Step #26650\tEpoch   8 Batch 1649/3125   Loss: 0.847083 mae: 0.715335 (20.968798249987152 steps/sec)\n",
      "Step #26700\tEpoch   8 Batch 1699/3125   Loss: 0.895485 mae: 0.710053 (21.35803638444938 steps/sec)\n",
      "Step #26750\tEpoch   8 Batch 1749/3125   Loss: 0.817950 mae: 0.710915 (19.442225638812065 steps/sec)\n",
      "Step #26800\tEpoch   8 Batch 1799/3125   Loss: 0.923817 mae: 0.716288 (19.13969430912354 steps/sec)\n",
      "Step #26850\tEpoch   8 Batch 1849/3125   Loss: 0.782541 mae: 0.715584 (21.786455437513453 steps/sec)\n",
      "Step #26900\tEpoch   8 Batch 1899/3125   Loss: 0.872842 mae: 0.711290 (20.468474489852685 steps/sec)\n",
      "Step #26950\tEpoch   8 Batch 1949/3125   Loss: 0.792631 mae: 0.708432 (20.254263855780778 steps/sec)\n",
      "Step #27000\tEpoch   8 Batch 1999/3125   Loss: 0.890424 mae: 0.715752 (19.324995590681844 steps/sec)\n",
      "Step #27050\tEpoch   8 Batch 2049/3125   Loss: 0.801217 mae: 0.720719 (20.33872727976292 steps/sec)\n",
      "Step #27100\tEpoch   8 Batch 2099/3125   Loss: 0.855469 mae: 0.706660 (19.55680424880644 steps/sec)\n",
      "Step #27150\tEpoch   8 Batch 2149/3125   Loss: 0.747274 mae: 0.704938 (18.895171205593936 steps/sec)\n",
      "Step #27200\tEpoch   8 Batch 2199/3125   Loss: 0.759156 mae: 0.709308 (20.49378527274987 steps/sec)\n",
      "Step #27250\tEpoch   8 Batch 2249/3125   Loss: 0.773488 mae: 0.710827 (19.967946960140885 steps/sec)\n",
      "Step #27300\tEpoch   8 Batch 2299/3125   Loss: 0.875119 mae: 0.720257 (18.318849795882358 steps/sec)\n",
      "Step #27350\tEpoch   8 Batch 2349/3125   Loss: 0.819546 mae: 0.714909 (20.308807069470454 steps/sec)\n",
      "Step #27400\tEpoch   8 Batch 2399/3125   Loss: 0.744139 mae: 0.716368 (17.83406102990743 steps/sec)\n",
      "Step #27450\tEpoch   8 Batch 2449/3125   Loss: 0.803600 mae: 0.704534 (18.171226747909355 steps/sec)\n",
      "Step #27500\tEpoch   8 Batch 2499/3125   Loss: 0.890609 mae: 0.717053 (18.09550073360418 steps/sec)\n",
      "Step #27550\tEpoch   8 Batch 2549/3125   Loss: 0.657891 mae: 0.720061 (18.386449546355788 steps/sec)\n",
      "Step #27600\tEpoch   8 Batch 2599/3125   Loss: 0.967405 mae: 0.715713 (20.011261601897505 steps/sec)\n",
      "Step #27650\tEpoch   8 Batch 2649/3125   Loss: 0.884758 mae: 0.712966 (20.199553657857653 steps/sec)\n",
      "Step #27700\tEpoch   8 Batch 2699/3125   Loss: 0.765658 mae: 0.710361 (19.146825605146937 steps/sec)\n",
      "Step #27750\tEpoch   8 Batch 2749/3125   Loss: 0.950626 mae: 0.711143 (16.86690475401519 steps/sec)\n",
      "Step #27800\tEpoch   8 Batch 2799/3125   Loss: 0.833700 mae: 0.714987 (18.344477138070104 steps/sec)\n",
      "Step #27850\tEpoch   8 Batch 2849/3125   Loss: 0.809909 mae: 0.711903 (18.469411560696596 steps/sec)\n",
      "Step #27900\tEpoch   8 Batch 2899/3125   Loss: 0.826937 mae: 0.710233 (18.08848188777277 steps/sec)\n",
      "Step #27950\tEpoch   8 Batch 2949/3125   Loss: 0.851113 mae: 0.713472 (15.203918194204125 steps/sec)\n",
      "Step #28000\tEpoch   8 Batch 2999/3125   Loss: 0.743611 mae: 0.710547 (18.338817673596203 steps/sec)\n",
      "Step #28050\tEpoch   8 Batch 3049/3125   Loss: 0.766241 mae: 0.702661 (18.45486998135639 steps/sec)\n",
      "Step #28100\tEpoch   8 Batch 3099/3125   Loss: 0.861332 mae: 0.707695 (16.10795619291432 steps/sec)\n",
      "\n",
      "Train time for epoch #9 (28125 total steps): 141.24248027801514\n",
      "Model test set loss: 0.833990 mae: 0.720189\n",
      "Step #28150\tEpoch   9 Batch   24/3125   Loss: 0.716734 mae: 0.719871 (50.536632667962486 steps/sec)\n",
      "Step #28200\tEpoch   9 Batch   74/3125   Loss: 0.859207 mae: 0.707566 (29.212329969260324 steps/sec)\n",
      "Step #28250\tEpoch   9 Batch  124/3125   Loss: 0.694794 mae: 0.716049 (36.65783120231069 steps/sec)\n",
      "Step #28300\tEpoch   9 Batch  174/3125   Loss: 0.742783 mae: 0.714873 (35.687469177757 steps/sec)\n",
      "Step #28350\tEpoch   9 Batch  224/3125   Loss: 0.599004 mae: 0.710816 (33.71346786770296 steps/sec)\n",
      "Step #28400\tEpoch   9 Batch  274/3125   Loss: 0.809395 mae: 0.710863 (32.90889119040894 steps/sec)\n",
      "Step #28450\tEpoch   9 Batch  324/3125   Loss: 0.784132 mae: 0.713387 (32.74630045933624 steps/sec)\n",
      "Step #28500\tEpoch   9 Batch  374/3125   Loss: 0.792459 mae: 0.714430 (31.232553966075596 steps/sec)\n",
      "Step #28550\tEpoch   9 Batch  424/3125   Loss: 0.867126 mae: 0.717902 (30.54377820852825 steps/sec)\n",
      "Step #28600\tEpoch   9 Batch  474/3125   Loss: 0.784734 mae: 0.712653 (29.472425604383357 steps/sec)\n",
      "Step #28650\tEpoch   9 Batch  524/3125   Loss: 0.823999 mae: 0.706574 (27.58130535999938 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #28700\tEpoch   9 Batch  574/3125   Loss: 0.884821 mae: 0.717003 (26.0406502712278 steps/sec)\n",
      "Step #28750\tEpoch   9 Batch  624/3125   Loss: 0.636150 mae: 0.711520 (27.099139480545727 steps/sec)\n",
      "Step #28800\tEpoch   9 Batch  674/3125   Loss: 0.838820 mae: 0.715884 (27.882427911465015 steps/sec)\n",
      "Step #28850\tEpoch   9 Batch  724/3125   Loss: 0.649148 mae: 0.711961 (27.370895609462725 steps/sec)\n",
      "Step #28900\tEpoch   9 Batch  774/3125   Loss: 0.769988 mae: 0.708091 (28.352692749523804 steps/sec)\n",
      "Step #28950\tEpoch   9 Batch  824/3125   Loss: 0.812116 mae: 0.711563 (26.15355375718391 steps/sec)\n",
      "Step #29000\tEpoch   9 Batch  874/3125   Loss: 0.764174 mae: 0.713033 (26.106849569840843 steps/sec)\n",
      "Step #29050\tEpoch   9 Batch  924/3125   Loss: 0.934654 mae: 0.716168 (24.27384842711191 steps/sec)\n",
      "Step #29100\tEpoch   9 Batch  974/3125   Loss: 0.747951 mae: 0.714664 (23.440317531248418 steps/sec)\n",
      "Step #29150\tEpoch   9 Batch 1024/3125   Loss: 0.812544 mae: 0.710256 (22.782488629950446 steps/sec)\n",
      "Step #29200\tEpoch   9 Batch 1074/3125   Loss: 0.857484 mae: 0.712340 (23.378958945867083 steps/sec)\n",
      "Step #29250\tEpoch   9 Batch 1124/3125   Loss: 0.834496 mae: 0.715347 (22.590864817315953 steps/sec)\n",
      "Step #29300\tEpoch   9 Batch 1174/3125   Loss: 0.749683 mae: 0.715878 (22.999682612030604 steps/sec)\n",
      "Step #29350\tEpoch   9 Batch 1224/3125   Loss: 0.748592 mae: 0.708029 (29.860123222102352 steps/sec)\n",
      "Step #29400\tEpoch   9 Batch 1274/3125   Loss: 0.940464 mae: 0.725323 (41.12504637753233 steps/sec)\n",
      "Step #29450\tEpoch   9 Batch 1324/3125   Loss: 0.768202 mae: 0.709616 (44.74717544592972 steps/sec)\n",
      "Step #29500\tEpoch   9 Batch 1374/3125   Loss: 0.789231 mae: 0.708782 (44.66267535804219 steps/sec)\n",
      "Step #29550\tEpoch   9 Batch 1424/3125   Loss: 0.813529 mae: 0.713073 (44.2172759999612 steps/sec)\n",
      "Step #29600\tEpoch   9 Batch 1474/3125   Loss: 0.720103 mae: 0.713136 (44.45958594313953 steps/sec)\n",
      "Step #29650\tEpoch   9 Batch 1524/3125   Loss: 0.739295 mae: 0.708787 (44.544255237712505 steps/sec)\n",
      "Step #29700\tEpoch   9 Batch 1574/3125   Loss: 0.792217 mae: 0.717712 (43.846678630067274 steps/sec)\n",
      "Step #29750\tEpoch   9 Batch 1624/3125   Loss: 0.898304 mae: 0.706910 (44.02756929588287 steps/sec)\n",
      "Step #29800\tEpoch   9 Batch 1674/3125   Loss: 0.760355 mae: 0.715196 (44.16258994290446 steps/sec)\n",
      "Step #29850\tEpoch   9 Batch 1724/3125   Loss: 0.914318 mae: 0.711116 (44.19053527147921 steps/sec)\n",
      "Step #29900\tEpoch   9 Batch 1774/3125   Loss: 0.743663 mae: 0.712055 (44.80455456455245 steps/sec)\n",
      "Step #29950\tEpoch   9 Batch 1824/3125   Loss: 0.738408 mae: 0.715196 (43.97808175513722 steps/sec)\n",
      "Step #30000\tEpoch   9 Batch 1874/3125   Loss: 0.894516 mae: 0.713975 (41.616277313787606 steps/sec)\n",
      "Step #30050\tEpoch   9 Batch 1924/3125   Loss: 0.992991 mae: 0.711061 (44.442080399792914 steps/sec)\n",
      "Step #30100\tEpoch   9 Batch 1974/3125   Loss: 0.812141 mae: 0.708755 (43.7459414864824 steps/sec)\n",
      "Step #30150\tEpoch   9 Batch 2024/3125   Loss: 0.865195 mae: 0.717403 (44.52466937999061 steps/sec)\n",
      "Step #30200\tEpoch   9 Batch 2074/3125   Loss: 0.891308 mae: 0.709128 (44.38274154990146 steps/sec)\n",
      "Step #30250\tEpoch   9 Batch 2124/3125   Loss: 0.654469 mae: 0.712983 (44.74312756328923 steps/sec)\n",
      "Step #30300\tEpoch   9 Batch 2174/3125   Loss: 0.718366 mae: 0.704672 (44.411370689264494 steps/sec)\n",
      "Step #30350\tEpoch   9 Batch 2224/3125   Loss: 0.710823 mae: 0.711746 (44.55350090353984 steps/sec)\n",
      "Step #30400\tEpoch   9 Batch 2274/3125   Loss: 0.837539 mae: 0.711394 (43.87149950368341 steps/sec)\n",
      "Step #30450\tEpoch   9 Batch 2324/3125   Loss: 0.684256 mae: 0.721031 (43.893334996568534 steps/sec)\n",
      "Step #30500\tEpoch   9 Batch 2374/3125   Loss: 0.778251 mae: 0.711867 (44.029593636515756 steps/sec)\n",
      "Step #30550\tEpoch   9 Batch 2424/3125   Loss: 0.774468 mae: 0.712456 (44.45551453263513 steps/sec)\n",
      "Step #30600\tEpoch   9 Batch 2474/3125   Loss: 0.870020 mae: 0.706798 (44.647841985771635 steps/sec)\n",
      "Step #30650\tEpoch   9 Batch 2524/3125   Loss: 0.773097 mae: 0.717932 (45.18108719317813 steps/sec)\n",
      "Step #30700\tEpoch   9 Batch 2574/3125   Loss: 0.879503 mae: 0.718317 (42.46829638332828 steps/sec)\n",
      "Step #30750\tEpoch   9 Batch 2624/3125   Loss: 0.945197 mae: 0.716208 (42.97437339216386 steps/sec)\n",
      "Step #30800\tEpoch   9 Batch 2674/3125   Loss: 0.822714 mae: 0.713807 (43.79479792264399 steps/sec)\n",
      "Step #30850\tEpoch   9 Batch 2724/3125   Loss: 0.794959 mae: 0.705139 (44.21698698993119 steps/sec)\n",
      "Step #30900\tEpoch   9 Batch 2774/3125   Loss: 0.756197 mae: 0.712512 (43.7760573385126 steps/sec)\n",
      "Step #30950\tEpoch   9 Batch 2824/3125   Loss: 0.850174 mae: 0.720121 (43.7260118595865 steps/sec)\n",
      "Step #31000\tEpoch   9 Batch 2874/3125   Loss: 0.849663 mae: 0.705544 (43.90381052011463 steps/sec)\n",
      "Step #31050\tEpoch   9 Batch 2924/3125   Loss: 0.852102 mae: 0.711335 (44.749743939911234 steps/sec)\n",
      "Step #31100\tEpoch   9 Batch 2974/3125   Loss: 0.712232 mae: 0.712991 (44.56681307266435 steps/sec)\n",
      "Step #31150\tEpoch   9 Batch 3024/3125   Loss: 0.700009 mae: 0.704994 (44.609805234986446 steps/sec)\n",
      "Step #31200\tEpoch   9 Batch 3074/3125   Loss: 0.817630 mae: 0.704930 (42.90327536730582 steps/sec)\n",
      "Step #31250\tEpoch   9 Batch 3124/3125   Loss: 0.854671 mae: 0.707056 (42.44827636460204 steps/sec)\n",
      "\n",
      "Train time for epoch #10 (31250 total steps): 87.58640313148499\n",
      "Model test set loss: 0.834580 mae: 0.720668\n",
      "Step #31300\tEpoch  10 Batch   49/3125   Loss: 0.912434 mae: 0.720156 (44.80973375503138 steps/sec)\n",
      "Step #31350\tEpoch  10 Batch   99/3125   Loss: 0.991739 mae: 0.704950 (45.421470734622254 steps/sec)\n",
      "Step #31400\tEpoch  10 Batch  149/3125   Loss: 0.770021 mae: 0.716312 (44.9352225471475 steps/sec)\n",
      "Step #31450\tEpoch  10 Batch  199/3125   Loss: 0.865508 mae: 0.713404 (45.64779694057858 steps/sec)\n",
      "Step #31500\tEpoch  10 Batch  249/3125   Loss: 0.817736 mae: 0.706502 (45.39459044795607 steps/sec)\n",
      "Step #31550\tEpoch  10 Batch  299/3125   Loss: 0.780226 mae: 0.712968 (45.25525393327481 steps/sec)\n",
      "Step #31600\tEpoch  10 Batch  349/3125   Loss: 0.903912 mae: 0.714585 (45.06944425043041 steps/sec)\n",
      "Step #31650\tEpoch  10 Batch  399/3125   Loss: 0.924636 mae: 0.713205 (45.48989631354648 steps/sec)\n",
      "Step #31700\tEpoch  10 Batch  449/3125   Loss: 0.801081 mae: 0.720017 (45.236911420812774 steps/sec)\n",
      "Step #31750\tEpoch  10 Batch  499/3125   Loss: 0.795090 mae: 0.707142 (44.927136337227736 steps/sec)\n",
      "Step #31800\tEpoch  10 Batch  549/3125   Loss: 0.775745 mae: 0.706359 (45.43983525840977 steps/sec)\n",
      "Step #31850\tEpoch  10 Batch  599/3125   Loss: 0.715152 mae: 0.719464 (42.964072592342504 steps/sec)\n",
      "Step #31900\tEpoch  10 Batch  649/3125   Loss: 0.777465 mae: 0.714845 (45.535964181839695 steps/sec)\n",
      "Step #31950\tEpoch  10 Batch  699/3125   Loss: 0.775324 mae: 0.714204 (45.19255654747859 steps/sec)\n",
      "Step #32000\tEpoch  10 Batch  749/3125   Loss: 0.777889 mae: 0.703001 (45.30389103486505 steps/sec)\n",
      "Step #32050\tEpoch  10 Batch  799/3125   Loss: 0.895793 mae: 0.714330 (44.849119421319344 steps/sec)\n",
      "Step #32100\tEpoch  10 Batch  849/3125   Loss: 0.936000 mae: 0.712631 (45.06209394411072 steps/sec)\n",
      "Step #32150\tEpoch  10 Batch  899/3125   Loss: 0.878608 mae: 0.710687 (45.40176458693883 steps/sec)\n",
      "Step #32200\tEpoch  10 Batch  949/3125   Loss: 0.705125 mae: 0.712164 (45.640097872089456 steps/sec)\n",
      "Step #32250\tEpoch  10 Batch  999/3125   Loss: 0.856595 mae: 0.710958 (45.31768484688184 steps/sec)\n",
      "Step #32300\tEpoch  10 Batch 1049/3125   Loss: 0.884611 mae: 0.717485 (44.80531078573888 steps/sec)\n",
      "Step #32350\tEpoch  10 Batch 1099/3125   Loss: 0.782076 mae: 0.711051 (45.02739880734351 steps/sec)\n",
      "Step #32400\tEpoch  10 Batch 1149/3125   Loss: 0.744506 mae: 0.715530 (45.00705746214219 steps/sec)\n",
      "Step #32450\tEpoch  10 Batch 1199/3125   Loss: 0.818696 mae: 0.709391 (45.68755549862272 steps/sec)\n",
      "Step #32500\tEpoch  10 Batch 1249/3125   Loss: 0.911049 mae: 0.717504 (45.5375758741734 steps/sec)\n",
      "Step #32550\tEpoch  10 Batch 1299/3125   Loss: 0.779647 mae: 0.720443 (42.91112352903984 steps/sec)\n",
      "Step #32600\tEpoch  10 Batch 1349/3125   Loss: 0.832864 mae: 0.700366 (40.805901930891835 steps/sec)\n",
      "Step #32650\tEpoch  10 Batch 1399/3125   Loss: 0.830921 mae: 0.715146 (43.98319154308782 steps/sec)\n",
      "Step #32700\tEpoch  10 Batch 1449/3125   Loss: 0.823525 mae: 0.712803 (44.234866106947386 steps/sec)\n",
      "Step #32750\tEpoch  10 Batch 1499/3125   Loss: 0.844196 mae: 0.711296 (42.75053780844866 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #32800\tEpoch  10 Batch 1549/3125   Loss: 0.880361 mae: 0.711089 (43.71848255118986 steps/sec)\n",
      "Step #32850\tEpoch  10 Batch 1599/3125   Loss: 0.831819 mae: 0.710834 (43.359759435904984 steps/sec)\n",
      "Step #32900\tEpoch  10 Batch 1649/3125   Loss: 0.845746 mae: 0.714113 (44.41818094786654 steps/sec)\n",
      "Step #32950\tEpoch  10 Batch 1699/3125   Loss: 0.889167 mae: 0.709035 (44.62835489079188 steps/sec)\n",
      "Step #33000\tEpoch  10 Batch 1749/3125   Loss: 0.819447 mae: 0.709769 (44.304477512301155 steps/sec)\n",
      "Step #33050\tEpoch  10 Batch 1799/3125   Loss: 0.917492 mae: 0.715533 (44.115944370397294 steps/sec)\n",
      "Step #33100\tEpoch  10 Batch 1849/3125   Loss: 0.779478 mae: 0.714456 (43.58808936303233 steps/sec)\n",
      "Step #33150\tEpoch  10 Batch 1899/3125   Loss: 0.875593 mae: 0.710438 (43.93036170270706 steps/sec)\n",
      "Step #33200\tEpoch  10 Batch 1949/3125   Loss: 0.789536 mae: 0.707544 (44.30994429437249 steps/sec)\n",
      "Step #33250\tEpoch  10 Batch 1999/3125   Loss: 0.886400 mae: 0.714648 (42.01606391909138 steps/sec)\n",
      "Step #33300\tEpoch  10 Batch 2049/3125   Loss: 0.796779 mae: 0.719374 (44.56474850260973 steps/sec)\n",
      "Step #33350\tEpoch  10 Batch 2099/3125   Loss: 0.853012 mae: 0.705747 (44.945633013464494 steps/sec)\n",
      "Step #33400\tEpoch  10 Batch 2149/3125   Loss: 0.746759 mae: 0.703829 (44.99029571520482 steps/sec)\n",
      "Step #33450\tEpoch  10 Batch 2199/3125   Loss: 0.756144 mae: 0.708377 (44.96848360195248 steps/sec)\n",
      "Step #33500\tEpoch  10 Batch 2249/3125   Loss: 0.769736 mae: 0.709595 (44.78091399399292 steps/sec)\n",
      "Step #33550\tEpoch  10 Batch 2299/3125   Loss: 0.871211 mae: 0.718900 (45.545270063658855 steps/sec)\n",
      "Step #33600\tEpoch  10 Batch 2349/3125   Loss: 0.819842 mae: 0.714114 (45.63955158527732 steps/sec)\n",
      "Step #33650\tEpoch  10 Batch 2399/3125   Loss: 0.744145 mae: 0.715468 (44.878343991435 steps/sec)\n",
      "Step #33700\tEpoch  10 Batch 2449/3125   Loss: 0.797774 mae: 0.703528 (45.260371794451224 steps/sec)\n",
      "Step #33750\tEpoch  10 Batch 2499/3125   Loss: 0.885260 mae: 0.715816 (45.355957907012105 steps/sec)\n",
      "Step #33800\tEpoch  10 Batch 2549/3125   Loss: 0.655116 mae: 0.719303 (45.109908984395084 steps/sec)\n",
      "Step #33850\tEpoch  10 Batch 2599/3125   Loss: 0.968461 mae: 0.714699 (45.10127480901091 steps/sec)\n",
      "Step #33900\tEpoch  10 Batch 2649/3125   Loss: 0.882447 mae: 0.711906 (44.575385925358965 steps/sec)\n",
      "Step #33950\tEpoch  10 Batch 2699/3125   Loss: 0.765261 mae: 0.709836 (43.910263725150465 steps/sec)\n",
      "Step #34000\tEpoch  10 Batch 2749/3125   Loss: 0.948921 mae: 0.710096 (43.602933930021635 steps/sec)\n",
      "Step #34050\tEpoch  10 Batch 2799/3125   Loss: 0.830495 mae: 0.714104 (45.01605177093234 steps/sec)\n",
      "Step #34100\tEpoch  10 Batch 2849/3125   Loss: 0.807255 mae: 0.710942 (45.490646243562736 steps/sec)\n",
      "Step #34150\tEpoch  10 Batch 2899/3125   Loss: 0.825238 mae: 0.709319 (45.03575326233218 steps/sec)\n",
      "Step #34200\tEpoch  10 Batch 2949/3125   Loss: 0.846968 mae: 0.712394 (44.123109913073044 steps/sec)\n",
      "Step #34250\tEpoch  10 Batch 2999/3125   Loss: 0.742343 mae: 0.709403 (44.945112857206844 steps/sec)\n",
      "Step #34300\tEpoch  10 Batch 3049/3125   Loss: 0.761819 mae: 0.701368 (44.980047850840926 steps/sec)\n",
      "Step #34350\tEpoch  10 Batch 3099/3125   Loss: 0.855985 mae: 0.706759 (40.708489909898475 steps/sec)\n",
      "\n",
      "Train time for epoch #11 (34375 total steps): 70.15244698524475\n",
      "Model test set loss: 0.835258 mae: 0.720507\n",
      "Step #34400\tEpoch  11 Batch   24/3125   Loss: 0.712263 mae: 0.720140 (86.83841234283159 steps/sec)\n",
      "Step #34450\tEpoch  11 Batch   74/3125   Loss: 0.855778 mae: 0.706334 (44.90153958082516 steps/sec)\n",
      "Step #34500\tEpoch  11 Batch  124/3125   Loss: 0.687899 mae: 0.714643 (45.147423532653626 steps/sec)\n",
      "Step #34550\tEpoch  11 Batch  174/3125   Loss: 0.739525 mae: 0.713478 (43.02433622281914 steps/sec)\n",
      "Step #34600\tEpoch  11 Batch  224/3125   Loss: 0.600563 mae: 0.709629 (43.96064916180015 steps/sec)\n",
      "Step #34650\tEpoch  11 Batch  274/3125   Loss: 0.806069 mae: 0.710023 (43.803469726936385 steps/sec)\n",
      "Step #34700\tEpoch  11 Batch  324/3125   Loss: 0.779419 mae: 0.712233 (42.00779922328261 steps/sec)\n",
      "Step #34750\tEpoch  11 Batch  374/3125   Loss: 0.790318 mae: 0.713345 (41.79063790170008 steps/sec)\n",
      "Step #34800\tEpoch  11 Batch  424/3125   Loss: 0.868231 mae: 0.717041 (44.32337359063684 steps/sec)\n",
      "Step #34850\tEpoch  11 Batch  474/3125   Loss: 0.784796 mae: 0.711729 (43.00992704634071 steps/sec)\n",
      "Step #34900\tEpoch  11 Batch  524/3125   Loss: 0.823286 mae: 0.705577 (45.07923869973126 steps/sec)\n",
      "Step #34950\tEpoch  11 Batch  574/3125   Loss: 0.881087 mae: 0.715942 (45.0121579822965 steps/sec)\n",
      "Step #35000\tEpoch  11 Batch  624/3125   Loss: 0.634445 mae: 0.710258 (44.83367314238608 steps/sec)\n",
      "Step #35050\tEpoch  11 Batch  674/3125   Loss: 0.835794 mae: 0.714155 (45.36183444930788 steps/sec)\n",
      "Step #35100\tEpoch  11 Batch  724/3125   Loss: 0.648005 mae: 0.710439 (42.95487647937281 steps/sec)\n",
      "Step #35150\tEpoch  11 Batch  774/3125   Loss: 0.765974 mae: 0.706501 (44.03203418502654 steps/sec)\n",
      "Step #35200\tEpoch  11 Batch  824/3125   Loss: 0.808008 mae: 0.710232 (43.65065356756307 steps/sec)\n",
      "Step #35250\tEpoch  11 Batch  874/3125   Loss: 0.760883 mae: 0.711644 (44.206277637460595 steps/sec)\n",
      "Step #35300\tEpoch  11 Batch  924/3125   Loss: 0.935404 mae: 0.714581 (44.7157853618763 steps/sec)\n",
      "Step #35350\tEpoch  11 Batch  974/3125   Loss: 0.748725 mae: 0.713671 (44.172654748426844 steps/sec)\n",
      "Step #35400\tEpoch  11 Batch 1024/3125   Loss: 0.810067 mae: 0.709281 (44.7448077275531 steps/sec)\n",
      "Step #35450\tEpoch  11 Batch 1074/3125   Loss: 0.855026 mae: 0.711468 (45.10453406435953 steps/sec)\n",
      "Step #35500\tEpoch  11 Batch 1124/3125   Loss: 0.828511 mae: 0.713888 (44.90953964098528 steps/sec)\n",
      "Step #35550\tEpoch  11 Batch 1174/3125   Loss: 0.746892 mae: 0.714908 (43.646283851774896 steps/sec)\n",
      "Step #35600\tEpoch  11 Batch 1224/3125   Loss: 0.743865 mae: 0.706859 (43.98914216063117 steps/sec)\n",
      "Step #35650\tEpoch  11 Batch 1274/3125   Loss: 0.934767 mae: 0.724053 (41.03093061853607 steps/sec)\n",
      "Step #35700\tEpoch  11 Batch 1324/3125   Loss: 0.766430 mae: 0.708449 (44.423798160687255 steps/sec)\n",
      "Step #35750\tEpoch  11 Batch 1374/3125   Loss: 0.787682 mae: 0.707704 (44.564350763382784 steps/sec)\n",
      "Step #35800\tEpoch  11 Batch 1424/3125   Loss: 0.808466 mae: 0.711421 (44.73456641332426 steps/sec)\n",
      "Step #35850\tEpoch  11 Batch 1474/3125   Loss: 0.714376 mae: 0.711428 (44.543980860008226 steps/sec)\n",
      "Step #35900\tEpoch  11 Batch 1524/3125   Loss: 0.735624 mae: 0.707414 (44.7395576331524 steps/sec)\n",
      "Step #35950\tEpoch  11 Batch 1574/3125   Loss: 0.790194 mae: 0.716520 (45.003155571950366 steps/sec)\n",
      "Step #36000\tEpoch  11 Batch 1624/3125   Loss: 0.892997 mae: 0.705540 (45.25717787889349 steps/sec)\n",
      "Step #36050\tEpoch  11 Batch 1674/3125   Loss: 0.756344 mae: 0.714035 (42.480950791299755 steps/sec)\n",
      "Step #36100\tEpoch  11 Batch 1724/3125   Loss: 0.909223 mae: 0.709979 (43.87123335126826 steps/sec)\n",
      "Step #36150\tEpoch  11 Batch 1774/3125   Loss: 0.745985 mae: 0.711176 (42.73106928736996 steps/sec)\n",
      "Step #36200\tEpoch  11 Batch 1824/3125   Loss: 0.739611 mae: 0.714262 (43.49811501088515 steps/sec)\n",
      "Step #36250\tEpoch  11 Batch 1874/3125   Loss: 0.887345 mae: 0.712761 (42.94735531455992 steps/sec)\n",
      "Step #36300\tEpoch  11 Batch 1924/3125   Loss: 0.994835 mae: 0.710347 (42.51798520311054 steps/sec)\n",
      "Step #36350\tEpoch  11 Batch 1974/3125   Loss: 0.809988 mae: 0.707667 (44.18662470681339 steps/sec)\n",
      "Step #36400\tEpoch  11 Batch 2024/3125   Loss: 0.858817 mae: 0.716214 (44.32490995636099 steps/sec)\n",
      "Step #36450\tEpoch  11 Batch 2074/3125   Loss: 0.887780 mae: 0.707709 (44.89970343079952 steps/sec)\n",
      "Step #36500\tEpoch  11 Batch 2124/3125   Loss: 0.654367 mae: 0.711476 (43.165744238140725 steps/sec)\n",
      "Step #36550\tEpoch  11 Batch 2174/3125   Loss: 0.714980 mae: 0.703454 (44.2128200717136 steps/sec)\n",
      "Step #36600\tEpoch  11 Batch 2224/3125   Loss: 0.709406 mae: 0.710445 (44.64971463086602 steps/sec)\n",
      "Step #36650\tEpoch  11 Batch 2274/3125   Loss: 0.835929 mae: 0.709579 (44.471333125094496 steps/sec)\n",
      "Step #36700\tEpoch  11 Batch 2324/3125   Loss: 0.683390 mae: 0.719985 (42.10685249896196 steps/sec)\n",
      "Step #36750\tEpoch  11 Batch 2374/3125   Loss: 0.777634 mae: 0.710707 (44.52418728016847 steps/sec)\n",
      "Step #36800\tEpoch  11 Batch 2424/3125   Loss: 0.776631 mae: 0.711576 (44.7475382636269 steps/sec)\n",
      "Step #36850\tEpoch  11 Batch 2474/3125   Loss: 0.865278 mae: 0.705754 (44.330175627357576 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #36900\tEpoch  11 Batch 2524/3125   Loss: 0.771267 mae: 0.716764 (44.692457410450544 steps/sec)\n",
      "Step #36950\tEpoch  11 Batch 2574/3125   Loss: 0.876019 mae: 0.717449 (43.9889945289376 steps/sec)\n",
      "Step #37000\tEpoch  11 Batch 2624/3125   Loss: 0.944979 mae: 0.714953 (42.45704193245103 steps/sec)\n",
      "Step #37050\tEpoch  11 Batch 2674/3125   Loss: 0.822535 mae: 0.713221 (45.0935456611628 steps/sec)\n",
      "Step #37100\tEpoch  11 Batch 2724/3125   Loss: 0.793307 mae: 0.704186 (44.86623684917651 steps/sec)\n",
      "Step #37150\tEpoch  11 Batch 2774/3125   Loss: 0.751812 mae: 0.711634 (44.93105393499588 steps/sec)\n",
      "Step #37200\tEpoch  11 Batch 2824/3125   Loss: 0.851153 mae: 0.719118 (43.73344344103243 steps/sec)\n",
      "Step #37250\tEpoch  11 Batch 2874/3125   Loss: 0.846536 mae: 0.704950 (43.558837659441934 steps/sec)\n",
      "Step #37300\tEpoch  11 Batch 2924/3125   Loss: 0.847201 mae: 0.710304 (43.45377119072523 steps/sec)\n",
      "Step #37350\tEpoch  11 Batch 2974/3125   Loss: 0.711707 mae: 0.711997 (43.671978699115 steps/sec)\n",
      "Step #37400\tEpoch  11 Batch 3024/3125   Loss: 0.696493 mae: 0.703606 (43.92920223480164 steps/sec)\n",
      "Step #37450\tEpoch  11 Batch 3074/3125   Loss: 0.815038 mae: 0.703899 (44.57206059320539 steps/sec)\n",
      "Step #37500\tEpoch  11 Batch 3124/3125   Loss: 0.853557 mae: 0.706215 (44.20146990323594 steps/sec)\n",
      "\n",
      "Train time for epoch #12 (37500 total steps): 71.08806920051575\n",
      "Model test set loss: 0.836067 mae: 0.721090\n",
      "Step #37550\tEpoch  12 Batch   49/3125   Loss: 0.911317 mae: 0.720473 (43.826089282549745 steps/sec)\n",
      "Step #37600\tEpoch  12 Batch   99/3125   Loss: 0.985963 mae: 0.703621 (44.47623747698675 steps/sec)\n",
      "Step #37650\tEpoch  12 Batch  149/3125   Loss: 0.767120 mae: 0.714871 (39.581392469473386 steps/sec)\n",
      "Step #37700\tEpoch  12 Batch  199/3125   Loss: 0.861651 mae: 0.711890 (44.45518470564194 steps/sec)\n",
      "Step #37750\tEpoch  12 Batch  249/3125   Loss: 0.815730 mae: 0.705389 (44.906972003378165 steps/sec)\n",
      "Step #37800\tEpoch  12 Batch  299/3125   Loss: 0.778718 mae: 0.711575 (43.067869385461734 steps/sec)\n",
      "Step #37850\tEpoch  12 Batch  349/3125   Loss: 0.905396 mae: 0.713834 (41.69160626096139 steps/sec)\n",
      "Step #37900\tEpoch  12 Batch  399/3125   Loss: 0.920922 mae: 0.712244 (42.4332973441959 steps/sec)\n",
      "Step #37950\tEpoch  12 Batch  449/3125   Loss: 0.800506 mae: 0.719176 (43.011311955027004 steps/sec)\n",
      "Step #38000\tEpoch  12 Batch  499/3125   Loss: 0.792298 mae: 0.706221 (38.55561322238505 steps/sec)\n",
      "Step #38050\tEpoch  12 Batch  549/3125   Loss: 0.772576 mae: 0.705471 (42.03154161402249 steps/sec)\n",
      "Step #38100\tEpoch  12 Batch  599/3125   Loss: 0.712004 mae: 0.718315 (41.820422521167934 steps/sec)\n",
      "Step #38150\tEpoch  12 Batch  649/3125   Loss: 0.774498 mae: 0.713428 (43.767588202933325 steps/sec)\n",
      "Step #38200\tEpoch  12 Batch  699/3125   Loss: 0.774895 mae: 0.712753 (43.317567046995016 steps/sec)\n",
      "Step #38250\tEpoch  12 Batch  749/3125   Loss: 0.773582 mae: 0.701107 (44.48248265955847 steps/sec)\n",
      "Step #38300\tEpoch  12 Batch  799/3125   Loss: 0.894543 mae: 0.712882 (45.09782206424673 steps/sec)\n",
      "Step #38350\tEpoch  12 Batch  849/3125   Loss: 0.930249 mae: 0.710981 (45.41697536204798 steps/sec)\n",
      "Step #38400\tEpoch  12 Batch  899/3125   Loss: 0.874326 mae: 0.708921 (44.94757888992885 steps/sec)\n",
      "Step #38450\tEpoch  12 Batch  949/3125   Loss: 0.703511 mae: 0.710454 (44.62675943195976 steps/sec)\n",
      "Step #38500\tEpoch  12 Batch  999/3125   Loss: 0.850288 mae: 0.709516 (44.33614552533362 steps/sec)\n",
      "Step #38550\tEpoch  12 Batch 1049/3125   Loss: 0.885037 mae: 0.716433 (42.28962888246917 steps/sec)\n",
      "Step #38600\tEpoch  12 Batch 1099/3125   Loss: 0.779795 mae: 0.709799 (41.25712381374693 steps/sec)\n",
      "Step #38650\tEpoch  12 Batch 1149/3125   Loss: 0.743248 mae: 0.714013 (44.35664472331666 steps/sec)\n",
      "Step #38700\tEpoch  12 Batch 1199/3125   Loss: 0.816173 mae: 0.708329 (41.83249345280163 steps/sec)\n",
      "Step #38750\tEpoch  12 Batch 1249/3125   Loss: 0.907576 mae: 0.716618 (44.812261558139966 steps/sec)\n",
      "Step #38800\tEpoch  12 Batch 1299/3125   Loss: 0.778387 mae: 0.719020 (42.55260608212646 steps/sec)\n",
      "Step #38850\tEpoch  12 Batch 1349/3125   Loss: 0.831222 mae: 0.699101 (44.59555697131993 steps/sec)\n",
      "Step #38900\tEpoch  12 Batch 1399/3125   Loss: 0.829931 mae: 0.713796 (45.36685867765949 steps/sec)\n",
      "Step #38950\tEpoch  12 Batch 1449/3125   Loss: 0.816263 mae: 0.710808 (44.6711804648789 steps/sec)\n",
      "Step #39000\tEpoch  12 Batch 1499/3125   Loss: 0.848030 mae: 0.709328 (44.119647514405166 steps/sec)\n",
      "Step #39050\tEpoch  12 Batch 1549/3125   Loss: 0.877608 mae: 0.710064 (44.39252168291558 steps/sec)\n",
      "Step #39100\tEpoch  12 Batch 1599/3125   Loss: 0.826641 mae: 0.709497 (45.18447481756771 steps/sec)\n",
      "Step #39150\tEpoch  12 Batch 1649/3125   Loss: 0.841719 mae: 0.712969 (42.84079455436342 steps/sec)\n",
      "Step #39200\tEpoch  12 Batch 1699/3125   Loss: 0.887847 mae: 0.707881 (44.047164225625274 steps/sec)\n",
      "Step #39250\tEpoch  12 Batch 1749/3125   Loss: 0.821310 mae: 0.708861 (42.25278672895261 steps/sec)\n",
      "Step #39300\tEpoch  12 Batch 1799/3125   Loss: 0.914713 mae: 0.714852 (44.37053417658743 steps/sec)\n",
      "Step #39350\tEpoch  12 Batch 1849/3125   Loss: 0.775346 mae: 0.713441 (44.54514462316201 steps/sec)\n",
      "Step #39400\tEpoch  12 Batch 1899/3125   Loss: 0.878225 mae: 0.709765 (44.07224971371926 steps/sec)\n",
      "Step #39450\tEpoch  12 Batch 1949/3125   Loss: 0.789012 mae: 0.706646 (44.8295425152916 steps/sec)\n",
      "Step #39500\tEpoch  12 Batch 1999/3125   Loss: 0.884889 mae: 0.713687 (45.11141302711135 steps/sec)\n",
      "Step #39550\tEpoch  12 Batch 2049/3125   Loss: 0.793079 mae: 0.718304 (42.338250634371605 steps/sec)\n",
      "Step #39600\tEpoch  12 Batch 2099/3125   Loss: 0.852383 mae: 0.704364 (43.452528704147646 steps/sec)\n",
      "Step #39650\tEpoch  12 Batch 2149/3125   Loss: 0.742534 mae: 0.702347 (43.834920082839275 steps/sec)\n",
      "Step #39700\tEpoch  12 Batch 2199/3125   Loss: 0.752626 mae: 0.706770 (43.84357112095098 steps/sec)\n",
      "Step #39750\tEpoch  12 Batch 2249/3125   Loss: 0.762954 mae: 0.707854 (44.87959252197967 steps/sec)\n",
      "Step #39800\tEpoch  12 Batch 2299/3125   Loss: 0.865904 mae: 0.717081 (44.46499679523264 steps/sec)\n",
      "Step #39850\tEpoch  12 Batch 2349/3125   Loss: 0.819094 mae: 0.712834 (44.68311590688858 steps/sec)\n",
      "Step #39900\tEpoch  12 Batch 2399/3125   Loss: 0.745233 mae: 0.714084 (43.85172124872788 steps/sec)\n",
      "Step #39950\tEpoch  12 Batch 2449/3125   Loss: 0.791595 mae: 0.702308 (42.44607694711566 steps/sec)\n",
      "Step #40000\tEpoch  12 Batch 2499/3125   Loss: 0.881079 mae: 0.714433 (44.111982045179424 steps/sec)\n",
      "Step #40050\tEpoch  12 Batch 2549/3125   Loss: 0.651581 mae: 0.718059 (44.611513360892474 steps/sec)\n",
      "Step #40100\tEpoch  12 Batch 2599/3125   Loss: 0.968651 mae: 0.713568 (44.67975544453774 steps/sec)\n",
      "Step #40150\tEpoch  12 Batch 2649/3125   Loss: 0.879585 mae: 0.710795 (44.240007746186656 steps/sec)\n",
      "Step #40200\tEpoch  12 Batch 2699/3125   Loss: 0.761557 mae: 0.709065 (44.2615017216835 steps/sec)\n",
      "Step #40250\tEpoch  12 Batch 2749/3125   Loss: 0.948012 mae: 0.708926 (44.63532686462429 steps/sec)\n",
      "Step #40300\tEpoch  12 Batch 2799/3125   Loss: 0.826389 mae: 0.713253 (44.874080303161605 steps/sec)\n",
      "Step #40350\tEpoch  12 Batch 2849/3125   Loss: 0.804493 mae: 0.710067 (44.46587359037283 steps/sec)\n",
      "Step #40400\tEpoch  12 Batch 2899/3125   Loss: 0.824001 mae: 0.708673 (44.63874715759911 steps/sec)\n",
      "Step #40450\tEpoch  12 Batch 2949/3125   Loss: 0.842843 mae: 0.711484 (44.66199052391085 steps/sec)\n",
      "Step #40500\tEpoch  12 Batch 2999/3125   Loss: 0.741016 mae: 0.708169 (44.087741069620996 steps/sec)\n",
      "Step #40550\tEpoch  12 Batch 3049/3125   Loss: 0.759628 mae: 0.700066 (44.64445831484814 steps/sec)\n",
      "Step #40600\tEpoch  12 Batch 3099/3125   Loss: 0.850409 mae: 0.705920 (44.18203533168992 steps/sec)\n",
      "\n",
      "Train time for epoch #13 (40625 total steps): 71.51949119567871\n",
      "Model test set loss: 0.837013 mae: 0.720970\n",
      "Step #40650\tEpoch  13 Batch   24/3125   Loss: 0.708785 mae: 0.720547 (79.8623588039108 steps/sec)\n",
      "Step #40700\tEpoch  13 Batch   74/3125   Loss: 0.851510 mae: 0.704866 (45.143137717920396 steps/sec)\n",
      "Step #40750\tEpoch  13 Batch  124/3125   Loss: 0.681795 mae: 0.713302 (43.47845557143177 steps/sec)\n",
      "Step #40800\tEpoch  13 Batch  174/3125   Loss: 0.734719 mae: 0.711799 (40.39990113625878 steps/sec)\n",
      "Step #40850\tEpoch  13 Batch  224/3125   Loss: 0.601689 mae: 0.708179 (44.96019263949727 steps/sec)\n",
      "Step #40900\tEpoch  13 Batch  274/3125   Loss: 0.805981 mae: 0.709090 (44.426903762455105 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #40950\tEpoch  13 Batch  324/3125   Loss: 0.772821 mae: 0.711016 (44.70180282072018 steps/sec)\n",
      "Step #41000\tEpoch  13 Batch  374/3125   Loss: 0.787452 mae: 0.712437 (44.812424343113925 steps/sec)\n",
      "Step #41050\tEpoch  13 Batch  424/3125   Loss: 0.869882 mae: 0.716312 (44.74372897122485 steps/sec)\n",
      "Step #41100\tEpoch  13 Batch  474/3125   Loss: 0.785949 mae: 0.710844 (44.99113543645747 steps/sec)\n",
      "Step #41150\tEpoch  13 Batch  524/3125   Loss: 0.822814 mae: 0.704467 (44.38708148047809 steps/sec)\n",
      "Step #41200\tEpoch  13 Batch  574/3125   Loss: 0.876767 mae: 0.715284 (44.346064545596796 steps/sec)\n",
      "Step #41250\tEpoch  13 Batch  624/3125   Loss: 0.633509 mae: 0.709451 (44.84167779965412 steps/sec)\n",
      "Step #41300\tEpoch  13 Batch  674/3125   Loss: 0.832791 mae: 0.712691 (44.39771883821475 steps/sec)\n",
      "Step #41350\tEpoch  13 Batch  724/3125   Loss: 0.648674 mae: 0.709151 (44.91871629600167 steps/sec)\n",
      "Step #41400\tEpoch  13 Batch  774/3125   Loss: 0.761092 mae: 0.704806 (41.46707264674612 steps/sec)\n",
      "Step #41450\tEpoch  13 Batch  824/3125   Loss: 0.805038 mae: 0.708478 (43.80350632410365 steps/sec)\n",
      "Step #41500\tEpoch  13 Batch  874/3125   Loss: 0.758887 mae: 0.710115 (43.14573602072491 steps/sec)\n",
      "Step #41550\tEpoch  13 Batch  924/3125   Loss: 0.933120 mae: 0.712971 (44.36593467827615 steps/sec)\n",
      "Step #41600\tEpoch  13 Batch  974/3125   Loss: 0.743407 mae: 0.712025 (44.64050501206079 steps/sec)\n",
      "Step #41650\tEpoch  13 Batch 1024/3125   Loss: 0.806505 mae: 0.708173 (44.47907683654307 steps/sec)\n",
      "Step #41700\tEpoch  13 Batch 1074/3125   Loss: 0.853455 mae: 0.710060 (44.38518383065687 steps/sec)\n",
      "Step #41750\tEpoch  13 Batch 1124/3125   Loss: 0.826480 mae: 0.712019 (44.80759874958043 steps/sec)\n",
      "Step #41800\tEpoch  13 Batch 1174/3125   Loss: 0.741795 mae: 0.713717 (44.44273966982519 steps/sec)\n",
      "Step #41850\tEpoch  13 Batch 1224/3125   Loss: 0.740315 mae: 0.705794 (44.34283897083015 steps/sec)\n",
      "Step #41900\tEpoch  13 Batch 1274/3125   Loss: 0.931612 mae: 0.722810 (44.32713974972728 steps/sec)\n",
      "Step #41950\tEpoch  13 Batch 1324/3125   Loss: 0.761256 mae: 0.707012 (44.778475776002566 steps/sec)\n",
      "Step #42000\tEpoch  13 Batch 1374/3125   Loss: 0.784886 mae: 0.706020 (44.55029240250577 steps/sec)\n",
      "Step #42050\tEpoch  13 Batch 1424/3125   Loss: 0.805250 mae: 0.709610 (44.6023194927997 steps/sec)\n",
      "Step #42100\tEpoch  13 Batch 1474/3125   Loss: 0.711127 mae: 0.709525 (44.7393571994211 steps/sec)\n",
      "Step #42150\tEpoch  13 Batch 1524/3125   Loss: 0.731566 mae: 0.705841 (45.209508355823274 steps/sec)\n",
      "Step #42200\tEpoch  13 Batch 1574/3125   Loss: 0.788514 mae: 0.715308 (43.01255580254135 steps/sec)\n",
      "Step #42250\tEpoch  13 Batch 1624/3125   Loss: 0.885240 mae: 0.704036 (45.420605035712256 steps/sec)\n",
      "Step #42300\tEpoch  13 Batch 1674/3125   Loss: 0.752018 mae: 0.712872 (45.389599371127986 steps/sec)\n",
      "Step #42350\tEpoch  13 Batch 1724/3125   Loss: 0.909256 mae: 0.708947 (44.83778535781466 steps/sec)\n",
      "Step #42400\tEpoch  13 Batch 1774/3125   Loss: 0.750419 mae: 0.710440 (45.091393220913325 steps/sec)\n",
      "Step #42450\tEpoch  13 Batch 1824/3125   Loss: 0.738755 mae: 0.713441 (44.54379163597467 steps/sec)\n",
      "Step #42500\tEpoch  13 Batch 1874/3125   Loss: 0.879024 mae: 0.711847 (44.713201697997754 steps/sec)\n",
      "Step #42550\tEpoch  13 Batch 1924/3125   Loss: 0.999139 mae: 0.710044 (44.43211838553393 steps/sec)\n",
      "Step #42600\tEpoch  13 Batch 1974/3125   Loss: 0.807688 mae: 0.707179 (44.643422405797395 steps/sec)\n",
      "Step #42650\tEpoch  13 Batch 2024/3125   Loss: 0.854824 mae: 0.715626 (44.95752282924175 steps/sec)\n",
      "Step #42700\tEpoch  13 Batch 2074/3125   Loss: 0.881838 mae: 0.707179 (44.27435018324917 steps/sec)\n",
      "Step #42750\tEpoch  13 Batch 2124/3125   Loss: 0.653953 mae: 0.710194 (44.58684364610179 steps/sec)\n",
      "Step #42800\tEpoch  13 Batch 2174/3125   Loss: 0.709562 mae: 0.702349 (44.87814231224877 steps/sec)\n",
      "Step #42850\tEpoch  13 Batch 2224/3125   Loss: 0.708694 mae: 0.708657 (44.85954759954643 steps/sec)\n",
      "Step #42900\tEpoch  13 Batch 2274/3125   Loss: 0.832886 mae: 0.707685 (42.10576192754519 steps/sec)\n",
      "Step #42950\tEpoch  13 Batch 2324/3125   Loss: 0.681161 mae: 0.718299 (45.38231122694748 steps/sec)\n",
      "Step #43000\tEpoch  13 Batch 2374/3125   Loss: 0.778067 mae: 0.708991 (45.046510319134725 steps/sec)\n",
      "Step #43050\tEpoch  13 Batch 2424/3125   Loss: 0.777151 mae: 0.710030 (45.383882596488405 steps/sec)\n",
      "Step #43100\tEpoch  13 Batch 2474/3125   Loss: 0.860471 mae: 0.704187 (44.97822456769772 steps/sec)\n",
      "Step #43150\tEpoch  13 Batch 2524/3125   Loss: 0.773460 mae: 0.715401 (45.47567206198468 steps/sec)\n",
      "Step #43200\tEpoch  13 Batch 2574/3125   Loss: 0.872419 mae: 0.716108 (45.46818866738214 steps/sec)\n",
      "Step #43250\tEpoch  13 Batch 2624/3125   Loss: 0.943017 mae: 0.713550 (45.40282615896549 steps/sec)\n",
      "Step #43300\tEpoch  13 Batch 2674/3125   Loss: 0.821944 mae: 0.712293 (44.84695189000056 steps/sec)\n",
      "Step #43350\tEpoch  13 Batch 2724/3125   Loss: 0.792461 mae: 0.702965 (45.156794905252966 steps/sec)\n",
      "Step #43400\tEpoch  13 Batch 2774/3125   Loss: 0.749778 mae: 0.710852 (44.811380624681355 steps/sec)\n",
      "Step #43450\tEpoch  13 Batch 2824/3125   Loss: 0.849930 mae: 0.717996 (45.37698902116574 steps/sec)\n",
      "Step #43500\tEpoch  13 Batch 2874/3125   Loss: 0.843670 mae: 0.704345 (44.848131538140926 steps/sec)\n",
      "Step #43550\tEpoch  13 Batch 2924/3125   Loss: 0.846033 mae: 0.709369 (45.053932974832854 steps/sec)\n",
      "Step #43600\tEpoch  13 Batch 2974/3125   Loss: 0.711502 mae: 0.710954 (43.65241623693905 steps/sec)\n",
      "Step #43650\tEpoch  13 Batch 3024/3125   Loss: 0.691923 mae: 0.702110 (44.698649141462354 steps/sec)\n",
      "Step #43700\tEpoch  13 Batch 3074/3125   Loss: 0.815224 mae: 0.703091 (45.02618070829186 steps/sec)\n",
      "Step #43750\tEpoch  13 Batch 3124/3125   Loss: 0.849775 mae: 0.705376 (45.370490160487826 steps/sec)\n",
      "\n",
      "Train time for epoch #14 (43750 total steps): 70.29592394828796\n",
      "Model test set loss: 0.838001 mae: 0.721714\n",
      "Step #43800\tEpoch  14 Batch   49/3125   Loss: 0.906702 mae: 0.720980 (41.848253705357365 steps/sec)\n",
      "Step #43850\tEpoch  14 Batch   99/3125   Loss: 0.981950 mae: 0.702047 (45.33382907985698 steps/sec)\n",
      "Step #43900\tEpoch  14 Batch  149/3125   Loss: 0.762536 mae: 0.713258 (43.928778952245736 steps/sec)\n",
      "Step #43950\tEpoch  14 Batch  199/3125   Loss: 0.860030 mae: 0.710239 (45.36653481688672 steps/sec)\n",
      "Step #44000\tEpoch  14 Batch  249/3125   Loss: 0.811374 mae: 0.704153 (45.21678009570946 steps/sec)\n",
      "Step #44050\tEpoch  14 Batch  299/3125   Loss: 0.776930 mae: 0.710159 (45.30513399367027 steps/sec)\n",
      "Step #44100\tEpoch  14 Batch  349/3125   Loss: 0.906158 mae: 0.713084 (45.07850227267744 steps/sec)\n",
      "Step #44150\tEpoch  14 Batch  399/3125   Loss: 0.916618 mae: 0.711207 (45.33511288316689 steps/sec)\n",
      "Step #44200\tEpoch  14 Batch  449/3125   Loss: 0.800557 mae: 0.718282 (44.34561443722579 steps/sec)\n",
      "Step #44250\tEpoch  14 Batch  499/3125   Loss: 0.789487 mae: 0.705500 (45.20162515761227 steps/sec)\n",
      "Step #44300\tEpoch  14 Batch  549/3125   Loss: 0.772355 mae: 0.704909 (44.90591426054506 steps/sec)\n",
      "Step #44350\tEpoch  14 Batch  599/3125   Loss: 0.710680 mae: 0.717672 (45.22730193947874 steps/sec)\n",
      "Step #44400\tEpoch  14 Batch  649/3125   Loss: 0.774431 mae: 0.712544 (44.96665161454387 steps/sec)\n",
      "Step #44450\tEpoch  14 Batch  699/3125   Loss: 0.775935 mae: 0.711896 (45.05329416250879 steps/sec)\n",
      "Step #44500\tEpoch  14 Batch  749/3125   Loss: 0.772159 mae: 0.700062 (45.186314860932335 steps/sec)\n",
      "Step #44550\tEpoch  14 Batch  799/3125   Loss: 0.892207 mae: 0.711693 (42.05776517183573 steps/sec)\n",
      "Step #44600\tEpoch  14 Batch  849/3125   Loss: 0.927573 mae: 0.709584 (43.647264917614216 steps/sec)\n",
      "Step #44650\tEpoch  14 Batch  899/3125   Loss: 0.871047 mae: 0.707409 (43.896109604639214 steps/sec)\n",
      "Step #44700\tEpoch  14 Batch  949/3125   Loss: 0.701246 mae: 0.708681 (45.00648758945383 steps/sec)\n",
      "Step #44750\tEpoch  14 Batch  999/3125   Loss: 0.844304 mae: 0.707971 (45.466986032652336 steps/sec)\n",
      "Step #44800\tEpoch  14 Batch 1049/3125   Loss: 0.883244 mae: 0.715165 (44.31980476184639 steps/sec)\n",
      "Step #44850\tEpoch  14 Batch 1099/3125   Loss: 0.777747 mae: 0.708467 (44.67149447376768 steps/sec)\n",
      "Step #44900\tEpoch  14 Batch 1149/3125   Loss: 0.738404 mae: 0.712221 (45.22791643383994 steps/sec)\n",
      "Step #44950\tEpoch  14 Batch 1199/3125   Loss: 0.812584 mae: 0.707049 (44.94241594373626 steps/sec)\n",
      "Step #45000\tEpoch  14 Batch 1249/3125   Loss: 0.906127 mae: 0.715236 (45.01750124395571 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #45050\tEpoch  14 Batch 1299/3125   Loss: 0.777681 mae: 0.717480 (44.838216752798125 steps/sec)\n",
      "Step #45100\tEpoch  14 Batch 1349/3125   Loss: 0.830919 mae: 0.697641 (45.391495458041014 steps/sec)\n",
      "Step #45150\tEpoch  14 Batch 1399/3125   Loss: 0.826172 mae: 0.712059 (45.082979345452046 steps/sec)\n",
      "Step #45200\tEpoch  14 Batch 1449/3125   Loss: 0.808279 mae: 0.708975 (44.72827887094982 steps/sec)\n",
      "Step #45250\tEpoch  14 Batch 1499/3125   Loss: 0.853002 mae: 0.706820 (43.1311478511674 steps/sec)\n",
      "Step #45300\tEpoch  14 Batch 1549/3125   Loss: 0.877091 mae: 0.708910 (44.60116222587267 steps/sec)\n",
      "Step #45350\tEpoch  14 Batch 1599/3125   Loss: 0.821940 mae: 0.707852 (45.085664077456585 steps/sec)\n",
      "Step #45400\tEpoch  14 Batch 1649/3125   Loss: 0.838420 mae: 0.711201 (45.63583718174194 steps/sec)\n",
      "Step #45450\tEpoch  14 Batch 1699/3125   Loss: 0.884306 mae: 0.706515 (45.6417169443638 steps/sec)\n",
      "Step #45500\tEpoch  14 Batch 1749/3125   Loss: 0.823557 mae: 0.707863 (45.08882413558605 steps/sec)\n",
      "Step #45550\tEpoch  14 Batch 1799/3125   Loss: 0.914905 mae: 0.713766 (45.42703954140428 steps/sec)\n",
      "Step #45600\tEpoch  14 Batch 1849/3125   Loss: 0.771406 mae: 0.712184 (44.80817317050293 steps/sec)\n",
      "Step #45650\tEpoch  14 Batch 1899/3125   Loss: 0.880817 mae: 0.709755 (44.61010889225408 steps/sec)\n",
      "Step #45700\tEpoch  14 Batch 1949/3125   Loss: 0.790225 mae: 0.706275 (44.88430875255492 steps/sec)\n",
      "Step #45750\tEpoch  14 Batch 1999/3125   Loss: 0.886078 mae: 0.713349 (45.13315036040416 steps/sec)\n",
      "Step #45800\tEpoch  14 Batch 2049/3125   Loss: 0.789189 mae: 0.718014 (44.79503219155932 steps/sec)\n",
      "Step #45850\tEpoch  14 Batch 2099/3125   Loss: 0.853886 mae: 0.703819 (44.84376810887628 steps/sec)\n",
      "Step #45900\tEpoch  14 Batch 2149/3125   Loss: 0.739310 mae: 0.701434 (45.347484240350624 steps/sec)\n",
      "Step #45950\tEpoch  14 Batch 2199/3125   Loss: 0.751663 mae: 0.705570 (42.74417701206635 steps/sec)\n",
      "Step #46000\tEpoch  14 Batch 2249/3125   Loss: 0.759598 mae: 0.706201 (45.461230039262475 steps/sec)\n",
      "Step #46050\tEpoch  14 Batch 2299/3125   Loss: 0.859471 mae: 0.715410 (45.55587608657118 steps/sec)\n",
      "Step #46100\tEpoch  14 Batch 2349/3125   Loss: 0.816230 mae: 0.711370 (44.18331988827929 steps/sec)\n",
      "Step #46150\tEpoch  14 Batch 2399/3125   Loss: 0.744919 mae: 0.712301 (44.61529068148374 steps/sec)\n",
      "Step #46200\tEpoch  14 Batch 2449/3125   Loss: 0.786464 mae: 0.700694 (44.69001929181789 steps/sec)\n",
      "Step #46250\tEpoch  14 Batch 2499/3125   Loss: 0.882069 mae: 0.712897 (44.44424664620726 steps/sec)\n",
      "Step #46300\tEpoch  14 Batch 2549/3125   Loss: 0.648701 mae: 0.716603 (45.26902791107105 steps/sec)\n",
      "Step #46350\tEpoch  14 Batch 2599/3125   Loss: 0.964052 mae: 0.712153 (45.669905212960515 steps/sec)\n",
      "Step #46400\tEpoch  14 Batch 2649/3125   Loss: 0.874668 mae: 0.709553 (45.360490264423625 steps/sec)\n",
      "Step #46450\tEpoch  14 Batch 2699/3125   Loss: 0.754421 mae: 0.707978 (45.481175259426685 steps/sec)\n",
      "Step #46500\tEpoch  14 Batch 2749/3125   Loss: 0.945934 mae: 0.707691 (44.915272206015295 steps/sec)\n",
      "Step #46550\tEpoch  14 Batch 2799/3125   Loss: 0.820092 mae: 0.712573 (45.0937589772436 steps/sec)\n",
      "Step #46600\tEpoch  14 Batch 2849/3125   Loss: 0.803264 mae: 0.709149 (45.04476872036164 steps/sec)\n",
      "Step #46650\tEpoch  14 Batch 2899/3125   Loss: 0.824428 mae: 0.708198 (44.22481956548099 steps/sec)\n",
      "Step #46700\tEpoch  14 Batch 2949/3125   Loss: 0.839891 mae: 0.710114 (43.65888663543662 steps/sec)\n",
      "Step #46750\tEpoch  14 Batch 2999/3125   Loss: 0.734943 mae: 0.707090 (45.25643562668499 steps/sec)\n",
      "Step #46800\tEpoch  14 Batch 3049/3125   Loss: 0.759258 mae: 0.699027 (45.642918906660356 steps/sec)\n",
      "Step #46850\tEpoch  14 Batch 3099/3125   Loss: 0.843865 mae: 0.705193 (45.625809326222864 steps/sec)\n",
      "\n",
      "Train time for epoch #15 (46875 total steps): 69.8086850643158\n",
      "Model test set loss: 0.838949 mae: 0.721542\n",
      "Step #46900\tEpoch  15 Batch   24/3125   Loss: 0.707223 mae: 0.721055 (69.88987726667405 steps/sec)\n",
      "Step #46950\tEpoch  15 Batch   74/3125   Loss: 0.847461 mae: 0.703391 (42.339267803815225 steps/sec)\n",
      "Step #47000\tEpoch  15 Batch  124/3125   Loss: 0.674269 mae: 0.711706 (45.228726031267456 steps/sec)\n",
      "Step #47050\tEpoch  15 Batch  174/3125   Loss: 0.728950 mae: 0.709935 (44.69065736747847 steps/sec)\n",
      "Step #47100\tEpoch  15 Batch  224/3125   Loss: 0.602419 mae: 0.706708 (45.26119232260064 steps/sec)\n",
      "Step #47150\tEpoch  15 Batch  274/3125   Loss: 0.803274 mae: 0.707930 (45.548326703567476 steps/sec)\n",
      "Step #47200\tEpoch  15 Batch  324/3125   Loss: 0.766361 mae: 0.709728 (45.11172355164649 steps/sec)\n",
      "Step #47250\tEpoch  15 Batch  374/3125   Loss: 0.784553 mae: 0.711423 (45.141748161312044 steps/sec)\n",
      "Step #47300\tEpoch  15 Batch  424/3125   Loss: 0.873435 mae: 0.715427 (44.57964989761454 steps/sec)\n",
      "Step #47350\tEpoch  15 Batch  474/3125   Loss: 0.785865 mae: 0.709930 (45.45062863394459 steps/sec)\n",
      "Step #47400\tEpoch  15 Batch  524/3125   Loss: 0.821114 mae: 0.703577 (45.08074069790763 steps/sec)\n",
      "Step #47450\tEpoch  15 Batch  574/3125   Loss: 0.874643 mae: 0.714888 (45.24481669410572 steps/sec)\n",
      "Step #47500\tEpoch  15 Batch  624/3125   Loss: 0.631989 mae: 0.708903 (44.0193536781623 steps/sec)\n",
      "Step #47550\tEpoch  15 Batch  674/3125   Loss: 0.829325 mae: 0.711990 (43.930177656095836 steps/sec)\n",
      "Step #47600\tEpoch  15 Batch  724/3125   Loss: 0.649773 mae: 0.708824 (45.161248642784265 steps/sec)\n",
      "Step #47650\tEpoch  15 Batch  774/3125   Loss: 0.755877 mae: 0.703619 (45.16054843370796 steps/sec)\n",
      "Step #47700\tEpoch  15 Batch  824/3125   Loss: 0.803153 mae: 0.707447 (45.35509470352743 steps/sec)\n",
      "Step #47750\tEpoch  15 Batch  874/3125   Loss: 0.757946 mae: 0.709108 (45.54285669611717 steps/sec)\n",
      "Step #47800\tEpoch  15 Batch  924/3125   Loss: 0.931293 mae: 0.711471 (45.33778852373392 steps/sec)\n",
      "Step #47850\tEpoch  15 Batch  974/3125   Loss: 0.738134 mae: 0.710270 (45.21401149003758 steps/sec)\n",
      "Step #47900\tEpoch  15 Batch 1024/3125   Loss: 0.804228 mae: 0.707222 (44.94575823806537 steps/sec)\n",
      "Step #47950\tEpoch  15 Batch 1074/3125   Loss: 0.851009 mae: 0.708746 (43.27629613121033 steps/sec)\n",
      "Step #48000\tEpoch  15 Batch 1124/3125   Loss: 0.821418 mae: 0.710443 (45.1917774596649 steps/sec)\n",
      "Step #48050\tEpoch  15 Batch 1174/3125   Loss: 0.739733 mae: 0.712455 (45.39743034965177 steps/sec)\n",
      "Step #48100\tEpoch  15 Batch 1224/3125   Loss: 0.736425 mae: 0.704521 (45.023300069601795 steps/sec)\n",
      "Step #48150\tEpoch  15 Batch 1274/3125   Loss: 0.927445 mae: 0.721437 (45.44442378610208 steps/sec)\n",
      "Step #48200\tEpoch  15 Batch 1324/3125   Loss: 0.753943 mae: 0.705816 (45.152312881772595 steps/sec)\n",
      "Step #48250\tEpoch  15 Batch 1374/3125   Loss: 0.782338 mae: 0.704106 (43.128664231180856 steps/sec)\n",
      "Step #48300\tEpoch  15 Batch 1424/3125   Loss: 0.796500 mae: 0.707827 (45.025610351232274 steps/sec)\n",
      "Step #48350\tEpoch  15 Batch 1474/3125   Loss: 0.708999 mae: 0.707832 (45.34242508826214 steps/sec)\n",
      "Step #48400\tEpoch  15 Batch 1524/3125   Loss: 0.726954 mae: 0.703964 (45.29177820515451 steps/sec)\n",
      "Step #48450\tEpoch  15 Batch 1574/3125   Loss: 0.781079 mae: 0.713723 (45.57055655717561 steps/sec)\n",
      "Step #48500\tEpoch  15 Batch 1624/3125   Loss: 0.876931 mae: 0.702066 (45.36154990717454 steps/sec)\n",
      "Step #48550\tEpoch  15 Batch 1674/3125   Loss: 0.743539 mae: 0.711192 (45.437482274274736 steps/sec)\n",
      "Step #48600\tEpoch  15 Batch 1724/3125   Loss: 0.907500 mae: 0.707488 (44.93307556715594 steps/sec)\n",
      "Step #48650\tEpoch  15 Batch 1774/3125   Loss: 0.753024 mae: 0.708917 (45.0100616058735 steps/sec)\n",
      "Step #48700\tEpoch  15 Batch 1824/3125   Loss: 0.735570 mae: 0.711952 (45.0861487201901 steps/sec)\n",
      "Step #48750\tEpoch  15 Batch 1874/3125   Loss: 0.870328 mae: 0.710754 (45.323600451603866 steps/sec)\n",
      "Step #48800\tEpoch  15 Batch 1924/3125   Loss: 1.003044 mae: 0.709924 (45.05708858630042 steps/sec)\n",
      "Step #48850\tEpoch  15 Batch 1974/3125   Loss: 0.803785 mae: 0.706553 (45.5358455341519 steps/sec)\n",
      "Step #48900\tEpoch  15 Batch 2024/3125   Loss: 0.851523 mae: 0.715194 (44.49840543023678 steps/sec)\n",
      "Step #48950\tEpoch  15 Batch 2074/3125   Loss: 0.880129 mae: 0.707086 (43.364305099640724 steps/sec)\n",
      "Step #49000\tEpoch  15 Batch 2124/3125   Loss: 0.654707 mae: 0.709788 (44.77502447822836 steps/sec)\n",
      "Step #49050\tEpoch  15 Batch 2174/3125   Loss: 0.708740 mae: 0.701929 (45.225946209393676 steps/sec)\n",
      "Step #49100\tEpoch  15 Batch 2224/3125   Loss: 0.708386 mae: 0.707531 (45.52987439919359 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #49150\tEpoch  15 Batch 2274/3125   Loss: 0.829890 mae: 0.706593 (44.82676363536872 steps/sec)\n",
      "Step #49200\tEpoch  15 Batch 2324/3125   Loss: 0.679968 mae: 0.717136 (45.104369150245375 steps/sec)\n",
      "Step #49250\tEpoch  15 Batch 2374/3125   Loss: 0.778055 mae: 0.707561 (45.4048413164934 steps/sec)\n",
      "Step #49300\tEpoch  15 Batch 2424/3125   Loss: 0.776164 mae: 0.708308 (44.85644837968093 steps/sec)\n",
      "Step #49350\tEpoch  15 Batch 2474/3125   Loss: 0.854944 mae: 0.702413 (45.20554205211819 steps/sec)\n",
      "Step #49400\tEpoch  15 Batch 2524/3125   Loss: 0.773859 mae: 0.714052 (44.405258285628136 steps/sec)\n",
      "Step #49450\tEpoch  15 Batch 2574/3125   Loss: 0.866445 mae: 0.714558 (44.84248321682688 steps/sec)\n",
      "Step #49500\tEpoch  15 Batch 2624/3125   Loss: 0.941415 mae: 0.712307 (30.047025846592753 steps/sec)\n",
      "Step #49550\tEpoch  15 Batch 2674/3125   Loss: 0.816954 mae: 0.711093 (36.804621637478114 steps/sec)\n",
      "Step #49600\tEpoch  15 Batch 2724/3125   Loss: 0.792887 mae: 0.701493 (22.997621996722227 steps/sec)\n",
      "Step #49650\tEpoch  15 Batch 2774/3125   Loss: 0.748389 mae: 0.709793 (28.745774458112205 steps/sec)\n",
      "Step #49700\tEpoch  15 Batch 2824/3125   Loss: 0.848570 mae: 0.717137 (44.947126121453465 steps/sec)\n",
      "Step #49750\tEpoch  15 Batch 2874/3125   Loss: 0.840230 mae: 0.703682 (44.81207004792001 steps/sec)\n",
      "Step #49800\tEpoch  15 Batch 2924/3125   Loss: 0.839815 mae: 0.708467 (44.26718215796687 steps/sec)\n",
      "Step #49850\tEpoch  15 Batch 2974/3125   Loss: 0.711000 mae: 0.709894 (43.89464875732937 steps/sec)\n",
      "Step #49900\tEpoch  15 Batch 3024/3125   Loss: 0.691322 mae: 0.700802 (42.78770349719748 steps/sec)\n",
      "Step #49950\tEpoch  15 Batch 3074/3125   Loss: 0.816590 mae: 0.702378 (42.25100759435526 steps/sec)\n",
      "Step #50000\tEpoch  15 Batch 3124/3125   Loss: 0.845699 mae: 0.704660 (41.73525685024537 steps/sec)\n",
      "\n",
      "Train time for epoch #16 (50000 total steps): 72.51242399215698\n",
      "Model test set loss: 0.839846 mae: 0.722292\n",
      "Step #50050\tEpoch  16 Batch   49/3125   Loss: 0.903577 mae: 0.721451 (26.08933749952882 steps/sec)\n",
      "Step #50100\tEpoch  16 Batch   99/3125   Loss: 0.979980 mae: 0.700554 (24.174325637648167 steps/sec)\n",
      "Step #50150\tEpoch  16 Batch  149/3125   Loss: 0.758158 mae: 0.711619 (25.437420354623125 steps/sec)\n",
      "Step #50200\tEpoch  16 Batch  199/3125   Loss: 0.861104 mae: 0.708460 (24.545913038857567 steps/sec)\n",
      "Step #50250\tEpoch  16 Batch  249/3125   Loss: 0.807281 mae: 0.702721 (23.94467525347033 steps/sec)\n",
      "Step #50300\tEpoch  16 Batch  299/3125   Loss: 0.776811 mae: 0.708682 (24.428732910445188 steps/sec)\n",
      "Step #50350\tEpoch  16 Batch  349/3125   Loss: 0.907170 mae: 0.712263 (24.06405416762193 steps/sec)\n",
      "Step #50400\tEpoch  16 Batch  399/3125   Loss: 0.912731 mae: 0.709947 (32.67437769715462 steps/sec)\n",
      "Step #50450\tEpoch  16 Batch  449/3125   Loss: 0.799357 mae: 0.717134 (44.372731008176096 steps/sec)\n",
      "Step #50500\tEpoch  16 Batch  499/3125   Loss: 0.788621 mae: 0.704369 (44.45060532800558 steps/sec)\n",
      "Step #50550\tEpoch  16 Batch  549/3125   Loss: 0.771047 mae: 0.704255 (41.51244798206127 steps/sec)\n",
      "Step #50600\tEpoch  16 Batch  599/3125   Loss: 0.711268 mae: 0.716990 (42.618881702258065 steps/sec)\n",
      "Step #50650\tEpoch  16 Batch  649/3125   Loss: 0.773504 mae: 0.712258 (41.84691763020342 steps/sec)\n",
      "Step #50700\tEpoch  16 Batch  699/3125   Loss: 0.774338 mae: 0.711570 (41.09270858132661 steps/sec)\n",
      "Step #50750\tEpoch  16 Batch  749/3125   Loss: 0.770830 mae: 0.699479 (40.09703659062612 steps/sec)\n",
      "Step #50800\tEpoch  16 Batch  799/3125   Loss: 0.889581 mae: 0.711067 (38.985013142778534 steps/sec)\n",
      "Step #50850\tEpoch  16 Batch  849/3125   Loss: 0.924571 mae: 0.709047 (37.456098989812396 steps/sec)\n",
      "Step #50900\tEpoch  16 Batch  899/3125   Loss: 0.866250 mae: 0.706569 (36.39829167792641 steps/sec)\n",
      "Step #50950\tEpoch  16 Batch  949/3125   Loss: 0.699419 mae: 0.707158 (35.15094872188547 steps/sec)\n",
      "Step #51000\tEpoch  16 Batch  999/3125   Loss: 0.839574 mae: 0.706530 (35.2214076893996 steps/sec)\n",
      "Step #51050\tEpoch  16 Batch 1049/3125   Loss: 0.881237 mae: 0.714373 (34.24268811580733 steps/sec)\n",
      "Step #51100\tEpoch  16 Batch 1099/3125   Loss: 0.778282 mae: 0.707251 (33.38134040323392 steps/sec)\n",
      "Step #51150\tEpoch  16 Batch 1149/3125   Loss: 0.733482 mae: 0.710704 (29.700840598913626 steps/sec)\n",
      "Step #51200\tEpoch  16 Batch 1199/3125   Loss: 0.808749 mae: 0.705777 (20.81645407033923 steps/sec)\n",
      "Step #51250\tEpoch  16 Batch 1249/3125   Loss: 0.903197 mae: 0.713892 (26.859394791678675 steps/sec)\n",
      "Step #51300\tEpoch  16 Batch 1299/3125   Loss: 0.776657 mae: 0.716236 (30.12196861308658 steps/sec)\n",
      "Step #51350\tEpoch  16 Batch 1349/3125   Loss: 0.827959 mae: 0.696179 (30.171935918129343 steps/sec)\n",
      "Step #51400\tEpoch  16 Batch 1399/3125   Loss: 0.824095 mae: 0.710668 (28.42487073178368 steps/sec)\n",
      "Step #51450\tEpoch  16 Batch 1449/3125   Loss: 0.805441 mae: 0.707638 (27.726389325915488 steps/sec)\n",
      "Step #51500\tEpoch  16 Batch 1499/3125   Loss: 0.859016 mae: 0.705013 (27.45578230936274 steps/sec)\n",
      "Step #51550\tEpoch  16 Batch 1549/3125   Loss: 0.874810 mae: 0.707403 (28.38541211967895 steps/sec)\n",
      "Step #51600\tEpoch  16 Batch 1599/3125   Loss: 0.814604 mae: 0.706081 (25.695105137067998 steps/sec)\n",
      "Step #51650\tEpoch  16 Batch 1649/3125   Loss: 0.833921 mae: 0.709081 (24.64727684372244 steps/sec)\n",
      "Step #51700\tEpoch  16 Batch 1699/3125   Loss: 0.882891 mae: 0.704720 (25.42154029064521 steps/sec)\n",
      "Step #51750\tEpoch  16 Batch 1749/3125   Loss: 0.819758 mae: 0.706402 (26.069965871579978 steps/sec)\n",
      "Step #51800\tEpoch  16 Batch 1799/3125   Loss: 0.913426 mae: 0.712357 (15.33910720724872 steps/sec)\n",
      "Step #51850\tEpoch  16 Batch 1849/3125   Loss: 0.769837 mae: 0.710561 (17.870182068363277 steps/sec)\n",
      "Step #51900\tEpoch  16 Batch 1899/3125   Loss: 0.879042 mae: 0.709111 (28.125058254347703 steps/sec)\n",
      "Step #51950\tEpoch  16 Batch 1949/3125   Loss: 0.790782 mae: 0.705764 (37.93214779695671 steps/sec)\n",
      "Step #52000\tEpoch  16 Batch 1999/3125   Loss: 0.883530 mae: 0.712920 (38.6541016457307 steps/sec)\n",
      "Step #52050\tEpoch  16 Batch 2049/3125   Loss: 0.789644 mae: 0.717907 (19.45915812254725 steps/sec)\n",
      "Step #52100\tEpoch  16 Batch 2099/3125   Loss: 0.854860 mae: 0.703876 (17.80716232484737 steps/sec)\n",
      "Step #52150\tEpoch  16 Batch 2149/3125   Loss: 0.733468 mae: 0.701432 (29.86229595220784 steps/sec)\n",
      "Step #52200\tEpoch  16 Batch 2199/3125   Loss: 0.755036 mae: 0.705078 (36.04788434710445 steps/sec)\n",
      "Step #52250\tEpoch  16 Batch 2249/3125   Loss: 0.750794 mae: 0.705324 (37.503979032199226 steps/sec)\n",
      "Step #52300\tEpoch  16 Batch 2299/3125   Loss: 0.858403 mae: 0.715116 (41.205170766058046 steps/sec)\n",
      "Step #52350\tEpoch  16 Batch 2349/3125   Loss: 0.816116 mae: 0.710136 (37.99840298688924 steps/sec)\n",
      "Step #52400\tEpoch  16 Batch 2399/3125   Loss: 0.744129 mae: 0.711141 (39.39045944692065 steps/sec)\n",
      "Step #52450\tEpoch  16 Batch 2449/3125   Loss: 0.782359 mae: 0.699244 (36.597620786081556 steps/sec)\n",
      "Step #52500\tEpoch  16 Batch 2499/3125   Loss: 0.881540 mae: 0.711479 (37.27135185320453 steps/sec)\n",
      "Step #52550\tEpoch  16 Batch 2549/3125   Loss: 0.645821 mae: 0.714957 (35.806488239095565 steps/sec)\n",
      "Step #52600\tEpoch  16 Batch 2599/3125   Loss: 0.954235 mae: 0.710771 (35.14842134355117 steps/sec)\n",
      "Step #52650\tEpoch  16 Batch 2649/3125   Loss: 0.871700 mae: 0.708157 (34.50500911593451 steps/sec)\n",
      "Step #52700\tEpoch  16 Batch 2699/3125   Loss: 0.745653 mae: 0.706431 (32.407832515109035 steps/sec)\n",
      "Step #52750\tEpoch  16 Batch 2749/3125   Loss: 0.942689 mae: 0.706643 (32.51015384233796 steps/sec)\n",
      "Step #52800\tEpoch  16 Batch 2799/3125   Loss: 0.812535 mae: 0.711663 (32.185823824469985 steps/sec)\n",
      "Step #52850\tEpoch  16 Batch 2849/3125   Loss: 0.801843 mae: 0.708189 (30.495370623497156 steps/sec)\n",
      "Step #52900\tEpoch  16 Batch 2899/3125   Loss: 0.822772 mae: 0.707508 (30.460259359677927 steps/sec)\n",
      "Step #52950\tEpoch  16 Batch 2949/3125   Loss: 0.840458 mae: 0.708947 (28.178304546831782 steps/sec)\n",
      "Step #53000\tEpoch  16 Batch 2999/3125   Loss: 0.729021 mae: 0.706203 (27.867711184932382 steps/sec)\n",
      "Step #53050\tEpoch  16 Batch 3049/3125   Loss: 0.760376 mae: 0.698137 (28.115141728520186 steps/sec)\n",
      "Step #53100\tEpoch  16 Batch 3099/3125   Loss: 0.837436 mae: 0.704673 (27.012076756137525 steps/sec)\n",
      "\n",
      "Train time for epoch #17 (53125 total steps): 105.12598299980164\n",
      "Model test set loss: 0.840710 mae: 0.722098\n",
      "Step #53150\tEpoch  17 Batch   24/3125   Loss: 0.705186 mae: 0.721557 (42.447305499877444 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #53200\tEpoch  17 Batch   74/3125   Loss: 0.845379 mae: 0.702242 (22.94024242917481 steps/sec)\n",
      "Step #53250\tEpoch  17 Batch  124/3125   Loss: 0.670297 mae: 0.710470 (21.30130466875114 steps/sec)\n",
      "Step #53300\tEpoch  17 Batch  174/3125   Loss: 0.725282 mae: 0.708313 (22.327160432708826 steps/sec)\n",
      "Step #53350\tEpoch  17 Batch  224/3125   Loss: 0.599842 mae: 0.705293 (21.310683794729886 steps/sec)\n",
      "Step #53400\tEpoch  17 Batch  274/3125   Loss: 0.799659 mae: 0.706652 (20.441914955710445 steps/sec)\n",
      "Step #53450\tEpoch  17 Batch  324/3125   Loss: 0.762590 mae: 0.708280 (22.091364398401574 steps/sec)\n",
      "Step #53500\tEpoch  17 Batch  374/3125   Loss: 0.782939 mae: 0.710225 (21.26127072354758 steps/sec)\n",
      "Step #53550\tEpoch  17 Batch  424/3125   Loss: 0.872094 mae: 0.714206 (21.265582602789635 steps/sec)\n",
      "Step #53600\tEpoch  17 Batch  474/3125   Loss: 0.785441 mae: 0.708729 (20.916686905277814 steps/sec)\n",
      "Step #53650\tEpoch  17 Batch  524/3125   Loss: 0.819400 mae: 0.702517 (20.26648740935463 steps/sec)\n",
      "Step #53700\tEpoch  17 Batch  574/3125   Loss: 0.874147 mae: 0.714210 (20.192425548836468 steps/sec)\n",
      "Step #53750\tEpoch  17 Batch  624/3125   Loss: 0.629862 mae: 0.708202 (20.48007000023926 steps/sec)\n",
      "Step #53800\tEpoch  17 Batch  674/3125   Loss: 0.827426 mae: 0.711669 (19.582041030973425 steps/sec)\n",
      "Step #53850\tEpoch  17 Batch  724/3125   Loss: 0.652202 mae: 0.708575 (18.156457494675315 steps/sec)\n",
      "Step #53900\tEpoch  17 Batch  774/3125   Loss: 0.749893 mae: 0.703111 (19.14575583417088 steps/sec)\n",
      "Step #53950\tEpoch  17 Batch  824/3125   Loss: 0.801496 mae: 0.707153 (20.080982638629887 steps/sec)\n",
      "Step #54000\tEpoch  17 Batch  874/3125   Loss: 0.757156 mae: 0.708810 (20.172596422254465 steps/sec)\n",
      "Step #54050\tEpoch  17 Batch  924/3125   Loss: 0.928278 mae: 0.710811 (19.897385509851812 steps/sec)\n",
      "Step #54100\tEpoch  17 Batch  974/3125   Loss: 0.734283 mae: 0.708853 (20.542900596220107 steps/sec)\n",
      "Step #54150\tEpoch  17 Batch 1024/3125   Loss: 0.802784 mae: 0.706563 (17.720165548597095 steps/sec)\n",
      "Step #54200\tEpoch  17 Batch 1074/3125   Loss: 0.848309 mae: 0.707747 (19.008667850007193 steps/sec)\n",
      "Step #54250\tEpoch  17 Batch 1124/3125   Loss: 0.822098 mae: 0.709278 (16.064158832605596 steps/sec)\n",
      "Step #54300\tEpoch  17 Batch 1174/3125   Loss: 0.735524 mae: 0.711228 (15.512866498194994 steps/sec)\n",
      "Step #54350\tEpoch  17 Batch 1224/3125   Loss: 0.736239 mae: 0.703503 (13.7446952893455 steps/sec)\n",
      "Step #54400\tEpoch  17 Batch 1274/3125   Loss: 0.925025 mae: 0.720257 (16.44483045565738 steps/sec)\n",
      "Step #54450\tEpoch  17 Batch 1324/3125   Loss: 0.747586 mae: 0.704524 (23.708886378112258 steps/sec)\n",
      "Step #54500\tEpoch  17 Batch 1374/3125   Loss: 0.781277 mae: 0.702637 (40.44544233923957 steps/sec)\n",
      "Step #54550\tEpoch  17 Batch 1424/3125   Loss: 0.794180 mae: 0.706754 (31.884810672381697 steps/sec)\n",
      "Step #54600\tEpoch  17 Batch 1474/3125   Loss: 0.712140 mae: 0.706843 (15.14927820041064 steps/sec)\n",
      "Step #54650\tEpoch  17 Batch 1524/3125   Loss: 0.722844 mae: 0.702475 (18.15940690843915 steps/sec)\n",
      "Step #54700\tEpoch  17 Batch 1574/3125   Loss: 0.781234 mae: 0.711959 (37.82485852778717 steps/sec)\n",
      "Step #54750\tEpoch  17 Batch 1624/3125   Loss: 0.874573 mae: 0.700161 (40.31669670815057 steps/sec)\n",
      "Step #54800\tEpoch  17 Batch 1674/3125   Loss: 0.736885 mae: 0.709194 (42.67478453034177 steps/sec)\n",
      "Step #54850\tEpoch  17 Batch 1724/3125   Loss: 0.908401 mae: 0.705997 (42.26420569910752 steps/sec)\n",
      "Step #54900\tEpoch  17 Batch 1774/3125   Loss: 0.748367 mae: 0.707213 (40.5870195591257 steps/sec)\n",
      "Step #54950\tEpoch  17 Batch 1824/3125   Loss: 0.736769 mae: 0.710102 (39.9541998403472 steps/sec)\n",
      "Step #55000\tEpoch  17 Batch 1874/3125   Loss: 0.864853 mae: 0.709451 (39.26037099765597 steps/sec)\n",
      "Step #55050\tEpoch  17 Batch 1924/3125   Loss: 1.007827 mae: 0.708871 (38.64836006087459 steps/sec)\n",
      "Step #55100\tEpoch  17 Batch 1974/3125   Loss: 0.802310 mae: 0.705985 (38.18799184872023 steps/sec)\n",
      "Step #55150\tEpoch  17 Batch 2024/3125   Loss: 0.845702 mae: 0.714631 (35.88973391838886 steps/sec)\n",
      "Step #55200\tEpoch  17 Batch 2074/3125   Loss: 0.878154 mae: 0.706990 (34.681308162845795 steps/sec)\n",
      "Step #55250\tEpoch  17 Batch 2124/3125   Loss: 0.654933 mae: 0.709573 (34.86939887484921 steps/sec)\n",
      "Step #55300\tEpoch  17 Batch 2174/3125   Loss: 0.710105 mae: 0.701832 (35.32814809187079 steps/sec)\n",
      "Step #55350\tEpoch  17 Batch 2224/3125   Loss: 0.706579 mae: 0.707202 (23.142180126689013 steps/sec)\n",
      "Step #55400\tEpoch  17 Batch 2274/3125   Loss: 0.827944 mae: 0.706409 (27.163210958790554 steps/sec)\n",
      "Step #55450\tEpoch  17 Batch 2324/3125   Loss: 0.680866 mae: 0.717101 (30.68713868952149 steps/sec)\n",
      "Step #55500\tEpoch  17 Batch 2374/3125   Loss: 0.773493 mae: 0.706770 (0.05054556252619395 steps/sec)\n",
      "Step #55550\tEpoch  17 Batch 2424/3125   Loss: 0.777633 mae: 0.707394 (28.448580848343834 steps/sec)\n",
      "Step #55600\tEpoch  17 Batch 2474/3125   Loss: 0.853322 mae: 0.701224 (28.309398079447124 steps/sec)\n",
      "Step #55650\tEpoch  17 Batch 2524/3125   Loss: 0.772908 mae: 0.712841 (27.917440652781465 steps/sec)\n",
      "Step #55700\tEpoch  17 Batch 2574/3125   Loss: 0.862777 mae: 0.713295 (0.04726998896277521 steps/sec)\n",
      "Step #55750\tEpoch  17 Batch 2624/3125   Loss: 0.940018 mae: 0.711194 (23.61817239903597 steps/sec)\n",
      "Step #55800\tEpoch  17 Batch 2674/3125   Loss: 0.814599 mae: 0.709507 (24.520341697823483 steps/sec)\n",
      "Step #55850\tEpoch  17 Batch 2724/3125   Loss: 0.791455 mae: 0.700227 (25.61867611496076 steps/sec)\n",
      "Step #55900\tEpoch  17 Batch 2774/3125   Loss: 0.745616 mae: 0.708650 (26.740444713515455 steps/sec)\n",
      "Step #55950\tEpoch  17 Batch 2824/3125   Loss: 0.844510 mae: 0.716088 (26.516743914108666 steps/sec)\n",
      "Step #56000\tEpoch  17 Batch 2874/3125   Loss: 0.835849 mae: 0.702856 (25.280805146736178 steps/sec)\n",
      "Step #56050\tEpoch  17 Batch 2924/3125   Loss: 0.835102 mae: 0.707479 (24.578126583999357 steps/sec)\n",
      "Step #56100\tEpoch  17 Batch 2974/3125   Loss: 0.711172 mae: 0.709155 (25.85265988809827 steps/sec)\n",
      "Step #56150\tEpoch  17 Batch 3024/3125   Loss: 0.691970 mae: 0.699861 (23.19067769152154 steps/sec)\n",
      "Step #56200\tEpoch  17 Batch 3074/3125   Loss: 0.816812 mae: 0.701629 (0.05194949714665006 steps/sec)\n",
      "Step #56250\tEpoch  17 Batch 3124/3125   Loss: 0.839064 mae: 0.704037 (23.358832379165786 steps/sec)\n",
      "\n",
      "Train time for epoch #18 (56250 total steps): 3134.2612841129303\n",
      "Model test set loss: 0.841398 mae: 0.722763\n",
      "Step #56300\tEpoch  18 Batch   49/3125   Loss: 0.898668 mae: 0.721840 (22.125086616020734 steps/sec)\n",
      "Step #56350\tEpoch  18 Batch   99/3125   Loss: 0.979093 mae: 0.699451 (42.98900549264146 steps/sec)\n",
      "Step #56400\tEpoch  18 Batch  149/3125   Loss: 0.753988 mae: 0.710360 (40.58615553105467 steps/sec)\n",
      "Step #56450\tEpoch  18 Batch  199/3125   Loss: 0.860620 mae: 0.706906 (43.483738426429895 steps/sec)\n",
      "Step #56500\tEpoch  18 Batch  249/3125   Loss: 0.804274 mae: 0.701187 (41.548297931017 steps/sec)\n",
      "Step #56550\tEpoch  18 Batch  299/3125   Loss: 0.775826 mae: 0.707259 (44.496649307795195 steps/sec)\n",
      "Step #56600\tEpoch  18 Batch  349/3125   Loss: 0.907547 mae: 0.711046 (44.24802586243638 steps/sec)\n",
      "Step #56650\tEpoch  18 Batch  399/3125   Loss: 0.907969 mae: 0.708597 (44.19842368273691 steps/sec)\n",
      "Step #56700\tEpoch  18 Batch  449/3125   Loss: 0.800049 mae: 0.716072 (41.817487176501274 steps/sec)\n",
      "Step #56750\tEpoch  18 Batch  499/3125   Loss: 0.792096 mae: 0.703143 (0.06086317326948421 steps/sec)\n",
      "Step #56800\tEpoch  18 Batch  549/3125   Loss: 0.767347 mae: 0.703376 (31.530620222637857 steps/sec)\n",
      "Step #56850\tEpoch  18 Batch  599/3125   Loss: 0.706042 mae: 0.715536 (35.81169162716054 steps/sec)\n",
      "Step #56900\tEpoch  18 Batch  649/3125   Loss: 0.775147 mae: 0.711543 (36.28553068370688 steps/sec)\n",
      "Step #56950\tEpoch  18 Batch  699/3125   Loss: 0.773985 mae: 0.711102 (33.97104198820997 steps/sec)\n",
      "Step #57000\tEpoch  18 Batch  749/3125   Loss: 0.771352 mae: 0.699081 (34.92564059007897 steps/sec)\n",
      "Step #57050\tEpoch  18 Batch  799/3125   Loss: 0.891582 mae: 0.710368 (35.74009004418362 steps/sec)\n",
      "Step #57100\tEpoch  18 Batch  849/3125   Loss: 0.918107 mae: 0.708750 (34.563993719269405 steps/sec)\n",
      "Step #57150\tEpoch  18 Batch  899/3125   Loss: 0.862511 mae: 0.706382 (36.22683645693016 steps/sec)\n",
      "Step #57200\tEpoch  18 Batch  949/3125   Loss: 0.695935 mae: 0.706476 (33.130117359318184 steps/sec)\n",
      "Step #57250\tEpoch  18 Batch  999/3125   Loss: 0.834506 mae: 0.705972 (32.98346966928172 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #57300\tEpoch  18 Batch 1049/3125   Loss: 0.877392 mae: 0.713931 (22.79426821681093 steps/sec)\n",
      "Step #57350\tEpoch  18 Batch 1099/3125   Loss: 0.779352 mae: 0.706743 (27.995427077531968 steps/sec)\n",
      "Step #57400\tEpoch  18 Batch 1149/3125   Loss: 0.727668 mae: 0.709858 (17.98105789549587 steps/sec)\n",
      "Step #57450\tEpoch  18 Batch 1199/3125   Loss: 0.810227 mae: 0.704998 (25.93591954783501 steps/sec)\n",
      "Step #57500\tEpoch  18 Batch 1249/3125   Loss: 0.899699 mae: 0.713300 (26.88500270433012 steps/sec)\n",
      "Step #57550\tEpoch  18 Batch 1299/3125   Loss: 0.773210 mae: 0.715550 (27.77342658453992 steps/sec)\n",
      "Step #57600\tEpoch  18 Batch 1349/3125   Loss: 0.821798 mae: 0.695324 (27.57482465549746 steps/sec)\n",
      "Step #57650\tEpoch  18 Batch 1399/3125   Loss: 0.830553 mae: 0.709634 (26.820139556928112 steps/sec)\n",
      "Step #57700\tEpoch  18 Batch 1449/3125   Loss: 0.805900 mae: 0.706897 (24.989168570408584 steps/sec)\n",
      "Step #57750\tEpoch  18 Batch 1499/3125   Loss: 0.859116 mae: 0.703841 (25.04002591460812 steps/sec)\n",
      "Step #57800\tEpoch  18 Batch 1549/3125   Loss: 0.873250 mae: 0.706085 (25.69650619011782 steps/sec)\n",
      "Step #57850\tEpoch  18 Batch 1599/3125   Loss: 0.810904 mae: 0.704447 (25.036091978585365 steps/sec)\n",
      "Step #57900\tEpoch  18 Batch 1649/3125   Loss: 0.835462 mae: 0.707379 (23.336930246553433 steps/sec)\n",
      "Step #57950\tEpoch  18 Batch 1699/3125   Loss: 0.880261 mae: 0.702811 (25.725389290647737 steps/sec)\n",
      "Step #58000\tEpoch  18 Batch 1749/3125   Loss: 0.821568 mae: 0.704823 (23.838199349515534 steps/sec)\n",
      "Step #58050\tEpoch  18 Batch 1799/3125   Loss: 0.904043 mae: 0.709923 (24.570653963180668 steps/sec)\n",
      "Step #58100\tEpoch  18 Batch 1849/3125   Loss: 0.762811 mae: 0.708625 (24.7206573685854 steps/sec)\n",
      "Step #58150\tEpoch  18 Batch 1899/3125   Loss: 0.873484 mae: 0.707988 (24.670733689968408 steps/sec)\n",
      "Step #58200\tEpoch  18 Batch 1949/3125   Loss: 0.788104 mae: 0.704457 (24.689371472056823 steps/sec)\n",
      "Step #58250\tEpoch  18 Batch 1999/3125   Loss: 0.880021 mae: 0.711615 (22.6515516052327 steps/sec)\n",
      "Step #58300\tEpoch  18 Batch 2049/3125   Loss: 0.783862 mae: 0.716720 (23.766799881049952 steps/sec)\n",
      "Step #58350\tEpoch  18 Batch 2099/3125   Loss: 0.853999 mae: 0.703528 (24.026196164656763 steps/sec)\n",
      "Step #58400\tEpoch  18 Batch 2149/3125   Loss: 0.733117 mae: 0.701030 (20.562744967689316 steps/sec)\n",
      "Step #58450\tEpoch  18 Batch 2199/3125   Loss: 0.757086 mae: 0.704793 (18.41555674219132 steps/sec)\n",
      "Step #58500\tEpoch  18 Batch 2249/3125   Loss: 0.749033 mae: 0.704952 (13.168549804435777 steps/sec)\n",
      "Step #58550\tEpoch  18 Batch 2299/3125   Loss: 0.857888 mae: 0.715571 (21.632342639724026 steps/sec)\n",
      "Step #58600\tEpoch  18 Batch 2349/3125   Loss: 0.815606 mae: 0.710000 (41.26880674295628 steps/sec)\n",
      "Step #58650\tEpoch  18 Batch 2399/3125   Loss: 0.743330 mae: 0.710669 (44.55040596987404 steps/sec)\n",
      "Step #58700\tEpoch  18 Batch 2449/3125   Loss: 0.780643 mae: 0.698821 (43.624766004521085 steps/sec)\n",
      "Step #58750\tEpoch  18 Batch 2499/3125   Loss: 0.880835 mae: 0.710733 (42.980468155974386 steps/sec)\n",
      "Step #58800\tEpoch  18 Batch 2549/3125   Loss: 0.645323 mae: 0.713902 (42.935810395118 steps/sec)\n",
      "Step #58850\tEpoch  18 Batch 2599/3125   Loss: 0.949319 mae: 0.709809 (17.622003573217775 steps/sec)\n",
      "Step #58900\tEpoch  18 Batch 2649/3125   Loss: 0.870519 mae: 0.707078 (19.593136938439688 steps/sec)\n",
      "Step #58950\tEpoch  18 Batch 2699/3125   Loss: 0.741874 mae: 0.705103 (29.77191708021836 steps/sec)\n",
      "Step #59000\tEpoch  18 Batch 2749/3125   Loss: 0.938079 mae: 0.705509 (44.23326133909287 steps/sec)\n",
      "Step #59050\tEpoch  18 Batch 2799/3125   Loss: 0.808198 mae: 0.710239 (43.62020186556966 steps/sec)\n",
      "Step #59100\tEpoch  18 Batch 2849/3125   Loss: 0.797179 mae: 0.707152 (43.16856090936824 steps/sec)\n",
      "Step #59150\tEpoch  18 Batch 2899/3125   Loss: 0.821588 mae: 0.706683 (42.48257722730218 steps/sec)\n",
      "Step #59200\tEpoch  18 Batch 2949/3125   Loss: 0.842350 mae: 0.708190 (41.225753121440064 steps/sec)\n",
      "Step #59250\tEpoch  18 Batch 2999/3125   Loss: 0.725945 mae: 0.705537 (40.732969385283624 steps/sec)\n",
      "Step #59300\tEpoch  18 Batch 3049/3125   Loss: 0.760564 mae: 0.697309 (39.901055598481015 steps/sec)\n",
      "Step #59350\tEpoch  18 Batch 3099/3125   Loss: 0.837107 mae: 0.703881 (38.26058146760383 steps/sec)\n",
      "\n",
      "Train time for epoch #19 (59375 total steps): 926.7546181678772\n",
      "Model test set loss: 0.841864 mae: 0.722379\n",
      "Step #59400\tEpoch  19 Batch   24/3125   Loss: 0.704043 mae: 0.721805 (28.82689897694606 steps/sec)\n",
      "Step #59450\tEpoch  19 Batch   74/3125   Loss: 0.846116 mae: 0.701440 (24.642417089623038 steps/sec)\n",
      "Step #59500\tEpoch  19 Batch  124/3125   Loss: 0.667078 mae: 0.709549 (37.13311353728801 steps/sec)\n",
      "Step #59550\tEpoch  19 Batch  174/3125   Loss: 0.723798 mae: 0.706970 (38.621704708836106 steps/sec)\n",
      "Step #59600\tEpoch  19 Batch  224/3125   Loss: 0.597165 mae: 0.703844 (37.3197897999968 steps/sec)\n",
      "Step #59650\tEpoch  19 Batch  274/3125   Loss: 0.796898 mae: 0.705540 (41.96126797480826 steps/sec)\n",
      "Step #59700\tEpoch  19 Batch  324/3125   Loss: 0.760027 mae: 0.706908 (42.10328510310241 steps/sec)\n",
      "Step #59750\tEpoch  19 Batch  374/3125   Loss: 0.782085 mae: 0.708867 (42.85008196501913 steps/sec)\n",
      "Step #59800\tEpoch  19 Batch  424/3125   Loss: 0.869266 mae: 0.712840 (43.110001885026534 steps/sec)\n",
      "Step #59850\tEpoch  19 Batch  474/3125   Loss: 0.785816 mae: 0.707383 (40.490023128032064 steps/sec)\n",
      "Step #59900\tEpoch  19 Batch  524/3125   Loss: 0.816641 mae: 0.701024 (40.31182211799532 steps/sec)\n",
      "Step #59950\tEpoch  19 Batch  574/3125   Loss: 0.873407 mae: 0.712842 (39.24634513320708 steps/sec)\n",
      "Step #60000\tEpoch  19 Batch  624/3125   Loss: 0.626180 mae: 0.706831 (39.41148281299038 steps/sec)\n",
      "Step #60050\tEpoch  19 Batch  674/3125   Loss: 0.824094 mae: 0.710727 (38.41960140674799 steps/sec)\n",
      "Step #60100\tEpoch  19 Batch  724/3125   Loss: 0.654603 mae: 0.708192 (37.04927532135311 steps/sec)\n",
      "Step #60150\tEpoch  19 Batch  774/3125   Loss: 0.749212 mae: 0.702400 (35.97696543167857 steps/sec)\n",
      "Step #60200\tEpoch  19 Batch  824/3125   Loss: 0.798764 mae: 0.706809 (36.894052078944384 steps/sec)\n",
      "Step #60250\tEpoch  19 Batch  874/3125   Loss: 0.757812 mae: 0.708627 (34.496506762190556 steps/sec)\n",
      "Step #60300\tEpoch  19 Batch  924/3125   Loss: 0.926111 mae: 0.710520 (35.0096457354097 steps/sec)\n",
      "Step #60350\tEpoch  19 Batch  974/3125   Loss: 0.732657 mae: 0.707897 (33.120149994819926 steps/sec)\n",
      "Step #60400\tEpoch  19 Batch 1024/3125   Loss: 0.804565 mae: 0.706286 (31.238499606083813 steps/sec)\n",
      "Step #60450\tEpoch  19 Batch 1074/3125   Loss: 0.851393 mae: 0.707231 (31.641441097548004 steps/sec)\n",
      "Step #60500\tEpoch  19 Batch 1124/3125   Loss: 0.820448 mae: 0.708803 (32.43981049222646 steps/sec)\n",
      "Step #60550\tEpoch  19 Batch 1174/3125   Loss: 0.733696 mae: 0.710835 (29.771739567124445 steps/sec)\n",
      "Step #60600\tEpoch  19 Batch 1224/3125   Loss: 0.735522 mae: 0.703082 (28.75857397980628 steps/sec)\n",
      "Step #60650\tEpoch  19 Batch 1274/3125   Loss: 0.931117 mae: 0.719652 (29.45678572928493 steps/sec)\n",
      "Step #60700\tEpoch  19 Batch 1324/3125   Loss: 0.747572 mae: 0.704122 (28.625078911179283 steps/sec)\n",
      "Step #60750\tEpoch  19 Batch 1374/3125   Loss: 0.781038 mae: 0.702058 (28.365673989628952 steps/sec)\n",
      "Step #60800\tEpoch  19 Batch 1424/3125   Loss: 0.794630 mae: 0.706484 (27.42358827793917 steps/sec)\n",
      "Step #60850\tEpoch  19 Batch 1474/3125   Loss: 0.711366 mae: 0.706704 (26.87971322180962 steps/sec)\n",
      "Step #60900\tEpoch  19 Batch 1524/3125   Loss: 0.720227 mae: 0.701218 (26.258918714315282 steps/sec)\n",
      "Step #60950\tEpoch  19 Batch 1574/3125   Loss: 0.778563 mae: 0.710822 (26.06701384534171 steps/sec)\n",
      "Step #61000\tEpoch  19 Batch 1624/3125   Loss: 0.878800 mae: 0.699574 (24.838422788009773 steps/sec)\n",
      "Step #61050\tEpoch  19 Batch 1674/3125   Loss: 0.731164 mae: 0.707785 (25.75175997513668 steps/sec)\n",
      "Step #61100\tEpoch  19 Batch 1724/3125   Loss: 0.910359 mae: 0.704329 (24.263081127803257 steps/sec)\n",
      "Step #61150\tEpoch  19 Batch 1774/3125   Loss: 0.742644 mae: 0.705725 (25.113108571535317 steps/sec)\n",
      "Step #61200\tEpoch  19 Batch 1824/3125   Loss: 0.730492 mae: 0.707961 (23.56112303250233 steps/sec)\n",
      "Step #61250\tEpoch  19 Batch 1874/3125   Loss: 0.853779 mae: 0.707496 (23.144131357318717 steps/sec)\n",
      "Step #61300\tEpoch  19 Batch 1924/3125   Loss: 1.006371 mae: 0.707641 (23.382908121197417 steps/sec)\n",
      "Step #61350\tEpoch  19 Batch 1974/3125   Loss: 0.796743 mae: 0.704324 (24.013109268445614 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #61400\tEpoch  19 Batch 2024/3125   Loss: 0.844735 mae: 0.713428 (23.40160072308718 steps/sec)\n",
      "Step #61450\tEpoch  19 Batch 2074/3125   Loss: 0.877361 mae: 0.705930 (24.813254452544157 steps/sec)\n",
      "Step #61500\tEpoch  19 Batch 2124/3125   Loss: 0.656184 mae: 0.708735 (45.391505282738066 steps/sec)\n",
      "Step #61550\tEpoch  19 Batch 2174/3125   Loss: 0.708990 mae: 0.701112 (42.69861765864023 steps/sec)\n",
      "Step #61600\tEpoch  19 Batch 2224/3125   Loss: 0.705421 mae: 0.707051 (44.204572449246946 steps/sec)\n",
      "Step #61650\tEpoch  19 Batch 2274/3125   Loss: 0.829594 mae: 0.706151 (43.04534519987849 steps/sec)\n",
      "Step #61700\tEpoch  19 Batch 2324/3125   Loss: 0.681269 mae: 0.716993 (43.19945288778844 steps/sec)\n",
      "Step #61750\tEpoch  19 Batch 2374/3125   Loss: 0.772263 mae: 0.706642 (42.013269381097444 steps/sec)\n",
      "Step #61800\tEpoch  19 Batch 2424/3125   Loss: 0.781611 mae: 0.707057 (41.036044920743755 steps/sec)\n",
      "Step #61850\tEpoch  19 Batch 2474/3125   Loss: 0.850736 mae: 0.700992 (40.8243864911248 steps/sec)\n",
      "Step #61900\tEpoch  19 Batch 2524/3125   Loss: 0.772985 mae: 0.712252 (39.732114692257575 steps/sec)\n",
      "Step #61950\tEpoch  19 Batch 2574/3125   Loss: 0.863395 mae: 0.712694 (38.200422378251126 steps/sec)\n",
      "Step #62000\tEpoch  19 Batch 2624/3125   Loss: 0.939530 mae: 0.710507 (37.66932679894821 steps/sec)\n",
      "Step #62050\tEpoch  19 Batch 2674/3125   Loss: 0.813736 mae: 0.708754 (37.65748964314528 steps/sec)\n",
      "Step #62100\tEpoch  19 Batch 2724/3125   Loss: 0.791367 mae: 0.699342 (36.32113802833294 steps/sec)\n",
      "Step #62150\tEpoch  19 Batch 2774/3125   Loss: 0.744596 mae: 0.707392 (34.79752197212152 steps/sec)\n",
      "Step #62200\tEpoch  19 Batch 2824/3125   Loss: 0.841847 mae: 0.714854 (32.77463686396495 steps/sec)\n",
      "Step #62250\tEpoch  19 Batch 2874/3125   Loss: 0.832798 mae: 0.701900 (32.4507985149536 steps/sec)\n",
      "Step #62300\tEpoch  19 Batch 2924/3125   Loss: 0.833699 mae: 0.706767 (32.017538904520755 steps/sec)\n",
      "Step #62350\tEpoch  19 Batch 2974/3125   Loss: 0.706814 mae: 0.708411 (30.969568143912845 steps/sec)\n",
      "Step #62400\tEpoch  19 Batch 3024/3125   Loss: 0.693594 mae: 0.699167 (30.579612998353603 steps/sec)\n",
      "Step #62450\tEpoch  19 Batch 3074/3125   Loss: 0.814354 mae: 0.700806 (31.87608715874387 steps/sec)\n",
      "Step #62500\tEpoch  19 Batch 3124/3125   Loss: 0.835882 mae: 0.703500 (29.21912294089429 steps/sec)\n",
      "\n",
      "Train time for epoch #20 (62500 total steps): 97.38325214385986\n",
      "Model test set loss: 0.842276 mae: 0.723000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 21:41:04.719266: W tensorflow/python/util/util.cc:299] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/wanyinzhen/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./models/export/assets\n"
     ]
    }
   ],
   "source": [
    "first_net = first_net()\n",
    "first_net.training(features, targets_values, epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962f8568",
   "metadata": {},
   "source": [
    "### 6.4 training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b83d80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 368
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(first_net.losses['train'], label='Training loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a158f5",
   "metadata": {},
   "source": [
    "### 6.5 test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47583c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 380
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(first_net.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd0418e",
   "metadata": {},
   "source": [
    "### 6.6Calculating user ratings for movie predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09f31d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rat_the_film(first_net, user_id_val, movie_id_val):\n",
    "    categories = np.zeros([1, 18])\n",
    "    categories[0] = movies.values[movieid2idx[movie_id_val]][2]\n",
    "    \n",
    "    titles = np.zeros([1, sentences_size])\n",
    "    titles[0] = movies.values[movieid2idx[movie_id_val]][1]\n",
    "    \n",
    "    inference_val = first_net.model([np.reshape(users.values[user_id_val-1][0], [1, 1]),\n",
    "              np.reshape(users.values[user_id_val-1][1], [1, 1]),\n",
    "              np.reshape(users.values[user_id_val-1][2], [1, 1]),\n",
    "              np.reshape(users.values[user_id_val-1][3], [1, 1]),\n",
    "              np.reshape(movies.values[movieid2idx[movie_id_val]][0], [1, 1]),\n",
    "              categories,  \n",
    "              titles])\n",
    "\n",
    "    return (inference_val.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f5eae",
   "metadata": {},
   "source": [
    "### 7.predict the rating of movie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b98e37",
   "metadata": {},
   "source": [
    "### 7.1 We start by randomly selecting 10 different people to rate the same movie film, then ten different scores will be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "712d1acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.491043]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rat_the_film(first_net, 300, 1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9220c401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.8555791]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rat_the_film(first_net, 209, 1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6346ca86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.9885392]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rat_the_film(first_net, 208, 1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cd9a2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.025576]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rat_the_film(first_net, 207, 1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "647ab2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.3729134]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rat_the_film(first_net, 1, 1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d15d11a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.6308107]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rat_the_film(first_net, 21, 1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb7ac25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.4454937]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rat_the_film(first_net, 568, 1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bfb4f5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.242361]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rat_the_film(first_net, 1001, 1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2284cd05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.789609]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rat_the_film(first_net, 3000, 1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106850f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
